# Training Quantized Neural Networks with a Full-precision Auxiliary Module



## Introduction

量化的核心挑战是离散量化器的不可微分性。目前的解决方案可分为两类。第一类是使用梯度代理函数。（STE；用于基于梯度的优化[1,31]）。第二类是从离散网络训练的全精度模型中寻求指导。如知识蒸馏[32,56,57]，通过从全精度的教师网络中提取知识来学习低精度的学生网络。

本文提出在混合精度(部分完全精度)模型和低精度模型之间共享参数。构建了一个全精度辅助模块。训练时，低精度网络和全精度辅助模块将相结合，形成一个增强的混合精度网络。然后，对混合精度网络和低精度模型进行联合优化.

**辅助监督 Auxiliary supervision**

直接的添加辅助监督的方法是在中间层引入额外的损失，有助于解决梯度消失问题，同时提供正则化。例如**GoogLeNet**、**DSN**以及**语义分割**等。

作者提出的辅助学习策略使用权重共享来协助优化，其动机与KD方法有很大的不同。作者不需要预训练一个通常更深的教师网络。此外，在网络量化方面，作者展示了与KD方法相比，其方法具有一致的优越性能。

## Method

**网络结构**

![image-20230927221148268](https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230927221148268.png)

辅助模块 *H* 由一系列适配器（adaptors）和聚合器（aggregators）组成，如图1(a)所示。*H* 接收来自低精度网络*F* 中对应块的*P* 个输出特征图，表示为 $\{O_p\}^P_{p=1}$。这里 $\{B_1,...,B_p\}$ 是生成这些特征图的块的索引。对于辅助模块*H* 的每个输入，设计了一个可训练的适配器$\phi_p(·)$。这个适配器接收来自*F* 中第*$B_p$* 个块的输出特征图$O_p$，并输出一个调整后的特征表示 $\phi_p(O_p)$。

**Adaptor结构**   adaptor的引入是为了弥补低精度模型和全精度模型之间的分布差异。这确保了量化激活 $\{O_p\}^P_{p=1}$与*H* 中的全精度计算相兼容。在本文中，**adaptor是通过一个简单的 1×1 卷积层实现的，后接一个批量归一化层。**

$g_p$ 表示第 *p* 个聚合特征。它是通过将来自 *F* 的适应特征 $\phi_p(O_p)$ 和来自 *H* 的第 (p-1) 个聚合特征相加，然后应用ReLU激活函数得到的：
$$
g_p = ReLU(\phi_p(O_p)+g_{p-1})
$$
$g_p$ 的计算涉及到逐层的特征聚合，这意味着每一层的聚合特征都依赖于前一层的聚合特征和当前层的适应特征。这种逐层聚合的方式有助于在辅助模块中融合来自不同层的信息。

在辅助模块 *H* 的最后一层，将 $g_p$ 送入分类器层进行类别预测，并计算辅助损失。这个辅助模块 *H* 的设计类似于ResNet中的跳跃连接，有助于梯度的反向传播，从而优化低精度网络的参数。

**总之， $g_p$ 在辅助模块中起到了特征聚合的作用，有助于融合不同层的信息，并通过辅助损失对低精度网络进行优化。**



## Difference

### 与应用辅助分类损失到中间激活的差异

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20231004165606521.png" alt="image-20231004165606521" style="zoom:67%;" />

另一种为低精度网络创建辅助全精度路径的方法是将分类损失应用于中间激活。这个想法的示意图如图2(a)所示，其中每个中间输出都连接到一个分类器以执行分类。在这种情况下，最终的训练目标变为：
$$
\widetilde{\mathcal{L}}_{obj}=\widetilde{\mathcal{L}}+\sum_{i=1}^M\alpha_i\widetilde{\ell}_i,
$$
其中，$\widetilde{\mathcal{L}}$ 是原始低精度网络的分类损失，$\widetilde{\ell}_i$ 是应用于第i个中间输出的分类损失，而$\alpha_i$是与第i个损失函数关联的权重。这种方案可以在训练期间通过全精度分类器直接传播梯度到每个块。但是，它的监督非常受限，因为它基本上假设中间输出可以直接用于分类。

### 与知识蒸馏的差异

知识蒸馏(KD)已被探索用于辅助量化模型训练。具体来说，低精度的学生网络学习生成与全精度教师网络相似的后验概率和/或特征表示，如图(2)所示。训练目标可以被表述为：
$$
\hat{\mathcal{L}}_{obj}=\hat{\mathcal{L}}_1+\hat{\mathcal{L}}_2+\sum_{i=1}^M\beta_i\hat{\ell}_i,
$$
其中，*L*1 和 *L*2 是学生和教师网络的任务特定目标。*$\hat{\ell}_i$* 表示第i个蒸馏损失。

在KD中，来自全精度模型到低精度模型的指导是通过蒸馏损失实现的，而在所提出的方法中，是通过使低精度模型的参数与混合精度模型共享来实现的。与KD相比，所提出的方法有许多优点：(1) 只需要额外的内存来存储辅助模块，而不是一个全精度网络，因此更加节省内存。(2) 只使用一个辅助损失，但可以为低精度模型的各个块创建指导信号。相比之下，KD需要多个蒸馏损失来实现这一点，因此通常涉及更多的超参数，即每个损失项的权重$\beta_i$。

