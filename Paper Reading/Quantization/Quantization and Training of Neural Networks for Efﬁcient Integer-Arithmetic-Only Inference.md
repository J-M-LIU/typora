# Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference



## 背景

视觉领域的卷积神经网络(Convolutional Neural Network，CNN)大部分都是大模型，在移动端上应用比较难，因而有很多工作在降低模型内存占用和延迟上。这块的工作主要分为两个方向：

- 模型结构的改进，比如SqueezeNet，ShuffleNet，DenseNet，MobileNet等。
- 量化，即将CNN模型参数和激活值从32位的float转成更少位数的整数，比如Ternary weight networks(TWN), Binary Neural Networks(BNN), XNOR-net等。

虽然在量化上有很丰富的工作，但是却有两个缺点：

- 之前的工作大部分都是在经典网络结构比如AlexNet，VGG，GoogleNet等上，这些网络为了达到较高的accuracy，本身就有些over-parameterized了，比较容易达到较好的效果。而在压缩的比较好的网络结构如MobileNet上还没有人尝试过做量化。
- 很多工作虽然做了量化，但是并没有达到latency的提升，比如只对权重做量化，虽然降低了内存占用，但是计算优化却并没有很多。又或者会带来效果的退化，比如BNN，虽然将矩阵操作中的加和乘操作变成了位操作带来了加速，但是效果却退化了不少。

## 主要思想

- 提供了量化推理框架，适用于只能整数运算的设备；
- 提供了一个量化训练框架，最小化量化带来的准确率的损失。

### 量化推理

在推理过程中使用纯整数运算，在训练过程中使用浮点运算。

量化的基本公式是
$$
r = S(q-Z)
$$
其中，r是真实值，q是量化后的值，S和r同类型，一般是float且S为正数；q和Z同类型，一般是8b整数。

另外有一点，在本文介绍的算法中，没有考虑查表的方式，因为查表方式不利于并行化。所谓的查表是指预先把一些计算算好，比如8bit整数一共有256个，可以先算好这些值的指数，然后用的时候直接查表可以节省指数运算。

### 纯整数矩阵运算

假设 $r_1$、$r_2$是浮点实数上的两个 $N\times N$矩阵，$r_3$为其相乘后的结果矩阵：
$$
r_3^{i,k} = \sum_{j=1}^N r_1^{i,j}r_2^{j,k}
$$
假设 $S_1$ $S_2$、$Z_1$ $Z_2$ 是矩阵对应的 scale 和 zero point：
$$
S_3(q_3^{i,k}-Z_3) = \sum_{j=1}^N S_1(q_1^{i,j}-Z_1) S_2(q_2^{i,k}-Z_2)
$$
所以：
$$
q_3^{i,k} = \frac{S_1 S_2}{S_3} \sum_{j=1}^N (q_1^{i,j}-Z_1) (q_2^{i,k}-Z_2) + Z_3
$$

可以重写为
$$
q_3^{i,k} = M \sum_{j=1}^N (q_1^{i,j}-Z_1) (q_2^{i,k}-Z_2) + Z_3
$$
公式里的M是：
$$
M := \frac{S_1S_2}{S_3}
$$
这里只有M是浮点数，但根据经验，M永远只在(0, 1)区间，因而可以使用归一化形式
$$
M = 2^{-n}M_0
$$
其中, M0是[0.5, 1)的数字，n是一个非负整数。从而M0可以被表示成一个fixed-point multiplier, 根据系统位数不同而定，假如系统为32bit，那么M0会被表示成整数中比较接近2^31*M0的数字，我们称其为有了32bit的精度。这样在计算的时候，只需要用位移就可以计算得到M了。

### zero-points高效计算

在上面的公式中，会有$N^2$个 q 值需要计算，因而，需要计算 $2*N^3$ 个减法运算。为了进一步节省这些计算，对上面的公式做了进一步展开：
$$
\left.\begin{array}{r}
q_{3}^{(i, k)}=Z_{3}+M\left(N Z_{1} Z_{2}-Z_{1} a_{2}^{(k)}\right. \\
-Z_{2} \bar{a}_{1}^{(i)}+\sum_{j=1}^{N} q_{1}^{(i, j)} q_{2}^{(j, k)}
\end{array}\right)
$$
其中，a1和a2可以被预先计算好，花费$2*N^2$个加法计算。
$$
a_{2}^{(k)}:=\sum_{j=1}^{N} q_{2}^{(j, k)}, \quad \bar{a}_{1}^{(i)}:=\sum_{j=1}^{N} q_{1}^{(i, j)}
$$
这样，主要的计算量就集中在量化值的乘法上：
$$
\sum_{j=1}^{N}q_1^{(i,j)}q_2^{(j,k)}
$$

### 训练时量化

一个常见的方法是post-quantization，即训练模型结束之后再进行量化，这种方法对大模型效果不错，对小模型则效果比较差，常见的失败原因是：

- 不同通道的激活值的误差很大，比如，如果同一层使用同一个S和Z，会导致有些通道对应的小区间的参数产生更大的误差。
- 参数中离群点的存在导致量化误差变大。

为了解决这些问题，在训练的时候就引入量化。需要对参数和激活值做量化。量化由两个参数控制，区间和量化层次。见下面公式，区间指的是a,b两个参数，层次指的是n。

**权重量化**

a := min w, b := max w. 且对于int8量化，

**激活量化**

采用exponential moving averages (EMA)指数移动平均来估计激活值的范围。