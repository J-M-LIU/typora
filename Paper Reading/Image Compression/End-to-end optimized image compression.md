# End-to-end optimized image compression

## 简介

​	整体算法分为三个部分：非线性分析变换（编码器）、均匀量化器和非线性综合变换（解码器）组成，在整个训练框架上联合优化整个模型的率失真性能。这个框架是后来被大家广泛使用的超先验（Hyperprior）框架的基础。率失真的联合优化是一个难点，因此大多数现有的图像压缩方法都是通过将数据向量线性转换为合适的连续值表示，独立量化，然后无损熵码对结果的离散表示进行编码。例如，JPEG在像素块上使用离散余弦变换，而JPEG 2000使用多尺度正交小波分解。通常，变换编码方法的三个组成部分——变换、量化器和熵编码，并分别通过手动参数调整。

## 整体算法结构及流程

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230128224738294.png" alt="image-20230128224738294" style="zoom:50%;" />

图1为非线性变换编码框架。$x$ 与 $\hat{x}$ 分别代表原图和重建图片。$g_a$表示编码器的非线性分析变换，$y$ 是由输入图片经过编码器网络后得到的latent code，通过量化器 $q$ 后，得到量化后结果 $ \hat{y}$ ，再通过 $g_s$ 解码器重建图片结果。其中，通过对 $y$ 的码率估计得到 $R$，计算 $x$ 与 $\hat{x}$ 的失真得到 $D$，$g_p$ 是指一种失真变换，可以将原图和重建图进行通过 $g_p$ 转化到感知空间上进行计算， $\hat{z} = g_p(\hat{x})$ ，并评估失真 $D=d(z,\hat{z})$ ，例如PSNR、MS-SSIM或其他感知域如 VMAF 等。通过得到的 $R$ 和 $D$ 进行率失真联合优化，定义损失函数为： $L=λ⋅D+R$ 通过 $\lambda$ 参数进行码率的选择控制，$\lambda$ 大则训练得到的模型的重建图失真，而压缩后的码流大，反之亦然。

​	此模型中，分析变换（编码器）共3层，每层用了卷积层+下采样+GDN的结构优化MSE（依次通过三个卷积层，分别进 行 4 倍、2 倍、2 倍的下采样，激活函数为 GDN），如下图左；而综合变换（解码器）采用分析变换的近似逆结构，每层IGND+上采样+卷积，如下图右。量化使用的是均匀的标量量化。如图2。

<img src="https://img-blog.csdnimg.cn/8daf038c5aac4dd89d39a157709e9f3b.png" style="zoom:80%;" />



## 改进

**激活层改进**

​	受到生物神经元的启发，在编码器与解码器中，论文使用了**GDN**（广义除数归一化），是作者2015年《Density modeling of image using a generalized normalization transformation》提出来的。GDN可以看做是更适合图像重建的BN层。之前大多数压缩方法都是建立在**正交线性变换**基础上，目的是为了降低数据间的相关性，从而简化后续的熵编码。但是线性变换输出之间的联合统计特性展现了很强的高阶依赖。这个问题可以通过使用局部增益控制操作（最早来源于生物神经元领域）来很好地解决，于是作者使用了他自己之前提出的 GDN 模型来替代线性变换（在那篇文章中作者已经验证了GDN具有很好的高斯化图像数据的能力）。

### 为什么要归一化

​	神经网络学习过程的本质就是为了学习数据分布，如果没有做归一化处理，那么每一批次训练数据的分布不一样，从大的方向上看，神经网络则需要在这多个分布中找到平衡点，从小的方向上看，由于每层网络输入数据分布在不断变化，这也会导致每层网络在找平衡点，显然，神经网络就很难收敛了。当然，如果我们只是对输入的数据进行归一化处理（比如将输入的图像除以255，将其规范化到0到1之间），只能保证输入层数据分布是一样的，并不能保证每层网络输入数据分布是一样的，所以也需要在神经网络的中间层加入归一化处理。

​	归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。

​	对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。

### BN层

​	与激活函数层、卷积层、全连接层、池化层一样， BN (Batch Normalization) 层也属于CNN中网络的一层。BN本质原理上是将数据归一化至：均值0、方差为1；BN层是可学习的，参数为γ、β的网络层。BN是针对一个batch里的数据进行归一化和尺度操作，且一旦神经网络训练完成后，BN的尺度参数也固定了，这就是一个完全变成一个关于归一化数据的仿射变换。

### GDN

​	GDN本质也是一种归一化方法，但和BN层不是同一类型的归一化。BN层不同于局部增益控制，其在所有空间位置上缩放因子都是相同的，一旦训练完成，缩放参数固定的；而GDN是空间自适应的，且具有高度非线性。

### 量化问题

​	传统的图像编码中，量化有着不可微分的问题，为分段函数，且在非边界点的导数为0，截断反向传播，在边界点则不存在导数。作者采用基于*概率模型连续松弛*的代理损失函数，用加性均匀噪声代替量化步骤（加了-0.5到0.5区间的均匀噪声）。

## 前向传播

​	在非线形分析变换 $g_a$ 中包含有3个阶段：卷积、降采样、GDN归一化。其归一化和尺度操作对整幅图像都是一致的，相当于一种空间自适应的归一化操作，而且GDN是非线性的。
**卷积**

​	$v_i^k(m,n)$ 表示第 $i$ 个输出的通道在第 $k$ 层网络的输入，总共有3层，所以k最大为3。其中(m,n)代表长宽的二维的位置。描述使用核$h_{i,j}^{(k)}$来卷积输入$u_j^{(k)}$, 并添加一个常数偏移


$$
v_i^{(k)}(m,n) =\sum_j (h_{k,ij}*u_i^{(k)})(m,n)+c_{k,i}
$$
**降采样**：$s_k$ 是k层的降采样因子，即几倍采样。
$$
w_i^{(k)}(m,n) = v_i^{(k)}(s_km,s_kn)
$$
**GDN归一化**
$$
u_{i}^{(k+1)}(m, n)=\frac{w_{i}^{(k)}(m, n)}{\left(\beta_{k, i}+\sum_{j} \gamma_{k, i j}\left(w_{j}^{(k)}(m, n)\right)^{2}\right)^{\frac{1}{2}}}
$$
**所有待优化参数：$h$（卷积核权重）、$c$（卷积核偏置）、$\beta$ 和 $\gamma$ 都在整个端到端过程被优化。**


**IGDN**
$$
w_i^{(k)}(m,n)=u_i^{(k)}(m,n).(\beta_{i,k}+\sum_j\gamma_{i,k,j}(u_j^{(k)}(m,n))^2)^{1/2}
$$
 IGDN是GDN 的近似逆操作。它将GDN操作中的变化进行了逆转。

**上采样**
$$
\hat{v}_i^{(k)}(m,n)=\begin{cases}\hat{w}_i^{(k)}(m/\hat{s}_k,n/\hat{s}_k)&\mathrm{if~}m/\hat{s}_k\mathrm{~and~}n/\hat{s}_k\text{ are integers,}\\0&\mathrm{otherwise},&\end{cases}
$$
上采样是一个过程，用于增加数据的分辨率。它与之前的下采样操作相反。如果 m 和 n 可以被上采样因子 $s^k$ 整除，那么使用从 $w^{(k)}_i$ 得到的值，否则该值为0。

**卷积**
$$
\hat{u}_i^{(k+1)}(m,n)=\sum_j(\hat{h}_{k,ij}*\hat{v}_j^{(k)})(m,n)+\hat{c}_{k,i}.
$$


## Optimization



### 松弛的率失真优化函数

**连续的概率密度函数逼近原始的离散量化值**: 作者引入了一个连续变量 $\tilde{y}$, 其密度函数 $\tilde{y}=y+$ $\Delta y$ 被设计为一个连续的概率密度函数，来逼近离散量化函数；加入独立的均匀噪声来逼近量化器: 通过添加独立的均匀噪声 $\Delta y$ 到每个值上，使得变量 $\tilde{y}$ 变成连续可微，可进一步优化。（$\Delta y$ 为连续函数，加上一个离散的函数，结果仍为连续可微）

![image-20231019144758783](https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20231019144758783.png)

通过适当设计的熵码，实际实现的bit rate只比熵略大(Rissanen and Langdon, 1981)。因此，直接用熵来定义目标函数:
$$
L[g_a,g_s,P_\boldsymbol{q}]=-\operatorname{E}[\log_2P_\boldsymbol{q}]+\lambda\operatorname{E}[d(\boldsymbol{z},\boldsymbol{\hat{z}})],
$$
两个期望都将近似于训练图像集的平均值。给定一组足够强大的变换，我们可以在不失一般性的情况下假设量化步长的大小始终为1:
$$
\hat{y}_i=q_i=\operatorname{round}(y_i),
$$
$y_i$ 的量化值的概率分布：
$$
P_{q_i}(n)=\int_{n-\frac12}^{n+\frac12}p_{y_i}(t)\mathrm{~d}t,\quad\text{ for all }n\in\mathbb{Z}.
$$
这意味着的微分熵可以作为q熵的近似。第二，独立均匀噪声根据其边缘矩近似量化误差，经常被用作量化误差的模型(Gray and Neuhoff, 1998)。因此，我们可以使用相同的近似来测量失真。我们在第4节中检查了这些率和失真近似的经验质量。

~ y我们为的松弛概率模型和熵码假设代码空间中有独立的边际，并对边际p ~ y i进行非参数化建模，以减少模型误差。具体来说，我们使用精细采样的分段线性函数，我们以类似于一维直方图的方式更新它们(见附录)。由于p ~ y i = p y i∗U(0,1)被一个箱车过滤器有效地平滑——单位区间上的均匀密度，U(0,1)——因此可以通过减少采样间隔使模型误差任意小。
