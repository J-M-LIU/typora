# End- to-end optimized image compression

## 简介

​	整体算法分为三个部分：非线性分析变换（编码器）、均匀量化器和非线性综合变换（解码器）组成，在整个训练框架上联合优化整个模型的率失真性能。这个框架是后来被大家广泛使用的超先验（Hyperprior）框架的基础。率失真的联合优化是一个难点，因此大多数现有的图像压缩方法都是通过将数据向量线性转换为合适的连续值表示，独立量化，然后无损熵码对结果的离散表示进行编码。例如，JPEG在像素块上使用离散余弦变换，而JPEG 2000使用多尺度正交小波分解。通常，变换编码方法的三个组成部分——变换、量化器和熵编码，并分别通过手动参数调整。

## 整体算法结构及流程

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230128224738294.png" alt="image-20230128224738294" style="zoom:50%;" />

图1为非线性变换编码框架。$x$ 与 $\hat{x}$ 分别代表输入的原图和经过编解码器后的重建图片。$g_a$表示编码器提供的非线性分析变换，$y$ 是由输入图片经过编码器网络后得到的潜在特征，通过量化器 $q$ 后，得到量化后结果 $ \hat{y}$ ，再通过 $g_s$ 解码器重建图片结果。其中，通过对 $y$ 的码率估计得到 $R$，计算原图 $x$ 与 $\hat{x}$ 的失真得到 $D$，$g_p$ 是指一种失真变换，可以将原图和重建图进行通过 $g_p$ 转化到感知空间上进行计算， $\hat{z} = g_p(\hat{x})$ ，并评估失真 $D=d(z,\hat{z})$ ，例如PSNR、MS-SSIM或其他感知域如 VMAF 等。通过得到的 $R$ 和 $D$ 进行率失真联合优化，定义损失函数为： $L=λ⋅D+R$ 通过 $\lambda$ 参数进行码率的选择控制，$\lambda$ 大则训练得到的模型的重建图失真，而压缩后的码流大，反之亦然。

​	此模型中，分析变换（编码器）共3层，每层用了卷积层+下采样+GDN的结构优化MSE，如下图左；而综合变换（解码器）采用分析变换的近似逆结构，每层IGND+上采样+卷积，如下图右。量化使用的是均匀的标量量化。如图2。

<img src="https://img-blog.csdnimg.cn/8daf038c5aac4dd89d39a157709e9f3b.png" style="zoom:80%;" />



## 改进

**激活层改进**

​	受到生物神经元的启发，在编码器与解码器中，论文使用了**GDN**（广义除数归一化），是作者2015年《Density modeling of image using a generalized normalization transformation》提出来的。GDN可以看做是更适合图像重建的BN层。之前大多数压缩方法都是建立在正交线性变换基础上，目的是为了降低数据间的相关性，从而简化后续的熵编码。但是线性变换输出之间的联合统计特性展现了很强的高阶依赖。这个问题可以通过使用局部增益控制操作（最早来源于生物神经元领域）来很好地解决，于是作者使用了他自己之前提出的 GDN 模型来替代线性变换（在那篇文章中作者已经验证了GDN具有很好的高斯化图像数据的能力）。

### 为什么要归一化

​	神经网络学习过程的本质就是为了学习数据分布，如果没有做归一化处理，那么每一批次训练数据的分布不一样，从大的方向上看，神经网络则需要在这多个分布中找到平衡点，从小的方向上看，由于每层网络输入数据分布在不断变化，这也会导致每层网络在找平衡点，显然，神经网络就很难收敛了。当然，如果我们只是对输入的数据进行归一化处理（比如将输入的图像除以255，将其规范化到0到1之间），只能保证输入层数据分布是一样的，并不能保证每层网络输入数据分布是一样的，所以也需要在神经网络的中间层加入归一化处理。

​	归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。

​	对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。

### BN层

​	与激活函数层、卷积层、全连接层、池化层一样， BN (Batch Normalization) 层也属于CNN中网络的一层。BN本质原理上是将数据归一化至：均值0、方差为1；BN层是可学习的，参数为γ、β的网络层。BN是针对一个batch里的数据进行归一化和尺度操作，且一旦神经网络训练完成后，BN的尺度参数也固定了，这就是一个完全变成一个关于归一化数据的仿射变换。

### GDN

​	GDN本质也是一种归一化方法，但和BN层不是同一类型的归一化。

### 量化问题

​	传统的图像编码中，量化有着不可微分的问题，为分段函数，且在非边界点的导数为0，截断反向传播，在边界点则不存在导数。作者采用基于*概率模型连续松弛*的代理损失函数，用加性均匀噪声代替量化步骤（加了-0.5到0.5区间的均匀噪声）。

## 前向传播

​	在非线形分析变换 $g_a$ 中包含有3个阶段：卷积、降采样、GDN归一化。其归一化和尺度操作对整幅图像都是一致的，相当于一种空间自适应的归一化操作，而且GDN是非线性的。
**卷积**

​	$v_i^k(m,n)$ 表示第 $i$ 个输出的通道在第 $k$​ 层网络的输入，总共有3层，所以k最大为3。其中(m,n)代表长宽的二维的位置。公式(1)描述输入在经过卷积层后的输出。


$$
v_i^{(k)}(m,n) =\sum_j (h_{k,ij}*u_i^{(k)})(m,n)+c_{k,i}
$$
**降采样**：$s_k$ 是k层的降采样因子，即几倍采样。
$$
w_i^{(k)}(m,n) = v_i^{(k)}(s_km,s_kn)
$$
**GDN归一化**
$$
u_{i}^{(k+1)}(m, n)=\frac{w_{i}^{(k)}(m, n)}{\left(\beta_{k, i}+\sum_{j} \gamma_{k, i j}\left(w_{j}^{(k)}(m, n)\right)^{2}\right)^{\frac{1}{2}}}
$$
所有待优化参数：$h$（卷积核权重）、$c$（卷积核偏置）、$\beta$ 和 $\gamma$ 都在整个端到端过程被优化。
