# 内容感知的深度学习视频编码



## 摘要

随着高清和超高清视频的广泛应用，视频压缩技术在多媒体存储、传输及处理领域的重要性日益提升。传统视频编码标准基于混合编码框架，虽然实现了较高的压缩效率，但由于依赖手工设计的特征提取，已陷入性能提升瓶颈。近年来，深度学习技术推动了端到端视频编码技术的发展，拥有了超越传统编码标准的率失真性能。然而，现有方法忽视了人眼感知的非均匀特性，在内容自适应性方面存在不足；码率调节灵活性受限，大多数方法采用固定量化策略，难以在单一模型中实现连续的码率调整，需存储多个模型以适应不同带宽需求，增加了部署成本。此外，计算开销较高，现有方法在低延迟场景中的实际应用受限，难以兼顾高效压缩与实时处理。针对上述问题，本文围绕内容感知的视频编码优化展开研究，主要工作如下：

（1）提出了内容感知的可变码率视频编码方法，基于人眼视觉系统的非均匀感知特性，自适应调整视频帧中关键区域和非关键区域的码率分配。为实现精准的码率分配和连续可变码率，构建分级可变码率调控机制，在全局级、通道级和局部级三个粒度上优化比特分配。全局级通过条件卷积模块实现离散码率级别；通道级则对隐变量特征逐通道缩放，以调整量化损失，并引入指数插值实现连续可变的精细码率调整；局部级结合显著性检测网络调整区域间失真权重，优化关键区域感知质量。实验结果表明，该方法能够在降低比特率的同时保持较高的视觉质量，并在单一模型中实现连续可变码率调节，更灵活地适应不同的带宽需求。

（2）提出了显著区域感知的深度编码模型量化方法，针对现有视频编码量化方案未能充分考虑视频内容的动态特性的问题，提出基于人眼显著性的区域自适应量化策略。首先，通过显著性检测模型生成 ROI（Region of Interest）掩膜，将视频帧划分为显著区域和非显著区域。设计轻量级位宽分配器，给定一组候选位宽项，结合输入区域的特征、光流梯度幅值和通道级标准差，估计候选位宽的选择概率，动态分配最优位宽，从而优化计算资源分配。为了增强硬件兼容性，使用ReLU 替代 GDN 作为非线性单元，并对权重采用固定量化比特以避免额外的比特流传输。实验结果表明，该方法在降低计算复杂度和存储开销的同时，能更好地保证主观重建质量，满足低延迟场景下的实际应用需求。

**关键词：**视频编码，深度学习，可变码率，模型量化

With the widespread adoption of high-definition video, video compression technology has become increasingly important in multimedia storage, transmission, and processing. Traditional video coding standards based on hybrid coding frameworks have achieved high compression efficiency, but their reliance on handcrafted feature extraction has led to a performance bottleneck. In recent years, deep learning has driven the development of end-to-end video coding, surpassing traditional standards in rate-distortion performance. However, existing methods often overlook the non-uniform characteristics of human visual perception, resulting in limited content adaptability. Moreover, their flexibility in rate adjustment is constrained—most methods rely on fixed quantization strategies and require multiple models to accommodate different bandwidth requirements, increasing deployment costs. In addition, the high computational complexity limits practical applications in low-latency scenarios, making it difficult to balance compression efficiency and real-time performance. To address these challenges, this work focuses on content-aware video coding optimization. The main contributions are as follows:

​    (1) A content-aware variable bitrate video coding method is proposed, which adjusts the bitrate allocation between ROI and non-ROI regions based on the non-uniform sensitivity of the human visual system. To enable continuous rate adjustment, a hierarchical bitrate regulation mechanism is designed across global, channel, and local levels. At the global level, a conditional convolution module enables discrete rate levels; at the channel level, latent features are scaled channel-wise with exponential interpolation to achieve fine-grained continuous rate adjustment; at the local level, a saliency detection network is used to adjust distortion weights between regions, enhancing perceptual quality in important areas. Experimental results show that the proposed method achieves lower bitrates while maintaining high visual quality and supports continuous rate adaptation within a single model, providing greater flexibility across different bandwidth constraints.

​    

​    (2) A saliency-aware quantization method for deep video coding is proposed to address the limitations of existing quantization schemes in exploiting content dynamics. A region-adaptive quantization strategy is introduced based on visual saliency. Specifically, a saliency detection model is used to generate masks that divide each frame into salient and non-salient regions. A lightweight bitwidth allocator is designed to estimate optimal bitwidths from a set of candidates, based on input features, optical flow gradient magnitudes, and channel-level statistics. To enhance hardware compatibility, ReLU is used in place of Generalized Divisive Normalization (GDN) as the nonlinearity, and fixed bitwidths are applied to weights to avoid extra bitstream overhead. Experimental results demonstrate that the method reduces computational and storage costs while preserving subjective reconstruction quality, making it suitable for low-latency scenarios such as mobile streaming and video conferencing.





## 绪论

### 研究背景及意义

近年来，随着互联网的迅猛发展，全球视频数据正经历前所未有的爆发式增长。据 Cisco 视觉网络指数（VNI）报告[]统计，当前视频流量已占据全球互联网总流量的 82%，预计到 2025 年，每日产生的视频数据量将突破 7.5 EB（Exabyte），相当于每分钟上传 超过 50 万小时的视频内容。这一趋势主要归因于以下两方面因素：（1）视频采集与显示设备的升级，推动视频数据向 超高清（UHD）、4K/8K、高帧率（HFR）、高动态范围（HDR） 方向发展；（2）视频通信需求的激增，短视频、直播、视频会议、远程监控等应用场景迅速普及，使得人们对视频内容的需求日益增长。然而，海量视频数据的指数级增长给存储与传输造成巨大压力，迫切需要更高效的视频压缩技术来优化视频存储与传输效率。

过去四十多年间，已经发展出多代视频编码标准。2003年，ITU-T视频编码专家组（VCEG）和ISO/IEC运动图像专家组（MPEG）的专家成立了联合视频小组（JVT），共同开发H.264/AVC，该标准目前已广泛应用于标清和高清视频中。2013 年，H.265/HEVC（High Efficiency Video Coding）标准正式发布，相较于 H.264/AVC，编码效率提升近一倍，在 4K/8K 视频场景下展现出更优越的压缩性能。随着超高清（UHD）、虚拟现实（VR）、增强现实（AR）等高质量视频应用的快速发展，2020 年，H.266/VVC（Versatile Video Coding）标准问世，相较于 H.265/HEVC 进一步提升 50% 的编码效率，在超高清视频、VR/AR 及沉浸式媒体应用广泛。尽管传统视频编码标准不断发展，但编码效率的提升速度远低于视频数据量的指数级增长，已陷入性能提升瓶颈。此外，传统编码器采用模块化设计思路，将视频压缩分解为预测、变换、量化、熵编码等独立环节。这种人工设计的模块之间相互依赖，难以进行联合优化：运动估计模块依赖穷举搜索导致计算复杂度陡增，离散余弦变换难以适应视频内容多样性，码率调整与质量评估环节存在割裂。

深度学习技术的崛起为突破传统编码瓶颈提供了全新路径。端到端编码模型通过可微分神经网络架构，实现了编码全流程的联合优化，使各环节参数能够基于率失真目标进行端到端训练，从而提升整体压缩效率。2019年，首个端到端的视频编码模型 DVC（Deep Video Compression）被提出，其核心思想是利用深度神经网络逐步替代传统视频编码框架中的手工设计模块，较 H.265/HEVC 实现了较大幅度码率节省；随后，微软提出 DCVC（Deep Conditional Video Coding），引入条件编码框架以替代传统的残差编码方式，并结合超先验熵模型学习多层次先验信息，在DVC的基础上进一步提升了编码效率。

针对低延迟场景下的视频编码模型设计，常常面临一个关键挑战：模型需要在资源受限的移动设备上部署。现有深度学习视频编码方法普遍存在计算复杂度高、内存占用大的缺陷，难以满足移动设备的实时处理需求。基于以上挑战，本文将根据视频帧的内容特性对现有的深度学习视频编码方法进行优化，核心思想是在基于条件上下文的视频编码框架上，根据人眼感知视频内容的非均匀特性，优化码率调节机制，在保证视频的视觉高质量感知的前提下减少码率传输；引入区域显著性感知的动态量化方案，自适应分配量化位宽，对关键区域和非关键区域分配不同的计算资源，保证高质量视觉感知的同时减少不必要的计算资源消耗。

本文提出基于内容感知的编码框架，对现有的深度学习视频编码方法进行优化。首先构建显著区域感知的视频编码压缩框架，引入分级可变码率调节机制，通过多粒度的码率调控，对关键区域分配更多比特，并实现了单一模型的连续可变码率调整，可在不损失视觉质量的同时，节约视频编码所需比特数，提升编解码效率、降低存储成本；除此之外，实现显著区域感知的编码模型量化，设计轻量化的位宽分配器对显著性和非显著性区域自适应分配最优量化位宽，实现在有限资源设备的更高效部署与推理。

### 国内外研究现状

深度学习视频编码技术是建立在图像编码的基础之上的，可以视为图像编码的扩展和延伸。两者的核心目标均是实现高效数据压缩，以减少存储和传输的带宽需求，同时尽可能保持较高的视觉质量。图像编码主要通过去除空间冗余来压缩单帧图像，而视频编码不仅需要帧内压缩，还需进一步利用帧间时序的相关性去除时间冗余，从而提升压缩效率。因此，本章首先回顾端到端图像编码的研究历史及其最新进展，分析关键技术与发展趋势。随后，将深入探讨深度学习端到端视频编码的研究现状，剖析其核心方法与挑战，以全面理解这一领域的前沿进展及未来应用前景。

#### 深度学习图像编码方法研究现状

图像压缩技术的核心目标是以尽可能少的比特数表示图像，同时保持较高的视觉质量，以满足存储和传输的需求。传统图像编码方法主要依赖于手工设计的信号处理技术，典型的编码标准包括 JPEG[]、JPEG2000[]、WebP[]、BPG[]（Better Portable Graphics）等。这些方法通常采用变换编码（如离散余弦变换 DCT 或离散小波变换 DWT）、量化、熵编码等步骤来减少数据冗余，并通过预测、变换和统计建模等手段优化压缩效率。然而，传统方法存在一些局限性，在低比特率条件下，由于变换编码和量化的作用，传统压缩方法易产生块状伪影，影响图像质量；重要的边缘和纹理信息可能会被丢弃，导致图像模糊。为消除块效应和恢复细节，一些研究者提出了多种基于去块（Deblocking）和去噪（Denoising）的方法[Efﬁcient image deblocking based on postﬁltering in shifted windows, Image postprocessing by non-local kuans ﬁlter, A generic post-deblocking ﬁlter for block based image compression algorithms, Adaptive non-local means ﬁlter for image deblocking, Concolor: constrained non-convex low-rank model for image deblocking]，但这些方法通常涉及计算代价高昂的迭代优化，难以满足实时应用的需求。深度学习技术的发展为图像编码技术的提升提供了新思路，相较于固定的数学变换和手工设计的量化策略，能够自适应地提取图像的多尺度特征，避免对高频细节的过度损失，从而在低比特率下依然保持较高的视觉质量；此外，端到端优化的图像编码可以对整个编码系统进行联合优化，避免了手工设计模块间优化目标不一致的问题。

当前深度学习端到端的图像编码方案可以分为两类，包含基于自编码器（Auto-Encoders，AE）的压缩方案和基于生成对抗网络（Generative Adversarial Network，GAN）的压缩方案。基于自编码器的图像压缩算法通过非线性变换网络对图像数据进行变换和反变换，编码器将图像映射为低维潜在空间的隐表示，解码器从隐表示中恢复原始图像。2016年，Ballé 等人提出了第一个基于自编码器的端到端图像编码框架[]，分为三个部分：非线性分析变换（编码器），均匀量化器和非线性合成变换（解码器），由于合成变换是分析变换的逆操作，网络中所有参数都根据率失真目标端到端地联合优化；受生物神经元抑制机制启发，在编码器与解码器中，使用广义除数归一化层[Density modeling of image using a generalized normalization transformation]（Generalized Divisive Normalization，GDN)作为网络的归一化层，相比于常见的批归一化层（Batch Normalization, BN），GDN具有更强的非线形能力，更适用于图像压缩的非线性变换。在端到端的图像压缩中，量化（Quantization） 是压缩过程中不可或缺的环节，它的作用是将编码器提取的连续潜在表示（Latent Representation） 映射到离散符号空间，以便于熵编码。然而，由于量化操作的不可微性，训练过程中梯度无法在反向传播中更新。为了解决这一问题，Ballé[] 在训练过程中使用加性均匀噪声来模拟量化过程，在推理过程直接进行量化取整操作。Ballé 等人的方法编码效果优于JPEG2000。在[balle]框架的基础上，Ballé 等人[VARIATIONAL IMAGE COMPRESSION WITH A SCALE HYPERPRIOR]进一步优化了熵模型，引入尺度超先验以捕捉潜在变量的空间相关性，提高了熵模型的表达能力，实现了与H.265/HEVC帧内编码接近的率失真性能。Minnen等人[Joint Autoregressive and Hierarchical Priors for Learned Image Compression]在超先验[VARIATIONAL IMAGE COMPRESSION WITH A SCALE HYPERPRIOR] 和单一高斯尺度混合模型（GSM, Gaussian Scale Mixture） 的基础上，进一步优化了熵建模，引入自回归先验和高斯混合模型（Gaussian Mixture Model, GMM），形成联合自回归和分层先验熵模型，以更好地捕捉潜在表示的概率分布，是第一个在PSNR和MSSSIM测度上均优于 H.265/HEVC 帧内编码的端到端图像编码研究。

由于自回归先验[Joint Autoregressive and Hierarchical Priors for Learned Image Compression]熵模型解码过程中需要严格的串行扫描顺序，这种顺序限制了并行化处理，导致编解码复杂度过大，且不利于并行优化，在实际部署的应用场景中难以实现实时性，因此，一些研究针对熵模型优化进行了改进。Minnen 和 Singh []提出通道级自回归熵模型，采用通道条件化和潜在残差预测减少序列处理需求，解决了传统的自回归上下文模型需要逐像素地进行序列处理的问题，在低比特率下相较于BPG编解码器可节省25%的码率。He 等人[Checkerboard Context Model for Efficient Learned Image Compression]提出了棋盘格上下文模型，采用两步棋盘格上下文计算，消除了空间位置上的限制，解决自回归上下文模型在解码过程中需要严格扫描顺序的问题，实验结果显示，在几乎不影响率失真性能的情况下，与基线模型相比解码速度提高近 40 倍。在上一篇的基础上，He等人[ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding]基于隐变量的能量分布并不均匀性，设计了非均匀分组的通道上下文，与棋盘格空间上下文模型结合，打破了串行解码的限制，提高了并行计算能力，相比均匀通道上下文解码速度提升了近25%。

一些研究针对GDN层进行了改进。由于卷积层的感受野有限，针对局部冗余信息比较敏感，而难以去除全局的空间冗余，为了解决这一问题，[End-to-end learnt image compression via non-local attention optimization and improved context modeling]引入非局部注意力机制来捕捉全局特征，提高了压缩模型对图像长距离依赖的建模能力。[Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules] 使用离散高斯混合模型来参数化潜在编码的分布，并在非线形变换网络中引入了注意力模块，以提升对复杂区域的空间冗余捕获能力，在PSNR指标上达到了与H.266/VVC相当的性能。[Enhanced Invertible Encoding for Learned Image Compression] 提出了一种可逆变换的编码方法，引入可逆神经网络构建图像与潜在特征空间的双向变换，并且在网络中添加密集连接的残差块，增强编码网络的非线性表达能力。[]中提出基于块匹配的参考搜索方法，在解码过程中逐点搜索当前特征全局最相关的特征，通过全局匹配的参考特征进行 条件概率估计，提高熵模型的预测精度；传统GDN仅通过乘法归一化 进行特征调整，可能导致均值偏移问题，影响特征表达能力，引入减法运算纠正均值偏移问题，使得特征表示更加稳定。

基于自编码器的图像压缩方案主要以峰值信噪比 PSNR 和多尺度结构相似性 MS-SSIM作为训练目标，在低比特率情况下面临较为明显的质量退化问题，因为它们倾向于精确保留局部结构，而忽略全局语义信息和纹理细节[Extreme Learned Image Compression with GANs]。为了解决这一问题，研究者开始探索基于GAN的图像压缩方法，以弥补自编码器压缩方案在极低比特率下的感知质量损失。GAN 通过对抗训练的方式，使得生成器（解码器）能够在极低比特率条件下合成缺失的细节，并提高重建图像的视觉真实性。相比于传统方法仅优化像素级损失，GAN 结合对抗损失（Adversarial Loss）和感知损失（Perceptual Loss），能够更好地保留全局语义和局部纹理，使重建图像在感知质量上更加逼真。2017年，Santurkar等人[Generative Compression]首次在图像编码中引入生成式压缩（Generative Compression）的概念，利用生成模型进行压缩，可以在更深的压缩级别上，为图像和视频数据提供更逼真的重建；一些工作将对抗损失应用于图像patch来进行伪影抑制[Deep generative adversarial compression artifact removal，Real-time adaptive image compression]，[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network]通过度量生成图像与原始高分辨率图像之间的感知相似性，生成更逼真的纹理细节。Agustsson等人[Generative adversarial networks for extreme learned image compression]提出了首个基于 GAN 的端到端极端图像压缩框架，引入选择性生成式压缩和多尺度对抗损失，极低比特率下的视觉质量优于传统压缩方法，特别适用于移动端、在线传输和云存储等高压缩需求场景。在此框架的基础上，Mentzer等人[High-fidelity generative image compression] 优化了生成器和判别器的网络结构，针对[Generative adversarial networks for extreme learned image compression]中存在的重构图像往往仅保留高层语义，但与实际输入有较大误差的问题，设计了主观更优的感知训练损失函数，在相同的重建图像主观质量约束下，该方法的码率消耗仅为 H.265/HEVC 帧内编码工具BPG的一半，显著提高了压缩效率。2023年，Agustsson 等人在[Generative adversarial networks for extreme learned image compression]的基础上，提出了一种自由控制主客观需求的端到端图像编码方案[Multi-realism image compression with a conditional generator]，该方法引入 条件生成器，使解码阶段能够根据用户需求生成不同真实感程度的图像。通过调整条件变量，解码端用户可以在低失真但真实感较弱与高真实感但失真较大的重建图像之间灵活选择，从而实现更具适应性的图像压缩。基于PSNR指标率失真性能接近H.266/VVC标准的参考软件VTM，而基于主观指标时，率失真性能大幅优于VTM。

#### 深度学习视频编码方法研究现状

传统视频编码方案主要依赖一系列人工设计的模块来减少视频序列中的冗余，包括时间冗余、空间冗余和统计冗余，从而减少存储和传输所需的比特数，实现视频压缩。典型的编码标准包括H.264/AVC、H.265/HEVC和最新的H.266/VVC等。传统视频编码方法基于手工设计的规则，如运动估计、变换编码和熵编码等，而这些人工设计的模块之间通常是相互独立的，在优化时难以兼顾整个系统的协同作用，因此改进往往局限于单一模块。这种局部优化方式可能会对其他模块的性能产生影响，难以实现全局最优。深度学习技术的发展为视频编码技术的提升提供了新思路，研究者开始将目光转向基于深度网络的端到端视频编码。端到端视频编码方法采用率失真损失函数对整个系统进行联合优化，不仅能够同时考虑各个模块之间的依赖关系，还能提升整体的压缩效率。

当前端到端深度视频编码解决方案主要分为两类，基于残差的编码和基于条件的编码。基于残差的编码网络设计思路来源于与传统编码类似的混合编码框架，具体而言，首先根据运动补偿生成预测帧，然后计算预测帧与当前帧的残差并进行编码。2019年，G Lu等人[26]首次提出了端到端优化的深度视频压缩模型（Deep Video Compression，DVC），利用神经网络替代传统视频编码中的关键模块，包括运动估计、运动补偿、运动信息和残差信息编码，它通过光流网络预测运动信息，使用变分自编码器（VAE）进行编码，整体性能可媲美 H.265/HEVC。M-LVC[]改进了[26]中仅利用前一帧作为参考的局限性，利用多帧参考进行运动估计和补偿，生成更精确的运动矢量，减少残差信息。[Improving Deep Video Compression by Resolution-Adaptive Flow Coding]针对光流信息的编码进一步改进，之前的编码方案所有区域的运动信息都使用相同的分辨率进行存储和编码，但实际场景中，不同区域的运动复杂度不同，因此提出分辨率自适应光流编码，根据不同区域的运动复杂度来动态调整光流编码分辨率，以减少码率传输。[Content Adaptive and Error Propagation Aware Deep Video Compression] 中关注长视频帧序列中的误差累积问题，提出误差传播感知训练策略以减少错误累计，并根据场景复杂度动态调整编码策略，在线更新编码器参数，以应对不同视频场景下的视频特性。为了解决光流网络与编码网络的不适配问题，一些工作针对运动估计进行了改进。[Neural Video Coding Using Multiscale Motion Compensation and Spatiotemporal Context Model] 建立多尺度光流场来捕捉不同尺度上的运动信息，使运动补偿更加准确，并结合时间和空间信息来进行熵编码，实现更精确的比特估计。Agustsson等人[Scale-space Flow for End-to-End Optimized Video Compression]提出了尺度空间流（Scale-space Flow, SSF）的运动估计方法，优化了传统基于光流的方法易在快速运动或复杂场景下失效的问题，显著提高了运动估计和运动补偿的准确性。

而除了像素域的编码框架，一些研究方案关注到在特征域进行编码有助于提升率失真性能。2020年，Feng等人[Learned Video Compression with Feature-level Residuals]首次提出了特征级残差压缩框架，由于像素空间的残差编码对于光流的运动补偿预测误差较为敏感，因此提出在特征域进行残差编码，可以进一步提升压缩性能。2021年，Hu Z等人提出一种特征域视频压缩框架FVC[]，区别于传统像素域的视频编码，该方法首先将视频帧转换为特征表示，然后在特征空间中进行运动估计、运动补偿和残差编码，并利用可形变卷积进行运动补偿。由于特征空间比像素域更易于建模，FVC 能够在低比特率条件下进一步去除时域冗余，提供更高的压缩效率。[Learned Video Compression with Feature-level Residuals]中主要探讨在特征空间进行残差计算和压缩，与传统像素级残差计算方法相比，特征级残差更能有效去除时域冗余，结合不同尺度的特征信息，使残差预测更加精准。

上述的工作大多集中于探索更优的网络结构和如何生成更优的隐变量，没有关注优化熵模型的分布估计。深度学习视频编码通过熵模型估计编码后隐变量的概率分布，并基于该分布进行熵编码，从而最小化数据表示所需的比特数。现有的端到端视频编码网络大多直接采用图像编码网络中的解决方案，如超先验熵模型[Variational image compression with a scale hyperprior.] 和自回归先验熵模型[Joint autoregressive and hierarchical priors for learned image compression.] 一些工作中针对利用空间和时间相关性来优化熵模型。例如，[Conditional entropy coding for efficient video compression.] 提出了条件熵编码，不在帧间进行任何的显式变换，每一帧都被单独编码，仅通过对图像隐变量之间的熵进行建模，在MSSSIM指标上优于H.265/HEVC。[dcvc]通过超先验模型学习层次先验，自回归网络学习空间先验，并结合时间先验模型编码器来探索时间相关性，从而准确进行分布估计；[dcvc-tem]优化了时间上下文信息的提取，设计多尺度时间上下文编码器，并且移除了[dcvc]中自回归熵模型，以构建并行化友好的解码器，实现了更快的编解码速度。[ Learning for Video Compression with Recurrent Auto-Encoder and Recurrent Probability Model]基于循环自编码器（Recurrent Auto-Encoder, RAE）和循环概率模型（Recurrent Probability Model, RPM）设计了循环熵模型，RAE 负责提取视频帧间的时序特征，而 RPM 则用于建模概率分布，提高熵编码的精度。结合这两个模块，该方法能够 自适应建模长时间的视频序列，减少比特率开销。[dcvc-hem]针对[dcvc]中的自回归先验进行了改进，采用低复杂度的双重空间先验有效捕获帧间的空间依赖关系，在UVG数据集上较H.266/VTM可实现18.2%的码率节省。

残差编码假设当前像素仅与预测像素相关，通过对当前帧 $x_t$ 与预测帧 $\tilde{x}_t$ 之间的残差进行编码，这种方法较为简单直接，广泛应用于编码框架中。但理论上，当前帧中的一个像素与之前解码帧的所有像素以及当前帧中已解码的像素都有相关性，而残差编码仅通过手工设计的减法操作来消除帧间冗余，未能充分利用帧间和帧内的复杂相关性。与残差编码相比，条件编码具有更低或相等的熵界[Optical Flow and Mode Selection for Learning-based Video Coding.]，如公式xxx所示：
$$
H(x_t-{\tilde{x}_t)\geq H(x_t|\tilde{x}_t)}
$$
其中，$H$ 表示香农熵。条件编码利用已知的先验信息或上下文信息，对当前数据进行条件概率建模，从而更精确地预测和表示数据。相比于传统的残差编码，条件编码通常具有更低或相等的熵值，这意味着在相同的压缩比下，条件编码可以实现更高的压缩效率或更优的重建质量。已有研究对特定模块的条件编码进行了探索。例如，[Conditional Coding for Flexible Learned Video Compression] 利用条件编码在潜在域融合参考帧信息，而非传统的残差域，从而提升在参考帧缺失（如 I 帧）情况下的鲁棒性。[Optical Flow and Mode Selection for Learning-based Video Coding] 以预测帧 $\tilde{x}_t$ 作为先验信息，对复杂区域进行条件编码，而对其他区域直接复用预测帧。[1, 2] 需要显式地将视频内容划分为“跳过模式”和“非跳过模式”，仅对“非跳过模式”的内容应用条件编码，这种方法依赖于手工设计的规则对视频内容进行预处理，而不是端到端的自动优化。在[Conditional entropy coding for efﬁcient video compression]中，每一帧均由独立的图像编码器压缩，条件编码仅用于帧间隐变量的熵建模，且由于缺乏光流估计和运动补偿（Motion Estimation and Motion Compensation，MEMC），该方法难以去除视频序列中的时间冗余，导致压缩相对较低，在 PSNR 指标上无法超越 DVC[]。[Learned Video Compression] 仅在编码器端使用条件编码，而解码器仍采用残差编码。总体来看，上述研究的条件编码主要针对特定网络模块（如熵模型或编码器），或依赖手工设计的规则来筛选哪些内容应进行条件编码。基于这些问题，微软在2021年提出了第一个系统性的条件编码网络框架，在编码器、解码器和熵概率模型中全面集成了条件编码，无需手工划分视频内容，该框架基于特征域MEMC进行上下文建模，具备更强的表达能力。在 1080p 测试数据集上，相比 H.265/HEVC 实现了 25% 的码率节省，有效提升了视频压缩效率。相比于DCVC基于已重建帧进行时间上下文建模，[DCVC-TCM]直接从传播的特征中挖掘时间上下文，可以精准地建模长时间依赖关系，精准建模长时依赖关系，并将学习到的时间上下文填充至多个关键压缩模块，使编码器更充分地利用时间信息。在此基础上，[DCVC-HEM]进一步优化了熵模型，引入双重空间先验以建模空间冗余，为解决自回归熵模型推理速度慢的问题，采用棋盘格（Checkerboard）两步编码方案，提高了并行计算能力。在 UVG 数据集 上，该方法相比 H.266/VTM 可节省 18.2% 的码率。2023年，[DCVC-DC]在上述研究的基础上进一步优化，动态调整帧级失真权重，引导模型周期性生成高质量参考帧和特征，并引入四叉树分区熵编码，相比传统棋盘格模型，引入更细粒度的空间与通道相关性，最终相比 DCVC-HEM，BD-Rate 进一步节省 23.5%，同时 MACs（乘加计算量）减少 19.4%。

### 论文研究内容

本文从人眼视觉感知特性出发对现有的深度学习视频编码方法进行内容感知优化，针对针对现有端到端视频编码模型在内容自适应性不足和低延迟场景下计算开销较高的问题，提出两项关键优化策略。本文的主要工作如下：

1. 提出显著区域感知的可变码率视频编码框架。本文设计了一种分级可变码率调控机制，涵盖三个层次的码率调节策略：全局级基于码率控制参数实现粗粒度的离散码率调整，通道级通过特征缩放参数实现细粒度的连续码率调节，局部级则通过区域比特分配优化 ROI 和非 ROI 区域的码率比例。 首先，利用显著性检测网络自动识别视频帧中的 ROI 区域，并调整 ROI 与非 ROI 区域的重建质量平衡参数，以优化区域间的码率分配。随后，引入条件卷积模块，以全局率失真权衡参数作为条件输入，实现全局级的离散码率调节。进一步地，对上下文编码得到的隐变量的通道冗余特性进行可视化分析，基于此特性，引入通道级自适应缩放单元，结合指数插值方法，实现更精细的码率控制。最后，通过实验分析所提方法的率失真性能和重建可视化效果，实验结果表明，该方法在低码率下能有效提升人眼感知质量，并在不同码率需求下展现出更灵活的适应性和更优的压缩性能。
2. 提出显著区域感知的动态网络量化方法。针对不同区域的人眼关注度特性，设计了一种自适应量化策略，在降低计算复杂度的同时，保证显著区域的重建质量。 首先，通过显著性检测模型生成 ROI 掩膜，将视频帧划分为ROI区域和非ROI区域。随后，设计轻量级位宽分配器，根据输入区域的特征、运动信息的梯度幅值和通道级标准差，计算候选位宽的选择概率，并自适应分配最优量化位宽。ROI 区域采用高比特量化以保留细节，而非 ROI 区域采用低比特量化以减少计算负担，从而实现计算资源的合理分配。同时，为了简化硬件实现，采用 ReLU 替代广义除数归一化作为非线性单元。最后，通过实验分析率失真性能、计算复杂度和重建可视化效果，结果表明，该方法能够在降低计算复杂度和存储开销的同时，有效保证人眼视觉感知质量，适用于移动端和低延迟场景的视频编码部署。

### 论文结构安排

本文的章节结构安排如下：

第一章：绪论。本章首先介绍了本文研究工作的背景及意义，随后回顾了深度学习图像编码和深度学习视频编码的国内外研究现状。接着，引出本文的主要研究内容，概述所提出的内容感知编码优化框架的核心思路与结构。最后，对本文的章节结构进行了简要介绍。

第二章：相关理论基础。本章首先介绍了视频编码的基本理论，随后对传统视频编码技术框架及其核心模块进行了详细分析，包括运动估计与补偿、变换与量化、熵编码等关键技术。最后，以两个经典的端到端视频编码模型为例，分别阐述了残差编码和条件编码的原理与方法，为第三章和第四章的研究内容奠定理论基础。

第三章：内容感知的可变码率视频编码。本章首先介绍了内容感知的可变码率视频编码框架，详细阐述了基于显著性感知优化的编码模型在编码端和解码端的具体流程，并对损失函数、码率估计和特征量化进行概述。随后，介绍分级可变码率调控机制，分别论述全局级、通道级和局部级的码率控制原理及实现方法。最后设计实验并进行结果分析，以验证所提方法在实现灵活码率调节和优化人眼感知方面的有效性。

第四章：显著区域感知的视频编码模型量化。本章首先介绍了显著区域感知的深度视频编码模型量化框架，详细阐述了模型编码和动态量化机制的具体流程。随后，提出特征动态量化方案，并分析位宽分配器在ROI 区域与非 ROI 区域的候选位宽选择原理及其实现方法。之后，介绍模型训练的渐进式训练策略，并详细说明损失函数设计与差错控制机制。最后，设计实验并对结果进行分析，以验证所提方法在降低计算复杂度和保持量化后重建质量方面的有效性。

第五章：总结与展望。对全文内容进行总结和梳理，分析本文工作的不足，并展望未来可以进一步进行的研究方向。



## 相关理论基础

视频编码技术是现代多媒体通信与信息处理的重要支撑，其理论基础对于理解编码原理、提升压缩效率具有重要意义。随着高清视频及实时传输需求的不断增长，如何在有限带宽条件下实现高效视频压缩，成为当前研究的关键问题之一。本章主要从视频编码的基本理论出发，探讨数据压缩原理和率失真理论；在此基础上，介绍传统编码技术框架的主要内容，重点分析运动估计、变换与量化、熵编码等核心环节的实现方式与优化策略。最后，针对基于深度学习的端到端视频编码技术，重点介绍其中的残差编码框架与条件编码框架。通过对上述理论方法的归纳分析，为后续视频编码系统的设计与优化提供理论依据与技术参考。

### 视频编码基本理论

#### 数据冗余性与压缩原理

视频信号是由时间上连续的图像帧序列构成的动态视觉信息载体，其基本特性由分辨率、帧率、比特深度和色彩空间等核心参数定义。分辨率表征单帧图像的像素密度，例如1920×1080（1080p）表示每帧包含207万像素；帧率（Frames Per Second, FPS）描述每秒传输的帧数，典型值包括24 FPS（电影标准）、30 FPS（广播电视）和60 FPS（高动态场景应用）。比特深度决定了每个像素的色彩精度，8-bit深度可表示256级灰度或色阶，而10-bit深度支持1024级分层，能够显著减少色彩断层现象。色彩空间则定义了亮度与色度信号的分离方式，例如YUV 4:2:0格式通过下采样色度通道（UV分量采样率为亮度的1/4），利用人类视觉系统对色度敏感性较低的特性实现数据量压缩。未经压缩的视频数据量可表示为公式xx：
$$
\mathrm{Data~Size}=W\times H\times F\times B\times T
$$
其中，$W$和$H$分别为图像的宽度与高度(像素)，$F$为帧率(FPS)，$B$为每像素比特数，$T$为视频时长(秒)。以1080p分辨率(1920x1080)、24 FPS帧率、8-bit色深、60秒时长的视频为例，其原始数据量为：$1920\times1080\times24\times8\times60=2.39\times10^{10}\mathrm{~bits}\approx239\mathrm{~Gb}$，庞大的数据量对存储介质和传输带宽提出了极高要求。因此，高效压缩技术是视频应用商业化的关键条件。

在视频信号的表示与传输过程中，数据冗余是影响存储和带宽需求的关键因素。未经压缩的原始视频数据量极为庞大，但其中包含大量的冗余信息，可以通过高效的编码技术加以消除，从而实现数据压缩。视频编码的核心目标之一便是去除这些冗余信息，以最低的比特数表示尽可能高质量的视频内容。根据冗余信息的不同来源，可以将视频数据冗余分为空间冗余、时间冗余和信息熵冗余。空间冗余源于单帧图像内像素值的空间关联性，相邻像素的亮度与色度数值往往高度接近，直接记录每个独立像素值会产生大量重复信息。为消除空间冗余，采用预测编码与变换编码技术：在预测编码中，利用已知像素来预测当前像素，只编码预测误差以减少信源熵；变换编码将像素块从空域转换至频域，使能量集中于低频分量，而高频细节（如细小纹理或噪声）可通过量化策略选择性舍弃。时间冗余来源于相邻帧间数据的高度相关性，相邻帧往往包含相同或类似的背景和运动物体，只不过运动物体所在的空间位置略有不同，使用帧间预测编码消除时间冗余：首先在参考帧中搜索与当前帧宏块最匹配的区域，计算运动矢量；随后仅传输运动矢量与残差数据（当前块与参考块的差异）。对于静态场景，残差数据趋近于零，可直接跳过编码。信息熵冗余指在表示图像数据时，由于实际使用的比特数超过了理论最小值（信息熵）而产生的冗余 。根据信息论，表示一组数据所需的最小平均比特数等于该数据的熵值 $H$。熵体现了数据源的信息含量或不确定性，即无损编码时理论上的最低码长。如果能根据符号出现的概率分布来编码数据，那么平均每个符号所需的比特数将接近熵 $H$。此时编码效率最高，几乎没有冗余。然而，在实际对图像数据进行编码时，通常并不完全了解符号的真实概率分布，实际分配给各符号的码字长度往往多于其信息论最优长度，平均每个符号的比特数 $\bar{L}$ 大于熵值 $H$。信息熵冗余正是指这种实际码长超出熵限所产生的多余比特开销，可用差值 $\bar{L} - H$ 来度量。采用熵编码来消除熵冗余，对量化后的符号进行无损压缩，尽可能接近信息熵极限。

#### 率失真理论

在信息论中，互信息衡量了两个随机变量之间共享的信息量，在信源编码中用于评估压缩过程中保留的原始信息量。设设 $X$ 为原始信源，$\hat{X}$ 为经过压缩和解码后的信号，则 $X$ 与 $\hat{X}$ 之间的互信息定义为：
$$
I(X;\hat{X})=H(X)-H(X|\hat{X})
$$
其中，$H(X)$ 表示信源 $X$ 的信息熵，衡量其不确定性；$H(X | \hat{X})$ 表示在收到 $\hat{X}$ 的情况下，$X$ 剩余的信息量。互信息越大，说明 $\hat{X}$ 能够更有效地保留 $X$ 的信息，从而减少重建失真。因此，在视频编码中，互信息是衡量编码质量的关键指标之一。

在视频编码中，率失真权衡是核心问题，其本质是通过数学优化方法，在压缩码率（Rate）与重建质量（Distortion）之间寻求最优平衡。码率指的是单位时间内存储或传输的比特数，通常以kbps（千比特每秒）或Mbps（兆比特每秒）为单位；而失真指的是压缩后视频与原始视频之间的质量损失，常用均方误差（MSE）、峰值信噪比（PSNR）、结构相似性指数（SSIM）或多尺度结构相似度（MS-SSIM）等指标来衡量。 

率失真理论由香农（Claude Shannon）于1948年提出，其核心思想是通过信息论量化信源编码的极限性能。对于视频信号，率失真函数 $R(D)$ 定义为在允许的平均失真 $D$ 下所需的最小码率 $R$，可表示为：
$$
R(D) = \inf_{p(\hat{X}|X)} I(X;\hat{X}) \quad \text{s.t.} \quad \mathbb{E}[d(X,\hat{X})] \leq D
$$
其中，$p(\hat{x}|x)$表示信源$X$与重建信号$\hat{X}$之间的条件概率分布(即编码方案)，$I(X，\hat{X})$为$X$与$\hat{X}$的互信息，表征所需码率，$d(X,\hat{X})$表示失真度量，$\mathbb{E}[·]$ 表示数学期望，约束条件要求平均失真不超过阈值 $D$。然而，实际编码器无法直接求解这一理论极限，而是通过拉格朗日优化 (Lagrangian Optimization)将问题转化为最小化代价函数来实现：
$$
L=\lambda D+R
$$
其中 $\lambda$为拉格朗日乘数，用于动态调节码率与失真的权重。编码器需要在多个编码方案（如不同的帧内/帧间预测模式、量化参数、变换方法等）之间进行选择，以最优地平衡码率和失真。率失真曲线可表示为图xxx，从图中曲线可知 xxxxx

<img src="/Users/liujiamin/Library/Application Support/typora-user-images/image-20250306220117063.png" alt="image-20250306220117063" style="zoom:40%;" />

#### 失真度量

失真度量用于评估原始视频和重建视频之间的差异，以帮助调整编码参数，优化视频质量和压缩率之间的平衡。失真通常由压缩过程中的量化、变换、运动补偿等环节引入，影响最终的视频观感。 根据评估目标的不同，失真度量可分为主观指标与客观指标。主观指标依赖于人眼的感知评价，常应用于极低比特下的基于GAN的视频或图像重建。客观指标 通过数学计算衡量失真程度，例如均方误差（Mean Squared Error, MSE）、峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）、结构相似性指数（Structure Similarity Index Measure，SSIM）和多尺度结构相似性指数（Multi-Scale Structural Similarity，MS-SSIM）。本小节主要介绍客观指标的计算方法。 

**MSE** 

MSE是最基础的失真度量方法，表示为原始图像与重建图像像素值差异的平方均值：
$$
\mathrm{MSE}=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N\left(x_{i,j}-\hat{x}_{i,j}\right)^2
$$
其中，$x_{i,j}$和$\hat{x}_{i,j}$分别为原始图像与重建图像在像素位置$(i,j)$的值，$M\times N$为图像分辨率。MSE 具有直观的物理意义和较低的计算复杂度，但其数值范围受图像像素范围影响较大，导致跨场景对比困难。不同图像或位深的 MSE 取值差异显著，无法直接比较，例如，MSE=100在8-bit图像中可能是严重失真，但在16-bit图像中可能失真极小。因此进一步引入PSNR来度量图像的失真。

**PSNR**

PSNR是 MSE 的对数变换版本，将图像的最大灰度值视为信号的峰值功率，将编码后MSE视为噪声功率，以两者的比值来衡量图像的相似程度，通过峰值像素值 $MAX_I$对MSE进行归一化，将其转换为对数分贝（dB）形式：
$$
\mathrm{PSNR}=10·\log_{10}\left(\frac{MAX_I^2}{\mathrm{MSE}}\right)
$$
尽管 PSNR 被广泛应用于 H.264、H.265 等编码标准的率失真优化，但仍存在局限性。PSNR 对于块效应等局部失真的感知能力较弱，仅能反映整体图像质量，而人眼对局部失真更为敏感。这可能导致前景区域 PSNR 较低、背景区域 PSNR 较高的情况，使得实际观感质量较差，与主观视觉体验不匹配。

**SSIM**

为了克服PSNR的局限性，进一步提出SSIM[]的失真度量方法，其基于人类视觉系统特性进行感知质量评估，认为人眼更关注图像的结构信息而非像素级误差，并从图像的亮度、对比度和结构三个维度比较图像相似性。

亮度相似性比较两幅图像的平均亮度，定义为：
$$
l(x,\hat{x})=\frac{2\mu_x\mu_{\hat{x}}+C_1}{\mu_x^2+\mu_{\hat{x}}^2+C_1}
$$
对比度相似性比较两幅图像的标准差，定义为：
$$
c(x,\hat{x})=\frac{2\sigma_x\sigma_{\hat{x}}+C_2}{\sigma_x^2+\sigma_{\hat{x}}^2+C_2}
$$
结构相似性比较两幅图像的协方差，定义为：
$$
s(x,\hat{x})=\frac{2\sigma_{x\hat{x}}+C_3}{\sigma_x\sigma_{\hat{x}}+C_3}
$$
其中 $x$ 和 $\hat{x}$ 分别表示原始图像和重建图像，$\mu_x$、$\sigma_x$分别为原始图像的均值和标准差，$\sigma_{x\hat{x}}$表示原始图像和重建图像的协方差，$C_1,C_2,C_3$代表不同常数，避免分母为零。最终SSIM值定义为三个相似度的乘积：
$$
\mathrm{SSIM}(x,\hat{x})=l(x,\hat{x})\cdot c(x,\hat{x})\cdot s(x,\hat{x})
$$
SSIM的取值范围为$[0,1]$，值越接近1表示两幅图像越相似。相较于PSNR，SSIM考虑了人眼视觉特性，能更准确地反映图像质量，并且对局部结构信息更加敏感，与人类视觉感知更一致。

**MS-SSIM**

在实际应用场景中，SSIM仅针对固定尺度计算，无法反映多分辨率下的视觉感知差异。因此，在SSIM的基础上，MS-SSIM提供了SSIM 的扩展版本，模拟人类视觉系统在不同观察距离下的多尺度信息感知能力，通过在多尺度上计算 SSIM 以提高质量评估的准确性。MS-SSIM 采用高斯金字塔对图像进行多尺度分解，并在不同尺度上计算 SSIM，最终结合所有尺度的信息来得到总体质量分数。其计算流程如下：

1. 尺度生成：通过低通滤波与下采样生成多尺度图像序列 (如原始尺度、1/2尺度、1/4尺度)；
2. 逐尺度计算：在每个尺度上计算亮度、对比度与结构相似性，并加权融合，可表示为：

$$
\mathrm{MSSSIM}(x,\hat{x})=\prod_{k=1}^K\left[l_k(x,\hat{x})\right]^{\alpha_k}\cdot\left[c_k(x,\hat{x})\cdot s_k(x,\hat{x})\right]^{\beta_k}
$$

其中，$K$为尺度总数，$\alpha_k$和$\beta_k$为各尺度的权重系数。相较于SSIM，MSSSIM具有多分辨感知能力，低尺度反映全局结构，高尺度捕捉细节纹理，更符合人眼观察习惯；并且相较于PSNR对模糊、噪声、压缩伪影等不同失真类型具有更高的判别灵敏度。

### 传统视频编码技术

#### 传统视频编码框架

传统视频编码的核心目标是在尽可能低的码率下提供高质量的视频，同时兼顾计算复杂度和编码效率。为了实现这一目标，主流视频编码标准，如ITU-T 发布的 H.26x系列编码标准均采用了基于块的混合编码框架，包含帧内预测、运动估计与补偿、变换编码、量化和熵编码等基本模块，整体的编解码流程基本一致。本节以主流的编码标准 H.265/HEVC 为例，编码框架如图xxx所示。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/%E4%BC%A0%E7%BB%9F%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E6%A1%86%E6%9E%B6.jpg" alt="传统视频编码框架" style="zoom:50%;" />

编码流程中首先按照设定参数将视频序列划分为多个图像序列组（Group of Pictures, GOP） ，每个 GOP 由不同类型的帧组成，包括 I 帧（Intra Frame）、P 帧（Predicted Frame）和 B 帧（Bidirectional Frame）。I 帧为关键帧，仅使用帧内预测进行压缩，不依赖其他帧。P 帧通过前向参考帧进行预测，仅存储预测误差和运动信息。B 帧同时利用前后参考帧进行双向预测，在编码效率上比 P 帧更优，但解码复杂度更高。GOP 结构直接影响视频压缩性能，短 GOP 适用于低延迟场景，长 GOP 能在存储和传输上获得更高的压缩比。

在每一帧内部，HEVC 采用分层分块结构进行处理。视频帧首先被划分为编码树单元（Coding Tree Unit, CTU），然后进一步划分为编码单元（Coding Unit, CU）、预测单元（Prediction Unit, PU） 和变换单元（Transform Unit, TU）。CU 负责决定预测方式， PU 具体执行预测操作，TU 执行变换编码。对于 I 帧，编码器利用已编码的相邻像素进行帧内预测；对于 P 帧和 B 帧，编码器通过运动估计和运动补偿技术进行帧间预测，利用时间冗余减少存储需求。

获取预测帧后，编码器计算预测帧与原始帧间的残差，并进行变换编码，采用离散余弦变换（Discrete Cosine Transform，DCT），将残差信号转换到频域，使信号能量更集中于低频分量，以便于后续的量化处理。之后，变换后的系数经过量化过程，通过量化步长（Quantization Parameter，QP）对系数进行舍入，以减少存储比特数。QP 值越高，压缩比越高，但失真也越明显，编码器采用率失真优化来在码率和视频质量之间找到最佳平衡。量化后的变换系数进行熵编码以进一步压缩数据。HEVC 采用上下文自适应二进制算术编码（Context-Adaptive Binary Arithmetic Coding，CABAC），通过分析数据的概率分布，动态调整编码方式。最终，编码后的比特流由多个部分组成，包括帧类型、预测模式、运动矢量、变换系数、量化参数和熵编码数据，形成符合 HEVC 规范的压缩格式。

#### 运动估计与补偿

帧间预测核心是运动估计与补偿（Motion Estimation and Motion Compensation，MEMC）。运动估计是在参考帧的一定范围内搜寻当前帧某个块的参考块，从而计算出该块的运动矢量（Motion Vector, MV），如图xxx所示。运动估计通常采用块匹配算法，即将当前块与参考帧中的不同位置的块进行匹配，以找到最佳匹配位置。匹配准则常采用均方误差MSE，计算当前块与候选块之间的像素差异，选取误差最小的匹配块作为运动补偿的参考。HEVC 进一步改进了块匹配方法，支持不规则的预测单元（PU）划分，使得不同运动特性的视频内容可以适应不同大小和形状的运动块。运动估计获取块匹配的运动矢量后，编码器通过运动补偿生成预测帧，基本原理是利用参考帧中的匹配块来生成当前帧的预测值，即根据运动矢量对参考帧的像素进行插值计算，得到预测像素值。预测帧与当前帧的残差、MV信息输送到解码端重构后，解码端生成预测帧与解码后的预测残差相加即得到重建图像。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20250307205905657.png" alt="image-20250307205905657" style="zoom:50%;" />

#### 变换与量化

在视频编码过程中，变换与量化是去除数据冗余、降低码率的关键步骤。变换的主要作用是将残差信号从像素域转换到频域，使得信号的能量更加集中，从而减少高频信息的存储需求。而量化则是对变换后的系数进行有损压缩，进一步降低数据量，同时允许编码器在码率和失真之间进行权衡。在 HEVC 编码框架中，变换与量化共同作用，使得压缩后的数据能够在有限的码率下保持尽可能高的重建质量。

变换通常使用离散余弦变换，它可以将图像块的像素值表示为不同频率的基函数的加权和。DCT 的核心思想是将信号的能量集中到低频分量，从而使人眼不敏感的高频分量可以通过量化被有效舍弃，减少存储需求。对于一个 $M \times N$ 的残差块 $R(m,n)$，DCT 可以表示为：
$$
F(u,v)=c(u)c(v)\sum_{m=0}^{M-1}\sum_{n=0}^{N-1}R(m,n)\cos\frac{(2m+1)u\pi}{2M}\cos\frac{(2n+1)v\pi}{2N}
$$

其中，$F(u,v)$ 是变换后的频域系数矩阵，$R(m,n)$ 是变换前的残差，$c(u), c(v)$ 是归一化因子，以确保变换的正交性。在 HEVC 中，TU的大小可以是 4×4、8×8、16×16 或 32×32，编码器会根据残差信号的能量分布选择最合适的块大小，以达到最优的压缩效果。变换虽然改变了残差信息的表示方式，但所包含的信息量并没有减少，因此需要进行量化来进一步压缩信息。量化的基本原理是利用人眼视觉系统对细节的不敏感性，舍弃对感知质量影响较小的高频分量，将连续信号映射为一组离散值，降低了编码所需的符号个数。HEVC 采用均匀标量量化，其数学表达式为：
$$
Q(u,v)=\mathrm{round}\left(F(u,v)/Q_{step}\right)
$$
其中，$Q(u,v)$ 是量化后的系数，$Q_{step}$ 是量化步长。$Q_{step}$ 的大小由量化参数QP决定，QP 越大，量化步长越大，意味着更多的高频信息被舍弃，压缩率提高。在解码过程中，为了恢复原始信号，需要执行反量化和逆变换。反量化用于将量化后的系数映射回接近原始系数的值，其计算公式如下：
$$
\hat{F}(u,v)=Q(u,v)\times Q_{step}
$$

#### 熵编码

经过预测、变换和量化的信息仍存在信息冗余，这种冗余主要来源于变换和量化后的数据分布特性，即某些符号的出现频率较高，而另一些符号较少出现。因此可以通过熵编码转换为二进制码流，减少数据存储和传输所需的比特数。熵编码利用符号出现的概率分布，为高频出现的符号分配较短的码字，而为低频符号分配较长的码字，以使得整体编码长度最小化。熵编码的基础来源于信息论中的香农定理[]，它定义了无损压缩所能达到的最小平均码率，提供了数据压缩的理论极限。在信息论中，自信息[]是用于衡量单个事件的信息量大小的概念。它描述了某个符号的罕见程度，出现概率越低的符号携带的信息量就越大，而出现概率越高的符号携带的信息量就越小。对于一个离散信源 $X$ 中的某个符号 $x_i$，自信息定义为：
$$
I(x_i)=-\log_2p(x_i)
$$
其中，$p(x_i)$ 是符号 $x_i$ 出现的概率。信息熵[]表示信源中所有可能符号的平均自信息量，即该信源的不确定性程度。假设一个信源 $X$ 由 $n$ 个可能的符号 ${x_1, x_2, ..., x_n}$ 组成，并且每个符号 $x_i$ 的出现概率为 $p(x_i)$，则该信源的信息熵定义为：
$$
H(X)=-\sum_{i=1}^np(x_i)\log_2p(x_i)
$$
信息熵表示信源在无损压缩条件下所需的最小平均比特数。在视频编码中，数据经过预测、变换和量化处理后，利用熵编码进一步压缩，生成更紧凑的比特流，以尽可能低的码率存储或传输，同时最大程度地保留原始信号的信息。

### 本章小节



本章介绍了视频编码的基本理论框架，重点阐述了视频信号的构成特性、冗余类型与压缩原理、率失真优化理论，传统视频编码的核心流程与关键技术以及深度学习端到端视频编码技术框架。首先从视频信号的基本特性入手，介绍了视频数据结构在分辨率、帧率、比特深度与色彩空间的定义，并分析了视频数据中的三类典型冗余。在此基础上，引入了率失真理论，阐述了编码过程中码率与失真之间的平衡问题，并基于信息论提出互信息与率失真函数的概念。接着，介绍了常见的图像/视频质量评估指标，比较了它们在感知质量评估上的差异与适用场景。之后，本章概述了以H.265/HEVC为代表的传统视频编码框架，详细介绍了帧内/帧间预测、运动估计与补偿、变换量化及熵编码等关键模块的基本原理与实现方式。最后，本章介绍了近年来基于深度学习的端到端视频编码方法解决方案，并重点对比了基于残差的编码网络与基于条件上下文的编码网络。



### 端到端视频编码技术

当前端到端深度视频编码解决方案主要分为两类，基于残差的编码和基于条件的编码。基于残差的编码网络压缩原始帧和预测帧之间的残差信息，解码端使用预测帧+解码的残差来恢复原始帧。基于条件的编码利用已知的先验信息或上下文信息，通过额外的条件信息优化编码策略。本节将以DVC[]和DCVC[]为例，分别介绍端到端的残差编码和条件编码技术框架。

#### 基于残差的编码

DVC是首个端到端优化的视频残差编码网络，借鉴传统视频编码的预测编码结构，并利用神经网络的非线性表示能力来统一优化运动补偿和残差压缩，DVC框架如图xxx所示，编码端包含图中所有模块， 解码端则不包含蓝色模块。DVC的编解码器网络模块与传统编码框架的各模块对应，包括光流估计、运动补偿、残差编码和熵编码，所有网络参数通过率失真损失函数联合训练。通过端到端的联合优化，DVC 在相同目标码率下显著提升了重建帧的质量。实验结果表明，DVC 的压缩性能超越传统 H.264，并可达到 H.265 标准。DVC的编解码流程主要如下：

运动估计（光流提取）：DVC采用SpyNet[Optical ﬂow estimation using a spatial pyramid network]光流估计网络替代传统的块匹配运动估计方法。相较于块匹配的运动估计，SpyNet 通过级联的金字塔结构逐层优化光流估计，能够更精细捕捉视频帧间运动信息。给定前一解码帧 $\hat{x}_{t-1}$ 和当前帧 $x_t$，SpyNet 预测逐像素的光流场 $v_t$。 随后通过MV 编码器对光流进行压缩，将光流映射至低维隐表示 $m_t$，随后进行量化得到 $\hat{m}_t$，再经解码器解码得到重建光流 $\hat{v}_t$。

运动补偿：获得重建光流 $\hat{v}_t$ 后，采用像素级运动补偿获得当前帧的预测$\bar{x}_t$。首先，重构光流 $\hat{v}_t$ 与 上一重构帧 $\hat{x}_{t-1}$ 通过 $\text{warp}$ 操作后，通过细化网络后得到预测帧 $\bar{x}_{t}=\text{warp}(\hat{x}_{t-1},\hat{v}_t)$。由于采用逐像素的光流补偿，预测帧能避免传统块匹配带来的块效应，无需额外的环路滤波纠正。

残差编码：计算输入帧 $x_t$ 与预测帧 $\bar{x}_t$ 之间的残差 $r_t = x_t - \bar{x}_t$，然后对残差进行编码。DVC 使用深度残差自编码器来替代DCT等变换，将残差信号映射到潜在表示 $y_t = f_{\text{enc}}(r_t)$，量化后得到 $\hat{y}_t$，最后经解码器重建残差 $\hat{r}_t = f_{\text{dec}}(\hat{y}_t)$。非线性变换网络能够更有效地提取残差的高阶相关性，相比DCT获得更高的压缩效率。预测帧与重建残差相加得到重建帧：$\hat{x}_t = \bar{x}_t + \hat{r}_t$  。在实际编码中，量化后的MV隐变量 $\hat{m}_t$ 和残差隐变量 $\hat{y}_t$ 会经过熵编码器输出比特流，并在解码器进行解码。

#### 基于条件的编码

DCVC 提出了首个端到端的深度上下文视频编码网络，采用条件编码技术框架，其整体架构如图 XXX 所示，其中黄色区域表示解码端模块。与传统残差编码方法通过计算残差完成图像重建不同，DCVC不传输残差信息，而是利用高维特征域上下文信息作为额外的条件信息，在编码、解码和熵模型中全面融合时空先验信息，显著提升了压缩效率。该方法通过特征域的运动估计与补偿生成上下文，能更有效地重建高频细节。实验表明，在1080P标准测试视频上，DCVC相比于x265（veryslow模式）可节省26%码率。DCVC 采用的条件编码框架的主要改进点主要为：

上下文特征提取：DCVC中，运动估计与运动信息编解码网络沿用DVC的设计思路。不同于残差编码，DCVC 采用特征域的MEMC来提取上下文。首先，对上一重建帧 $\hat{x}_{t-1}$ 通过特征提取网络后提取其高维特征 $\check{x}_{t-1}$；同时，使用运动估计预测当前帧 $x_t$ 和前一解码帧 $\hat{x}_{t-1}$ 之间的光流 $m_t$。经过运动信息解码后，重建光流 $\hat{m}_t$ 与 高维特征 $\check{x}_{t-1}$ 进行 warp 操作，并送入上下文细化网络，最终得到高维特征空间的上下文信息 $\bar{x}_t$，为后续的隐变量概率建模提供精准的先验信息。

基于条件概率的熵模型：DCVC 采用条件概率建模的熵模型，当前帧的编码过程依赖于参考帧提取的上下文信息 $\bar{x}_t$。具体而言，当前帧经过上下文编码生成的隐变量 $y_t$，其概率分布不由全局统计特性决定，而是由上下文特征 $\bar{x}_t$ 进行条件化建模，即 $p(y_t | \bar{x}_t, \dots)$，从而使编码器能够更精确地估计潜在变量的分布，提高压缩效率。为了实现更精确的概率估计，DCVC 提出一种条件熵模型，结合三种先验信息进行概率建模：第一，采用超先验[Variational image compression with a scale hyperprior]$z_t$ 网络学习层次先验，捕捉全局统计特性；第二，引入自回归熵模型[Joint]建模已解码像素的空间相关性；第三，引入特征域上下文，通过时间先验编码器提取与当前帧相关的运动模式、纹理特征等时域信息。最后，这三种先验信息通过先验融合网络进行非线性融合，动态生成每个隐变量的拉普拉斯分布参数，包括均值 $\mu$ 和尺度参数 $\sigma$，相比固定分布建模显著提升概率估计精度。条件概率模型的表达形式如下：
$$
p(\hat{y}_t\mid\hat{z}_t,\bar{x}_t)=\prod_i\mathcal{L}(\hat{y}_{t,i}\mid\mu_{t,i},\sigma_{t,i})
$$
其中，索引 $i$ 表示潜在变量在特征图中的空间位置。通过融合多种先验信息，DCVC 能够精确建模潜在变量的概率分布，使得编码器在比特分配上更加高效。相比传统的残差编码方法，这种条件概率建模方式能够更有效地降低数据的不确定性，从而在保持相同视觉质量的情况下减少比特开销。

## 内容感知的可变码率视频编码

### 引言

随着视频数据在现代社会中的广泛应用，视频压缩技术在多媒体传输、存储及处理领域中的重要性日益增加。尤其在高清视频和超高清视频的普及下，如何有效地降低视频数据的存储和传输成本，同时保持良好的视觉质量，成为了计算机视觉和信号处理领域的重要研究课题。传统视频编码标准（如H.264/AVC、H.265/HEVC）基于混合编码框架，通过运动补偿、变换编码等模块实现了较高的压缩效率，然而，这类方法依赖人工设计的特征提取，在动态视频内容适应性及感知质量优化方面存在显著局限性。

近年来，深度学习技术为视频编码领域带来了新的突破。基于神经网络的端到端视频编码框架（如DVC、FVC等）实现了超越传统编码标准的率失真性能。然而，现有的深度学习视频编码框架仍然存在一些问题，尤其是在针对视频内容特性和人眼感知特点的优化方面，仍然缺乏有效的研究和应用，普遍存在两个关键问题：其一，现有框架忽视了人类视觉系统（Human Visual System, HVS）对视频内容感知的非均匀特性，未充分考虑人眼感兴趣区域（Region of Interest, ROI）与背景区域的感知差异；其二，现有神经网络编码模型通常采用固定量化策略，难以在单一模型中实现动态码率调节，导致实际应用中需要存储多个模型参数以满足不同带宽需求。

在已有的内容感知的深度编码工作中，大多采用预训练的语义分割来增强视频编码的内容感知能力，并且集中于图像编码任务，例如[Focussing learned image compression to semantic classes for V2X applications.] [End-to-End learned ROI image compression] [End-to-end optimized roi image compression.]。通过识别视频中的人物、车辆或其他重要物体，编码器可以为这些区域分配更多的资源，以保证图像质量。然而，这种方法在处理开放世界中海量视频内容时面临显著的挑战，特别是在视频内容复杂且标注资源有限的情况下。传统的依赖于大量手动标注的语义信息的方法在实际应用中遇到了瓶颈，因为手动标注大规模且多样化的视频数据集成本高昂且耗时。此外，对于一些特定领域或罕见事件的视频内容，可能难以获得足够的标注数据来训练有效的语义分割或目标检测模型。因此，我们提出根据人眼观看视频帧的不均匀显著性来进行编码优化，由于视频的不同部分对感知质量的贡献不同，对ROI区域分配更多码率，适当降低非ROI区域的重建质量，可以保证视频整体感知质量的前提下减少码率传输。

此外，当前大部分的深度学习的可变码率工作集中于图像编码任务，如[Variable Rate Deep Image Compression With a Conditional Autoencoder] [Channel-Level Variable Quantization Network for Deep Image Compression] [Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform]。大多数深度学习视频编码方法[Deep contextual video compression.] [Conditional entropy coding for efficient video compression.] [Learning for Video Compression with Recurrent Auto-Encoder and Recurrent Probability Model.] [Temporal Context Mining for Learned Video Compression.]不能在单一模型中实现平滑的码率调整。在传统的编码方法中，码率调节通常通过调整量化参数来实现。然而，大多数视频编码模型缺乏对量化过程的灵活控制，使用固定的量化步长和率失真损失函数，在面对不同带宽和码率需求时，需要单独训练多个固定码率的模型来实现不同的压缩比和重建质量，单一模型只能基于固定码率进行编码。这种做法不仅增加了训练和部署的复杂性，带来了巨大的训练和模型存储负担，也导致了码率调节的不灵活性。而已有的一些针对视频编码的可变码率方案，如[A DEEPLY MODULATED SCHEME FOR VARIABLE-RATE VIDEO COMPRESSION] [Bit Allocation using Optimization]   [Rate Control for Learned Video Compression] 大多针对的是帧级码率分配，即根据一组视频图像序列中帧间的相对视觉重要性来估算每一帧的复杂度，帧间码率分配策略不同，帧内区域分配相同的比特，这些方法没有考虑到视觉关注的不均匀显著性，对ROI和非ROI区域采用统一的码率分配模式，无法针对视觉感知的实际需求进行优化。因此，在ROI优化的深度编码框架基础上，引入了分级码率控制方案，针对不同区域和帧级实现了连续可变的码率控制。

基于上述问题，本研究提出了一种内容感知的可变码率视频编码方案，旨在通过结合人眼显著性检测和深度学习模型的优势，提升视频编码的效率和质量。分级可变码率包含三个粒度的码率控制，首先通过显著性网络来检测视频帧中的ROI区域，通过局部码率控制参数生成加权ROI掩码，调整ROI与非ROI区域的失真权重，实现比特重分配；引入条件卷积模块，以控制全局率失真权衡的拉格朗日乘子作为条件输入，通过帧特征图的缩放和偏移实现粗粒度可变码率；引入通道级自适应量化，基于特征通道的信息分布差异性，对编码后的隐变量特征逐通道缩放以调整量化损失，并引入指数插值，实现细粒度的可变码率调节。

### 总体方案

本章提出的内容感知的可变码率视频编码框架如图xxx所示。本方案采用条件编码框架，基于深度上下文视频压缩模型DCVC，融合显著区域（ROI）感知的可变码率编码策略，构建端到端优化的深度学习视频编码。整体流程包含ROI检测与区域划分、I帧编解码、运动估计与编码、特征提取与上下文细化、隐变量生成和熵编码、解码和重构、全局码率调节以及通道自适应量化等核心模块，具体步骤如下：

1. ROI检测与区域划分
   通过显著性检测网络提取当前帧 $x_t$ 的显著性图，通过阈值分割生成二值ROI掩码 $m_t$。采用一个局部码率控制参数 $\alpha_t$ 与ROI掩码 $m_t$ 组合形成加权掩码 $m_t^{\alpha}$，与视频帧 $x_t$ 输入I帧编码器和P帧编码器共同作用。

2. I 帧编解码：

   采用Cheng等人提出的离散混合高斯似然图像编码模型[cheng2020]进行I帧编码。视频序列的第一帧 $x_0$ 和ROI加权掩码 $m_0^{\alpha}$ 共同输入I帧编码器，编码后的潜在表示 $z_0 = f_I^{enc}(x_0, m_0^{\alpha})$ 经过量化和熵编码后输送到解码端；解码端对二进制码流解码后，得到重构潜在表示 $\hat{z}_0$ 用于重建视频帧 $\hat{x}_0 = f_I^{dec}(\hat{z}_0)$。视频序列中的第一帧使用I帧编码器传输，其余帧均采用P帧编码器传输。

3. 运动估计与编码：
   对于输入帧 $x_t$，采用光流估计网络 SpyNet[] 估计当前帧 $x_t$ 与前一重构帧 $\hat{x}_{t-1}$ 之间的运动信息，得到帧间各像素对应的运动矢量 $v_t$。运动矢量 $v_t$ 与ROI加权掩码 $m_t^{\alpha}$ 共同输入运动向量编码器（MV Encoder）编码为运动信息隐变量 $g_t$ ，并向解码端传送二进制码流。 

4. 特征提取与上下文细化：

   上一重构帧 $\hat{x}_{t-1}$ 通过特征提取网络转换到特征域，得到 $\check{x}_{t-1}$，并与解码后的重构运动向量 $\hat{m}_t$ 进行 $warp(\hat{x}_{t-1}，\hat{m}_t)$ 操作，共同输入特征精调网络后得到最终的高维上下文信息 $\bar{x}_t$。

5. 隐变量生成和熵编码：
   输入帧 $x_t$、上下文信息 $\bar{x}_t$ 和ROI加权掩码 $m_t^{\alpha}$ 联合输入上下文编码网络后，得到上下文信息隐变量 $y_t$，经过超先验熵模型编码后，将二进制码流输送到解码端。

6. 全局码率调节：

   在I帧编解码器和P帧编解码器中均采用条件卷积块，采用全局码率调节参数 $\lambda_t$ 作为额外输入条件，输出激活值的缩放因子和偏移量，从而实现对特征逐通道调整；将全局码率调节参数 $\lambda_t$ 转换为 one-hot嵌入向量，作为条件卷积的输入。

7. 通道级自适应量化：

   设计一个通道调制器对上下文编码器输出的隐变量 $y_t$ 进行通道级缩放，每个通道都对应一个缩放的数值，通过简单的通道相乘，重新调整隐变量 $y_t$ 的表示。然后在量化过程中精细调整隐变量的量化损失，并通过指数插值实现连续的可变码率。

8. 解码和重构：

   在解码端，运动向量解码器（MV Decoder）将接收到的运动向量隐变量 $\hat{g}_t$ 进行解码，得到重构运动信息 $\hat{v}_t$，解码运动信息 $\hat{v}_t$ 与 与前一重构帧 $\hat{x}_{t-1}$ 通过上下文编解码处理后得到当前的解码帧 $\hat{x}_t$ 。

### 基于显著性的压缩框架

在本章提出的基于显著性的视频编码框架中，采用预训练的UNISAL（Uniﬁed Image and Video Saliency Modeling）模型作为显著性预测网络组件，由于其较小的内存占用和参数数量，可实现实时推理，适用于低延时场景下的深度学习视频编解码。网络结构如图xxx所示。相较于传统的双流架构（如SalEMA，Two-Stream），UNISAL在推理效率上表现出显著的优势。传统的双流网络通常由两个独立的分支组成，每个分支处理不同类型的信息，一个处理空间信息（如单帧图像），另一个处理时间信息（如帧间光流），通过后期融合进行特征融合。由于需要同时处理这两种模态的输入，并且分支间存在重复的骨干网络，双流结构往往面临参数冗余和计算开销更大的问题。

与此不同，UNISAL采用了单流架构，使用共享的MobileNet-V2编码器提取基础特征，并通过轻量级的旁路RNN（如GRU模块）捕捉视频序列中的时序信息，避免了传统LSTM结构带来的高计算开销，有效减少了计算资源的浪费。UNISAL的关键创新之一是通过可学习的门控机制，在单次前向传播中自适应分配计算资源。在处理静态图像时，时序处理分支被关闭，而在处理视频时，GRU模块被动态激活并与光流先验信息进行融合。因此，避免了双流架构中的大量重复计算，通过跨模态参数共享显著降低模型复杂度，提升了处理效率，更适用于资源受限的实时场景。

为量化UNISAL模型在视频编码框架中的计算开销，本研究基于HEVC Class B-D测试序列（1920x1080至416x240）、UVG 4K 视频集，在NVIDIA RTX 3090平台上进行多分辨率基准测试，并对比了DCVC编码模型在相同测试条件下的编解码时延，测试结果如表xxx所示。实验表明，UNISAL处理单帧的平均时延为1.4ms（240p）至33.1ms（4K），平均仅占编码总时延的3.82%，相比于编解码的整体时延，显著性检测的推理耗时对系统性能的影响非常微小，不会带来额外的计算负担。

\begin{table}[]
\begin{tabular}{ccccc}
\hline
数据集          & 分辨率       & 单帧平均时延（ms） & DCVC编解码单帧时延（ms） & 显著性检测耗时占比 \\ \hline
HEVC Class B & 1920x1080 & 8.2        & 452.3           & 1.81\%    \\
HEVC Class C & 832x480   & 3.5        & 72.4            & 4.83\%    \\
HEVC Class D & 416x240   & 1.4        & 21.8            & 6.42\%    \\
HEVC Class E & 1280x720  & 5.7        & 147.1           & 3.87\%    \\ 
UVG 4K       & 3840×2160 & 33.1       & 1533.2          & 2.16\%    \\ \hline
\end{tabular}
\end{table}

**显著性区域编码模型的端到端优化**

视频编码的目标是以尽可能低的比特率实现高质量的视频帧重建。本章提出的显著性视频编码模型的基本目标是实现ROI区域的更高重建质量情况下，降低视频帧的平均码率。因此，本章所提出的ROI区域编码模型的优化问题就是寻找率失真损失最低时对应的条件编码网络参数。基于率失真函数定义的优化目标（公式xxx），定义如下损失函数：
$$
\mathcal{L}_t=\lambda_t \mathcal{D}_t+\mathcal{R}_t=\lambda_t [d_{ROI} + \frac{1}{\alpha}d_{BG}]+[H(\hat{y}_t)+H(\hat{g}_t)]\\
d_{ROI} = d[m_t\odot(x_t,\hat{x}_t)]\\
d_{BG} = d[(1-m_t)\odot(x_t,\hat{x}_t)]
$$
$L_t$ 表示当前时间步$t$的总损失函数，它由帧重建失真项$D_t$和估计码率项$R_t$两部分组成。具体而言，$d_{ROI}$ 和 $d_{BG}$ 分别表示重建帧ROI和非ROI区域与原始帧 $x_t$ 相应区域的失真，$d[x_t, \hat{x}_t]$ 衡量原始帧 $x_t$ 与重建帧 $\hat{x}_t$ 之间的失真程度。失真项可以通过均方误差MSE或多尺度结构相似度MS-SSIM等指标来进行量化。$\lambda_t$ 是一个帧级的拉格朗日乘数，用于控制失真$D_t$与码率$R_t$之间的权衡。通过调整 $\lambda_t$，模型能够实现针对不同场景和网络带宽需求的码率范围调整，从而使得单一编码模型能够适应多种压缩比和重建质量要求。这种方法避免了为不同码率需求训练多个独立模型的计算开销。$\alpha$ 是局部码率控制参数，用于在ROI区域和非ROI区域之间平衡重建质量，并会间接影响区域间的比特分配。$\alpha$ 作为非帧级参数，在视频序列中的所有帧之间共享，保证对重要区域（即显著区域）进行更高质量的编码，适当降低非显著区域重建质量和编码所需比特。$H(\hat{y}_t)$和$H(\hat{g}_t)$表示用于编码隐变量$\hat{y}_t$和$\hat{g}_t$的比特数。在所提出的方法中，对于P帧编码，运动信息隐变量 $g_t$ 和上下文信息隐变量 $y_t$ 需要编码到比特流中；I帧和P帧所对应的$\lambda_t$ 参数也需通过旁路编码编写进二进制码流。

**特征量化**

在深度学习视频编码模型中，I帧变换编码的隐变量，以及P帧编码后的运动信息隐变量 $g_t$ 和上下文信息隐变量 $y_t$ 均需经过量化、熵编码后编写进二进制码流。由于量化操作是不可微分的，量化的离散性质会使得反向传播无法进行梯度更新，网络不能进行端到端优化。因此，已有相关工作提出了解决方法[Variable rate image compression with recurrent neural networks.] [Soft-to-hard vector quantization for end-to-end learning compressible representations.] [End-to-end optimized image compression.] 本章中，采用 Ballé 提出方法[End-to-end optimized image compression.]，在训练阶段添加随机均匀噪声来近似量化过程，产生一个“平滑”的近似值，具体可表示为：
$$
\hat{y_t}\approx y_t+\epsilon	,\epsilon\sim\mathcal{U}(-\frac{1}{2},\frac{1}{2}）
$$
其中，$\epsilon$ 表示在$[-0.5, 0.5]$ 之间均匀分布的随机噪声，有效模拟了量化过程中引入的扰动，从而帮助网络学习如何更有效地抑制实际的量化噪声。采用直通估计器（Straight-Through Estimator，STE）进行梯度更新，核心思想是在反向传播时直接将梯度 $\frac{\partial L}{\partial y_t}$ 传递给输入，从而绕过离散化的中间节点，使模型可进行端到端优化。在模型推理阶段，直接进行量化取整操作，即 $\hat{y}_t=round(y_t)$。

**码率估计**

在视频编码中，码率估计旨在精确预测编码后比特流的大小，以优化率失真权衡。深度学习编码框架采用概率建模对隐变量 $\hat{y}_t$ 进行优化，以确保熵编码能够尽可能接近理论最优码率。基于信息论[A mathematical theory of communication]，实际码率 $R(\hat{y}_t)$ 受隐变量的概率分布影响，其理论最小值由交叉熵决定，表达式如下：
$$
R(\hat{y}_t)\geq\mathbb{E}_{\hat{y}_t\thicksim q_{\hat{y}_t}}[-log_2p_{\hat{y}_t}(\hat{y}_t)]
$$
其中，$q_{\hat{y}_t}(\hat{y}_t)$ 为量化隐变量 $\hat{y}_t$ 的真实概率分布；$p_{\hat{y}_t}(\hat{y}_t)$ 为熵模型预测的概率分布。该公式表明，交叉熵是实际码率的紧下界，如果 $p_{\hat{y}_t}$ 完全拟合 $q_{\hat{y}_t}$，则实际码率 $R(\hat{y}_t)$可以接近最优理论码率。因此，提高熵模型概率估计 $p_{\hat{y}_t}(\hat{y}_t)$ 的准确性对于提升率失真性能至关重要。本节中采用[dcvc]的条件熵模型，结合超先验网络[]、自回归熵模型[]和时间先验编码器[]提取多种先验信息进行概率建模，并通过先验融合网络，将多种先验信息进行非线性组合，估计得到隐变量 $\hat{y}_{t}$ 的均值 $\mu_{t,i}$ 和尺度参数 $\sigma_{t,i}$。假设隐变量 $\hat{y}_t$ 服从拉普拉斯分布，其概率密度函数可写作：
$$
p_{\hat{y}_t}(\hat{y}_t|\bar{x}_t,\hat{z}_t)=\prod_i\mathcal{L}(\mu_{t,i},\sigma_{t,i}^2)*\mathcal{U}(-\frac{1}{2},\frac{1}{2})(\hat{y}_{t,i})
$$
其中，索引 $i$ 代表隐变量在特征图中的空间位置，$\mathcal{U}(-\frac{1}{2},\frac{1}{2})$ 代表均匀量化误差分布。

### 分级可变码率调控机制

已有的深度学习视频编码网络大多不支持在单一模型中实现平滑的码率调整，为了实现多种码率，需要重新调整率失真损失中的权重，训练多个固定码率的模型以适应不同的压缩比和重建质量。且现有的工作大多针对的是帧级码率分配，即根据一组视频图像序列中帧间的相对视觉重要性来估算每一帧的复杂度，帧间码率分配策略不同，帧内区域分配相同的比特，这些方法没有考虑到视觉关注的不均匀显著性，对ROI和非ROI区域采用统一的码率分配模式，无法针对视觉感知的实际需求进行优化。因此，在显著区域感知的深度编码框架基础上，引入了分级可变码率框架，针对不同区域和帧级实现了连续可变的码率调整。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20250220221450386.png" style="zoom:50%;" />

图xx展示了本章分级码率压缩框架的流程图。所提出的分级可变码率涉及三个粒度调整，由全局码率调节参数实现粗粒度的可变码率，由通道级特征缩放参数实现细粒度的码率调整，再由局部码率控制参数实现ROI和非ROI区域的码率比重调整。全局码率调节参数 $\lambda_t$ 是公式xxx定义的率失真函数中的拉格朗日平衡因子，由编码端用户进行输入控制，可输出率失真曲线上固定节点的目标码率，实现单模型的粗粒度离散码率调整；引入条件卷积，以控制全局率失真权衡的拉格朗日乘子 $\lambda_t$ 作为条件输入，生成特征级的缩放和偏移量，通过调整特征图数值分布，间接控制编码后的比特数，如高码率时增强细节保留，低码率时抑制冗余信息。通道级特征缩放基于隐变量特征通道的信息分布不均衡冗余特性，不同特征通道对于视频图像的重建质量影响不同，因此设计一组特征通道缩放参数 $(s_i,s'_i)$ 与隐变量逐通道相乘，引导网络优化通道级比特分配，为更影响图像重建的通道分配更多比特；经过缩放、量化和熵编码的隐变量传输到解码端后，需进行逆缩放变换以映射回原有的数值区间，保证重建特征的正确性，从而实现更精细粒度的码率调整。局部码率控制参数 $\alpha$ 是公式xxx定义的率失真函数中的区域特征重建损失加权参数，用于平衡ROI区域和非ROI区域之间的重建质量，并间接影响不同区域间的码率分配。$\alpha$ 作为非帧级参数，在视频序列中的所有帧之间共享，保证对显著区域进行更高质量的编码，适当降低非显著区域重建质量和编码所需比特。I帧和P帧所对应的$\lambda_t$ 参数采用16比特整数，通过旁路编码编写进二进制码流，这种额外的比特开销可以忽略不计，因为每帧只需额外传输一个整数，不会产生性能影响；局部码率控制参数 $\alpha$ 则不需要发送到解码端。

#### 全局码率调节

在所提出的分级可变码率框架中，全局码率调节基于帧级的拉格朗日乘子 $\lambda_t$ 对率失真优化目标的引导。基于条件自编码器在可变码率的图像编码任务中的应用[Variable rate deep image compression with a conditional autoencoder.]，在分级可变码率中引入了条件自编码器的设计思路，以拉格朗日乘子 $\lambda_t$ 作为条件卷积模块的条件变量与图像特征共同输入，根据设定的不同 $\lambda_t$ 值动态调整卷积输出，生成对应的控制特征图缩放和偏移的参数，网络因此可学习到在不同目标码率下视频的图像重建策略。训练阶段的 $\lambda_t$ 离散集合决定了模型可支持的码率范围上限与下限。

条件卷积模块结构如图xxx所示。为实现多码率调整，需定义一组拉格朗日乘子集合$\Lambda = \{\lambda_1, \lambda_2, ..., \lambda_n\}$，其中每个$\lambda_i \in \Lambda$ 对应一个固定的目标输出码率，即率失真曲线上的一个码率-失真平衡点。更高的 $\lambda$ 值表示更高的重建质量和更多比特传输，较低的 $\lambda$ 值则表示更低的码率且引入更多失真。

对条件变量 $\lambda_t$ 的处理包含两个关键步骤：首先将其通过独热编码（One-hot Encoding）转换为$n$维独热向量 $\lambda_t^*$ ，其中，$n = |\Lambda|$，表示预定义拉格朗日乘子集合中 $\lambda$ 值的数量。独热向量 $\lambda_t^*$ 随后输入到两个并行的全连接层：

1. 第一个全连接层将 $\lambda_t^*$ 向量映射为标量缩放因子$s' \in \mathbb{R}$，用于实现特征的缩放。
2. 第二个全连接层将 $\lambda_t^*$ 向量映射为标量偏置项 $b \in \mathbb{R}$，用于补偿不同 $\lambda$ 值下的特征分布偏移。

在条件卷积中，$\lambda$ 通过全连接层映射为缩放因子 $s(\lambda)$ 与偏置项 $b(\lambda)$，所有输出通道共享这两个参数。对于任意卷积层，其输出的特征图与映射的参数进行仿射变换，可计算为：
$$
Y=s(\lambda)\cdot\left(X*W\right)+b(\lambda)
$$
在本章所提出的显著性视频编码框架中，I帧编码器、运动信息编码器和上下文信息编码器的最后一层与相应解码器的第一层设置为条件卷积，因为其直接影响运动信息隐变量和上下文信息隐变量的特征分布，不同的特征缩放和偏移会影响隐变量的量化误差和输出码率，从而使得模型能够在不同码率级别下进行推理，实现了单模型的粗粒度离散码率调整。

#### 通道级自适应量化

在上一节中，基于全局码率调节参数 $\lambda_t$ 的动态调整实现了离散的码率控制效果，但由于所有特征通道采用相同的量化步长，模型只能实现粗粒度的码率调整。因此，受到通道注意力机制的启发[Squeeze-and-excitation networks.]，基于通道级特征缩放在图像编码任务中的应用[DCVC-HEM] [Asymmetric Gained Deep Image Compression With Continuous Rate Adaptation] ，在所提出的分级可变码率框架中引入了通道级自适应量化，通过隐变量的通道特征变换，在量化过程中更加精细地控制量化损失和信息失真；引入指数插值方法，实现在离散码率点之间的连续码率调整，并且无需额外的训练即可扩展率失真曲线至连续范围。

大部分的深度视频编码框架都忽视了隐变量特征通道的信息分布不均衡冗余特性，为了验证通道间信息分布差异对于视频图像重建的影响，在预训练的DCVC模型上对HEVC-Class xxx数据集的xxx视频序列上进行推理，获取上下文编码网络输出的高维隐变量特征 $y_t$ 并进行量化。取上下文编码得到的隐变量 $y_t$ 的前32维通道进行做定量比较，将隐变量 $y_t$ 的前32维通道依次设置为0，观察不同通道缺失情况下对最终图像重建结果的影响。图xxx和图xx分别展示了在PSNR和MS-SSIM失真测度下不同通道缺失造成的失真程度比较。图中显示，通道1、18、22和27的缺失对于图像的重建质量影响最为严重。由此可见，不同通道对于视频图像的重建质量影响不同，通道间的信息分布是不均匀的，通过在通道特征缩放以调整隐变量的量化损失，可以直接影响输出码率和重建质量。为了充分利用上述性质并灵活缩放隐变量，设计缩放单元。缩放单元中包含了一个可学习的缩放系数矩阵 $M \in \mathbb{R}^{C \times N}$，包括了 $n$ 个缩放向量 $s_i$，其中 $i=0,1,2,\dots,n-1$，且$n = |\Lambda|$，表示拉格朗日乘子集合$\Lambda = \{\lambda_1, \lambda_2, ..., \lambda_n\}$ 中的元素数量，每一个预设的 $\lambda_i$ 值对应一个缩放向量。缩放向量 $s_i$ 维度大小 $C$ 与隐变量 $y_t$ 通道数相同，可以表示为 $s_i = \{s_{i,0}, s_{i,1}, \dots, s_{i,c-1}\}$，每个通道对应一个缩放系数。$s_{i,j}$ 表示采用 $\lambda_i$ 参数时隐变量 $y_t$ 的 $j$ 通道对应的缩放系数。对隐变量进行缩放的过程即是隐变量与缩放系数在通道维度的相乘，可表示为：
$$
\bar{y}_t=y_t\odot s_i
$$
$\bar{y}_t$ 表示缩放后的隐变量。通过逐通道的特征缩放，可以精细地调整量化损失，引导网络优化通道级比特分配，为更影响图像重建的通道分配更多比特，从而实现更精细粒度的码率调整。缩放系数矩阵与整个视频编码网络联合端到端优化，以保证它们之间的兼容性。

经过缩放、量化和熵编码的隐变量 $\hat{y}_t$ 传输到解码端后，需进行逆缩放变换以映射回原有的数值区间，保证重建特征的正确性。由于量化操作的不可微性质，无法通过简单的倒数运算将缩放后隐变量映射回原有的数值区间。因此在 $\hat{y}_t$ 传输到解码网络之前，设计另一组逆缩放系数矩阵 $M’ \in \mathbb{R}^{C \times N}$ 对隐变量再一次自适应缩放，以恢复为正确的重建特征。$M'$ 与 $M$ 的缩放向量形式相同并一一对应，可以表示为 $s’_i = \{s’_{i,0}, s’_{i,1}, \dots, s’_{i,c-1}\}$。逆缩放的过程可表示为：
$$
\bar{y}'_t=\hat{y}_t\odot s'_i
$$
在训练过程中，每次迭代随机选择拉格朗日乘子集合中不同的 $\lambda_i$ 值，动态调节率失真权衡参数，优化可学习的缩放/反缩放系数，使模型能够自适应地学习在不同码率约束下的压缩特征表示。可使编码网络学习到在率失真曲线上的不同离散点的压缩能力，从而适应多种码率需求。在推理阶段，用户可根据期望的码率需求设置 $\lambda_i$ 值，编码网络自动选择对应的缩放系数以实现目标码率下的压缩。

基于指数插值在图像可变码率压缩方法中的应用[Asymmetric Gained Deep Image Compression With Continuous Rate Adaptation]，在缩放单元之间引入指数插值实现连续的可变码率。假设在目标码率 $a$ 和 $b$ 处分别有一对缩放向量 $\{s_a,s'_a\}$ 和 $\{s_b,s'_b\}$，它们满足以下关系：
$$
s_a \cdot s'_s = s_b \cdot s'_b = C
$$
其中，$C \in R^c$是一个常数向量，确保增益单位数值间隔的一致性。为了在$a$和$b$之间插值，设置实数插值系数 $l\in[0,1]$，可以进一步推理得到：
$$
s_a \cdot s'_a = s_b \cdot s'_b = C,\\
(s_a \cdot s'_b)^l \cdot (s_a \cdot s'_b)^{1-l} = C,\\
s_v = (s_a)^l \cdot (s_b)^{1-l}, \quad s'_v = (s'_a)^l \cdot (s'_b)^{1-l}	
$$
其中，$\{s_v,s'_v\}$ 是由插值生成的缩放向量对，$l$ 用于控制生成缩放向量对所对应的比特率。当 $l=0$ 或 $l=1$，码率处于离散级别，即不进行插值，仅选取预定义的码率级别。实际应用中，系数 $l$ 也需要通过旁路编码写入二进制码流。通过指数插值的方式，可以在离散码率点之间实现平滑的码率控制。插值在推理阶段进行，因此不需要额外的训练来适应不同的比特率，能够在不增加计算开销的情况下，实现更灵活的可变码率调控。

#### 局部码率控制

为了在分级可变码率框架中实现ROI区域和非ROI区域间的局部码率调整，引入了局部码率调节参数 $\alpha$，即率失真损失函数（公式xxx）中的区域重建损失加权参数，用于控制ROI区域和非ROI区域的失真权衡，$\alpha$越大，模型会优先保证ROI区域的重建质量，非ROI区域承受更高的压缩率。$\alpha$ 作为非帧级参数，在整个视频序列的所有帧间共享，不需要显式地传输至解码端参与解码，而是直接作用于编码过程中。

在所提出的显著性区域编码框架中，将显著性掩码 $m_t$ 与局部码率调节参数 $\alpha$ 结合，构建每个时间步的加权掩码 $m_t^{\alpha}$。图 \ref{framework1} 展示了加权掩码在编码端的作用过程：$m_t^{\alpha}$ 与当前时间步的输入帧 $x_t$ 或光流信息 $v_t$ 逐元素相乘后，作为输入送入编码器。具体地，加权过程通过引入一个权重参数 $\mu_{\alpha} \in (0,1)$ 实现，$\mu_{\alpha}$ 用于调整非 ROI 区域的重建质量。最终的加权掩码可以表示为：
$$
m_t^{\alpha} = m_t + (1-m_t) \times \mu_\alpha
$$
在本节中，$\alpha$ 取值范围设定为 $[1,30]$，训练阶段对每个视频序列随机采样一个 $\alpha$ 值，使模型学习不同重建质量约束下的区域码率分配。为了实现 $\alpha$ 到 $\mu_{\alpha}$ 之间的映射，采用线性变换：$u_\alpha=\frac{30-\alpha}{29}$。当 $\alpha=1$ 时，$\mu_{\alpha}=1$，表示不使用掩码，各区域重建权重相等；当 $\alpha$ 增大，$\mu_\alpha$ 逐渐减小，使得非 ROI 区域受到更强的压缩，从而优先保证 ROI 区域的图像质量。在推理阶段，可以根据目标码率、网络情况等设置 $\alpha$ 值。通过这种方式，可以保证在人眼视觉感知不降低的情况下，更大程度减少需要传输的比特。

### 实验设置及结果

#### 实验设置

**数据集** 训练数据集采用Vimeo-90k数据集。Vimeo-90k是一个用于底层视频处理的大规模高质量视频数据集，包含89800个视频片段，涵盖各种场景和动作，每个片段由连续7帧组成。在训练过程中，视频帧经过随机裁剪和翻转后生成 $256 \times 256$ 分辨率大小的视频图像。测试数据集来自联合视频专家小组（Joint Video Experts Team，JVET）制定的通用测试条件（Common Test Conditions，CTC），包括HEVC 标准测试序列的以下类别：Class B（1080P)，Class C（480P)，Class D（240P)，Class E（720P) 。此外，还选用了 MCL-JCV（1080P） 和 UVG（1080P） 数据集进行测试，以确保模型在不同分辨率和场景下的泛化性能。

**实验细节** 本章提出的内容感知编码策略应用于上下文视频压缩框架DCVC。此外，为了评估该方法相较于传统编码码框架的性能表现，在测试数据集上使用 FFmpeg 中标准编解码器 x265 和 x264 对H.265/HEVC 和 H.264/AVC进行编码测试，并采用 veryslow 模式，以评估其相较于深度编码框架的性能表现。采用BD-Rate（Bjøntegaard-Delta rate）[Calculation of average psnr differences between rd-curves] 衡量压缩比，其中负数表示相较于基准模型的码率节省，正数则表示码率增加。采用渐进式训练框架，选用Adam优化器，其中$\beta_1$和$\beta_2$分别设为 0.9和0.999；训练共分为 5 个训练阶段，每个阶段的初始学习率均设置为 le-4，当损失趋于稳定后， 学习率调整为 1e-5。训练 Batch size 设为 4。参考[DCVC]中的参数设置，拉格朗日乘子集合 $\Lambda$ 包含4个 $\lambda$ 值，$\Lambda = \{85, 170, 380, 480\}$。训练过程中，每次迭代随机选择拉格朗日乘子集合中不同的 $\lambda$ 值，通过选择 $\lambda$ 值，模型选择对应的缩放向量 $\{s_i,s'_i\}$，所选择的缩放向量将在对应的拉格朗日乘子下与整个编解码网络共同优化。在推理阶段，给定目标码率，可以通过选择 $\lambda$ 值实现大范围内的离散码率级别调整，通过调整插值系数 $l$ 控制两个离散码率级别之间的平滑过渡，码率随着 $\lambda$ 和系数 $l$ 的增大而增大。在测试过程中，在每个离散码率级别之间设置4个线性均匀分布的插值点，插值系数 $l=\{0.2,0.4,0.6,0.8\}$，最终生成多码率点的平滑率失真曲线。针对每个视频序列随机在 [1, 30] 范围内采样 $\alpha$ 值，并计算变换后的区域重建权重控制参数 $\mu_{\alpha}$。本研究采用 PyTorch 深度学习框架，并在 2张 NVIDIA RTX 3090 GPU 上完成所有实验。

**编码设置** 由于深度编码框架对编码序列的分辨率有特定要求，需确保长宽为 64 的倍数，因此对测试序列进行适当裁剪。例如，对于 HEVC Class B 1080p 分辨率视频，将其从 $1920 \times 1080$ 裁剪为 $1920 \times 1024$。对于测试数据集的GOP参数设置，HEVC数据集的dGOP设置为10，非HEVC数据集的GOP为12，均采用 I帧 + (GOP-1)P帧的编码结构。其中，I 帧采用 Cheng等人提出的离散混合高斯似然图像编码模型[cheng2020]进行编码，而P帧则使用条件视频编码模型进行编码。

**训练策略** 

本章提出的内容感知编码框架采用渐进式训练策略，并基于公式xxx定义的率失真函数进行优化，分为5个训练阶段，如表xxx所示。首先，固定其他网络模块参数，只训练运动信息（Motion Vector，MV） 编码器（Encoder，E)/解码器（Decoder，D)；之后，联合训练MV-E/D和运动估计网络（Motion Estimation，ME）；第三阶段中，固定MV-E/D和MV网络参数，开放并训练上下文编解码器（Contextual-E/D）模块网络参数；最后，开放网络所有参数，整个网络联合以端到端方式训练。遵循[OpenDVC]的训练思路，首先在完整的训练阶段基于PSNR进行训练；对于MS-SSIM失真测度下优化的模型，在基于PSNR失真测度优化的预训练模型上使用 $D_t = 1- MSSSIM(x_t, \hat{x}_t)$ 进行网络微调进行微调，重复最后两个阶段的训练步骤，以进一步优化人眼感知。

| Stage   | Training Modules |                  Loss                  | Steps | LR        | Frames |
| ------- | ---------------- | :------------------------------------: | ----- | --------- | ------ |
| Stage 1 | MV-E/D           |       $\lambda D_{warp}+ R_{mv}$       | 200k  | $1E^{-4}$ | 2      |
| Stage 2 | MV-E/D，ME       |       $\lambda D_{warp}+R_{mv}$        | 200k  | $1E^{-4}$ | 2      |
| Stage 3 | Contextual-E/D   |         $\lambda D_{recon}+R$          | 300k  | $1E^{-4}$ | 2      |
| Stage 4 | All              |         $\lambda D_{recon}+R$          | 300k  | $1E^{-4}$ | 2      |
| Stage 5 | All              | $\frac{1}{T}\sum^T{(\lambda D_t+R^t)}$ | 600k  | $1E^{-4}$ | 5      |

在前两阶段，率失真损失函数中的失真项，是参考帧 $\hat{x}_{t-1}$ 经过重构运动信息 $\hat{v}_t$ 的warp操作后得到的特征 $\ddot{x}_t$ 产生的失真，码率项是压缩运动信息 $g_t$ 的比特率：
$$
L_t = \lambda D_{warp} + R_{mv} = \lambda [d_{ROI}(x_t, \ddot{x}_t)+\frac{1}{\alpha}d_{BG}(x_t, \ddot{x}_t)] + H(\hat{g}_t)
$$
第2阶段之后，运动估计网络ME和运动编解码网络MV-E/D已收敛，固定已收敛的网络参数，开放并对条件编解码 Contextual-E/D 的网络参数进行训练。损失函数由重构帧 $\hat{x}_t$ 和原始帧 $x_t$ 之间ROI区域和BG区域的失真，以及所有隐变量的比特率组成：
$$
L_t = \lambda D_{recon} + R = \lambda [d_{ROI}(x_t, \hat{x}_t)+\frac{1}{\alpha}d_{BG}(x_t, \hat{x}_t)] + [H(\hat{g}_t)+ H(\hat{y}_t)]
$$
在前4个阶段，训练只涉及连续两个时间步的帧，忽略了当前重构帧的重构误差 $\hat{x}_t$ 对于下一帧 $x_{t+1}$ 的潜在误差影响，从而导致误差传播。基于xxxx的误差传播感知训练策略[Content Adaptive and Error Propagation Aware Deep Video Compression]，在最后一个阶段，将 $\hat{x}_t$ 作为 $x_{t+1}$ 编码阶段的参考帧，获得多个连续时间间隔的损失：
$$
L^T=\frac{1}{T}\sum_{t=1}^TL_t=\frac{1}{T}\sum_{t=1}^T\{\lambda [d_{ROI}+\frac{1}{\alpha}d_{BG}]+[H(\hat{g}_t)+ H(\hat{y}_t)]\}
$$
其中T表示时间间隔，本节中将T设置为5，即采用连续5个时间步的帧进行损失计算。



#### 实验结果

\begin{table}[]
\begin{tabular}{ccccccccc}
\hline
Model     & $\alpha$                     & MCL-JCV & UVG   & HEVC-B & HEVC-C & HEVC-D & HEVC-E & Average \\ \hline
DCVC      & /                            & 0.0     & 0.0   & 0.0    & 0.0    & 0.0    & 0.0    & 0.0     \\ \hline
Ours-Full & \multirow{3}{*}{$\alpha=10$} & 16.8    & 15.4  & 17.5   & 19.8   & 16.6   & 17.9   & 17.3    \\
Ours-ROI  &                              & -24.1   & -30.5 & -29.6  & -27.7  & -24.6  & -22.8  & -26.6   \\
Ours-non  &                              & 20.6    & 21.4  & 19.7   & 24.2   & 18.3   & 21.4   & 20.9    \\ \hline
Ours-Full & \multirow{3}{*}{$\alpha=15$} & 20.6    & 19.6  & 22.8   & 23.9   & 22.6   & 24.5   & 22.3    \\
Ours-ROI  &                              & -35.7   & -45.4 & -40.6  & -37.1  & -32.6  & -29.4  & -36.8   \\
Ours-non  &                              & 27.8    & 26.2  & 29.1   & 30.3   & 26.3   & 27.4   & 27.9    \\ \hline
Ours-Full & \multirow{3}{*}{$\alpha=20$} & 26.7    & 25.2  & 29.6   & 34.2   & 30.7   & 28.5   & 29.2    \\
Ours-ROI  &                              & -42.4   & -51.9 & -49.8  & -43.1  & -44.2  & -39.2  & -45.1   \\
Ours-non  &                              & 30.4    & 29.3  & 35.2   & 38.6   & 31.7   & 35.2   & 33.4    \\ \hline
\end{tabular}
\end{table}


\begin{table}[]
\begin{tabular}{ccccccccc}
\hline
Model     & $\alpha$                     & MCL-JCV & UVG   & HEVC-B & HEVC-C & HEVC-D & HEVC-E & Average \\ \hline
DCVC      & /                            & 0.0     & 0.0   & 0.0    & 0.0    & 0.0    & 0.0    & 0.0     \\ \hline
Ours-Full & \multirow{3}{*}{$\alpha=10$} & 18.6    & 16.2  & 17.4   & 20.2   & 19.1   & 18.9   & 18.4    \\
Ours-ROI  &                              & -23.1   & -28.1 & -31.7  & -24.5  & -27.6  & -19.8.2  & -25.8   \\
Ours-non  &                              & 21.8    & 19.6  & 28.4   & 25.6   & 19.6   & 22.7   & 23.0    \\ \hline
Ours-Full & \multirow{3}{*}{$\alpha=15$} & 24.1    & 28.2  & 23.5   & 18.3   & 21.7   & 23.0   & 23.1    \\
Ours-ROI  &                              & -35.2   & -40.1 & -42.6  & -34.5  & -32.3  & -30.2  & -35.8   \\
Ours-non  &                              & 29.2    & 24.3  & 20.2   & 24.0   & 22.4   & 26.3   & 24.4    \\ \hline
Ours-Full & \multirow{3}{*}{$\alpha=20$} & 28.4    & 22.6  & 29.2   & 30.1   & 29.9   & 27.8   & 28.0    \\
Ours-ROI  &                              & -41.2   & -48.1 & -52.2  & -42.6  & -37.4  & -32.8  & -42.4   \\
Ours-non  &                              & 31.0    & 28.9  & 30.1   & 33.5   & 29.6   & 32.4   & 30.9    \\ \hline
\end{tabular}
\end{table}

**率失真性能**

为了评价所提出的内容感知可变码率压缩算法的性能，需要考虑多个编码结果：整张帧的编码后结果、ROI区域的重建失真以及非ROI区域的重建失真。本实验中以DCVC模型作为基线模型，图xxx和图xxx分别展示了基于PSNR度量情况下，当 $\alpha=1$ 和 $\alpha=15$ 时，整张帧、ROI区域与非ROI区域的R-D曲线。当 $\alpha=1$ 时，ROI区域与非ROI区域的重建质量同等重要，从图xxx中可以观察到，基于显著性的压缩框架与基线模型DCVC在整体帧和ROI/非ROI区域上的率失真曲线都十分相近，这表明所提出的编码框架和训练方法不会对整体编码性能造成显著影响，确保了基本的编码质量。当 $\alpha=15$ 时，ROI区域的重建质量权重大于非ROI区域，模型会优先保证ROI区域的重建质量，非ROI区域承受更高的压缩率，从图xxxx中可以观察到，相比于基线模型DCVC，ROI区域的PSNR显著提升，由于ROI区域只占画面的一小部分，整体的PSNR主要受非ROI区域的影响；与基线模型相比，非ROI区域和画面整体的PSNR均有下降。图xxx展示了基于MS-SSIM度量下，$\alpha=15$ 时的R-D曲线，从图中可以观察出，ROI区域与非ROI区域的重建质量趋势与PSNR度量下的R-D曲线趋势一致。

表xxx和表xxx分别展示了在不同 $\alpha$ 取值下，基于PSNR和MS-SSIM度量的BD-Rate比较结果，以DCVC编码模型作为基准。从表xxx中可以观察到，与基线模型相比，显著性编码框架在多种数据集上的ROI区域重建后平均可实现26.6\%～45.1\%的码率节省。尽管非ROI区域和整帧的重建质量略有下降，但对人眼的主观视觉感知影响较小。随着 $\alpha$ 的增大，ROI区域的BD-Rate增益提升，$\alpha$ 值的动态调整不会造成编码模型的性能下降。在静态数据集（如 HEVC-E，视频会议类）中，ROI 编码策略的收益较低，主要原因是在静态场景中，条件编码在获取上下文隐变量的过程中，已经将大部分比特自动用于信息密度更高或纹理细节更丰富的人物面部等区域，背景区域的编码比特占用较少，因此ROI 编码器的增益较有限；而在动态场景数据集（如UVG、HEVC-B）中，ROI编码策略的增益更为突出，主要原因是，非显著性的编码框架中，背景区域的运动信息会占用更多比特，而关键区域的比特分配相对较少，ROI编码策略将比特集中于需要高质量重建的区域，不重要的区域使用更大的压缩比，从而可实现更高的BD-Rate增益。表xxx及R-D曲线对比结果表明，显著性编码框架确保ROI区域高质量重建且不降低人眼视觉感知质量的前提下，通过提升非ROI区域的压缩率，有效减少编码比特量，从而提升整体率失真性能。

由图xxx和图xxx可以观察到，所提出的分级可变码率框架可以实现平滑的速率调整。在图xx和图xx中，ROI编码框架的率失真曲线均含有4个离散的码率级别，在训练过程中改变 $\lambda$ 值来实现；离散码率级别之间的码率点在推理过程中通过改变插值系数来实现，在上述R-D曲线中，采用了4个不同的插值系数 $l$，在实际编码过程中，用户可根据目标码率需求随意设置 $l$ 的数值，以实现连续可变的码率调整。相较于[VCT_A Video Compression Transformer] [Deep Hierarchical Video Compression][Neural Video Compression with Spatio-Temporal Cross-Covariance Transformer] [DCVC-DC_Neural Video Compression with Diverse Contexts] [RATE CONTROL FOR LEARNED VIDEO COMPRESSION]所提出的框架可以实现单模型的连续可变码率调整。[RATE CONTROL FOR LEARNED VIDEO COMPRESSION]关注帧级码率分配，而xxxx，xx等框架不同的码率点需要训练不同模型。

**主观结果对比**

为了更好地验证ROI编码框架的编码性能提升，提取HEVC-B中的"BasketballDrive" 视频序列和HEVC-C中的"RaceHorses"视频序列的ROI掩码，对基准模型DCVC和ROI编码模型的重构图像进行可视化，图xxx和图xx分别为"BasketballDrive"和"RaceHorses"序列的可视化结果。基准模型DCVC分别在 $\lambda=170$ 和 $\lambda=840$ 下进行测试，ROI编码框架则在 $\lambda=85,l=0.4$ 和 $\lambda=380,l=0.4$ 两种条件下测试。第一行图像是重建后的完整帧，其中黄色框标注了部分ROI区域和部分非ROI区域；第二行图像为重构的ROI区域图像，第三行则为重构的非ROI区域图像。从图xx和图xx可以观察到，DCVC的重建结果和其右侧的ROI编码重建结果，在码率相近的情况下，ROI编码后的ROI区域重建PSNR相比DCVC，均实现了1dB以上的编码增益，虽然帧整体和非ROI区域的PSNR都有所下降，但从图中来看，无论是场景中的栏杆细节和草地的纹理，这种易被忽视区域的重建质量都不会对人眼的视觉感知造成显著影响。因此在实际编解码过程中，所提出的ROI编码策略能够在保证人眼视觉感知的质量的前提下，进一步提升压缩率，有效减少编码比特量，从而提升整体率失真性能。

### 本章小节

为了解决现有深度学习视频编码框架在内容适应性不足和码率调节不灵活的问题，本章提出了一种显著性感知的可变码率视频编码方案。现有方法在编码过程中通常采用固定码率分配策略，未能充分考虑人眼视觉系统对不同区域的关注度差异，导致ROI区域与非ROI区域享有相同的比特分配，无法有效利用有限码率资源。同时，大多数深度学习视频编码框架采用固定量化参数，难以在单一模型中实现动态码率调节，导致实际应用中需要存储多个模型以适应不同的带宽需求，增加了存储和部署的复杂性。本章针对上述问题，提出了一种结合人眼显著性检测和分级可变码率的优化方案，在保证视觉质量的同时，提高压缩率和编码灵活性。

本研究提出的分级可变码率调控机制包含三个层次的调节策略。首先，在区域级，通过显著性检测网络识别视频帧中的 ROI 区域，并利用局部码率控制参数生成加权 ROI 掩码，动态调整 ROI 与非 ROI 区域的失真权重，实现比特资源的重分配。其次，在全局级，引入条件卷积模块，以全局率失真权衡的拉格朗日乘子作为调节因子，通过对帧特征图的缩放和偏移，优化视频的整体比特分配策略，实现粗粒度的可变码率调节。最后，在通道级，采用通道级自适应量化策略，根据特征通道的信息分布差异，对编码后的隐变量进行逐通道缩放，以动态调整量化损失，实现更细粒度的码率控制。

实验结果表明，所提方法在相同感知质量下，能够有效减少视频数据的比特率，在不同带宽条件下提供更优的压缩性能。相较于现有固定码率的深度学习视频编码框架，本研究提出的方案能够在单一模型中实现连续可变码率调节，无需存储多个独立模型；并且能在不同码率级别之间平滑过渡，能够更加灵活地适应不同的带宽约束。

## 显著区域感知的视频编码模型量化

### 引言

近年来，随着深度学习技术的快速发展，端到端视频压缩技术逐渐成为视频编码领域的重要研究方向。相较于基于预测编码的传统编码标准（如H.264/AVC、H.265/HEVC），深度学习视频比那吗通过端到端优化的深度网络架构，能够更高效地挖掘视频序列中的时空冗余信息[1]。然而，现有编码模型普遍面临高计算复杂度与存储开销的挑战，尤其在低延迟应用场景（如实时视频会议、移动端流媒体）中，模型部署受到硬件资源限制的严重制约。为此，网络量化技术被广泛应用于降低模型的计算与存储需求，通过将32位浮点权重和激活值转换为低比特定点数，显著减少运算量[2]。

神经网络模型量化是一种减小模型加快运算速度的压缩方法。低比特位宽量化为定点计算，可以在降低内存功耗的同时加速运算。量化通过允许在硬件中使用精度较低的计算单元，从而减少内存瓶颈。通过用低精度定点值替换模型中的浮点权重，在不改变原始网络结构的情况下缩小模型大小。此外，在权重和激活都量化到低精度的情况下，可以使用具有更简单操作数和运算符的低精度运算，有效地实现权重和激活之间的矩阵乘法，可以显著减少嵌入式平台上的推理延迟。在不损失模型准确性的情况下，速度和功耗直接取决于执行量化的力度。神经网络模型量化通过用较低的比特宽度表示来近似网络中权重值和激活值，成为对硬件友好的方法之一。因此，使用定点乘累加（Multiply-accumulate ，MAC)操作来执行神经网络推理，可以显著降低卷积神经网络的存储开销和推理延迟。

然而，现有针对端到端视频编码的模型量化工作很少，大多数量化方案集中于图像编码任务。比如在[40,38,16]中，只对模型的权重进行量化，没有考虑对激活的量化，并不能真正降低计算成本；[16,39]同时考虑了权重量化和激活量化，但他们均依赖于推理时的动态量化，根据图像的实时统计信息确定量化参数，这需要很高的计算开销，难以满足低延迟场景的实时性需求。以上方案普遍采用静态量化策略，即对网络所有层或区域的采用固定量化比特。这种粗粒度的量化方式忽略了视频内容的动态特性与人眼视觉系统的感知偏好，导致计算资源分配失衡：一方面，对非显著区域的冗余量化增加了不必要的计算开销；另一方面，关键区域的低比特量化可能引入显著的失真，影响主观视觉质量。

在观看视频时，人眼对画面中不同区域的关注度存在显著差异。例如，快速运动的物体或复杂纹理的细节区域（如人脸、前景目标）往往属于显著区域（Region of Interest，ROI），而静态背景或平滑区域（非ROI）则对主观质量影响较小[4]。现有的量化工作没有考虑到深度学习视频编码的特点：视频片段的多样性，用相同的位宽处理不同的视频片段限制了模型的性能。受基于显著性感知方法的高效编码的启发[29,28,30]，由于人眼对实时视频的不同区域关注程度不同，在低关注区域和高关注区域使用相同的位宽无法有效分配计算资源，导致性能下降。因此，我们设想高关注区域采用更高的位宽，低关注区域采用更低的精度，这样可以在不牺牲人类关注区域精度的情况下有效地节省计算量。因此，本文旨在探索一种ROI区域感知的量化方法，实现更好的计算效率和重构质量的权衡。

针对上述问题，本章提出一种显著区域感知的动态网络量化方法，旨在根据输入视频区域的不同人眼关注度，为视频图像不同区域动态选择量化位宽，该方法可以实现关键区域的高质量重建，提高整体计算效率。具体而言，通过视觉显著性检测模型生成ROI掩膜，将视频帧划分为显著与非显著区域；设计轻量级的位宽分配器（Bit-Allocator）动态调整各区域的量化比特宽度，计算开销可以忽略不计。其中，ROI区域采用高比特以保留细节信息，而非ROI区域则采用低比特量化以降低计算负载。位宽分配器根据输入区域特征和光流的梯度幅值以及通道级标准差，来估计候选位宽的概率，从而为不同区域动态分配最优位宽。对于权重量化，采用固定位宽量化，避免了比特流中额外的量化信息传输。为保障硬件兼容性，采用ReLU替代广义除数归一化（GDN）模块，并通过知识蒸馏补偿非线性表达能力损失。在标准测试视频上的大量实验验证了该方法的有效性。本章的研究为低延迟场景下的深度视频编码网络部署提供了新的技术路径，通过结合视觉感知特性与动态量化机制，实现了资源分配的精细化控制，为移动端实时视频处理提供了可行的解决方案。

### 总体方案

本章提出的编码模型量化框架如图xxx所示。本方案基于深度上下文视频压缩模型DCVC，融合显著区域感知的动态量化策略，构建端到端优化的视频编码动态量化框架。整体流程包含运动估计与编码、ROI区域检测、动态位宽分配、特征提取与上下文细化、隐变量生成和熵编码以及解码和重构等核心模块，具体步骤如下：

1. 运动估计与编码：
   对于输入帧 $x_t$，采用光流估计网络 SpyNet[] 估计当前帧 $x_t$ 与前一重构帧 $\hat{x}_{t-1}$ 之间的运动信息，得到帧间各像素对应的运动矢量 $v_t$ 。运动信息随后被运动向量编码器（MV Encoder）编码为隐变量 $g_t$ ，并向解码端传送二进制码流。

2. ROI检测与区域划分
   采用UNISAL显著性检测模型提取当前帧 $x_t$ 的显著性图 $m_t$，通过阈值分割生成二值掩膜，分别对当前帧 $x_t$ 和 光流 $v_t$ 划分为 ROI 特征与 非ROI 特征，ROI 和 非ROI 特征在通道维度进行拼接，拼接特征通过编解码器保持分组卷积。ROI区域包含人眼关注的高动态或人脸等区域，非ROI区域主要为静态或平滑背景。

3. 动态位宽分配
   特征复杂度分析：针对ROI 和 非ROI 区域，提取其通道标准差、灰度梯度幅值均值及运动向量的梯度统计量，综合衡量区域内容复杂度。

   位宽分配：设计轻量级位宽分配器，为每个区域分别设定 K 个量化位宽候选项，位宽选择器将根据输入的复杂度信息得到的概率向量为区域分配具有最高概率分数的量化位宽，并通过可微分的Gumbel-Softmax采样实现离散决策，确保端到端训练兼容性。位宽分配器对每帧的ROI和非ROI区域分别分配位宽，分配的位宽使用2个16位整数通过旁路编码直接写入二进制码流。

4. 特征提取与上下文细化：

   上一重构帧 $\hat{x}_{t-1}$ 通过特征提取网络转换到特征域，得到 $\check{x}_{t-1}$，并与解码后的重构运动向量 $\hat{m}_t$ 进行 $warp(\hat{x}_{t-1}，\hat{m}_t)$ 操作，共同输入特征精调网络后得到最终的高维上下文信息 $\bar{x}_t$。

5. 隐变量生成和熵编码
   输入帧 $x_t$、上下文信息 $\bar{x}_t$ 经过上下文编码网络后，得到隐变量 $y_t$，经过超先验熵模型编码后，将二进制码流输送到解码端。

6. 解码和重构

   在解码端，运动向量解码器（MV Decoder）将接收到的运动向量隐变量 $\hat{g}_t$ 进行解码，得到重构运动信息 $\hat{v}_t$，解码运动信息 $\hat{v}_t$ 与 与前一重构帧 $\hat{x}_{t-1}$ 通过上下文编解码处理后得到当前的解码帧 $\hat{x}_t$ 。



### 模型量化

视频片段具有高度的内容多样性，采用相同的量化位宽处理不同类型的视频片段将限制模型的整体性能。此外，在深度视频编码模型中直接应用简单的量化方案，而未针对视频内容特性进行分析，可能会并引入较大的量化误差，并影响编码框架的帧重建质量。同时，由于人眼对实时视频的不同区域具有不同的关注度，对低关注度区域与高关注度区域采用相同的量化位宽，无法实现计算资源的合理分配，从而导致性能下降。因此，本研究旨在在保持深度学习编解码性能的同时，最大程度降低量化带来的性能损失。

在深度编码模型的常见实现中，常采用广义除数归一化（Generalized Divisive Normalization，GDN) [16]作为更适合图像重建的批归一化层（Batch Normalization，BN)。GDN层是一种用于图像和视频处理的非线性变换，其核心思想是通过局部的归一化操作来降低图像数据中各像素间的冗余度。这种归一化过程考虑了像素间的相关性，通过调整每个像素点的响应强度来实现对比度的自适应调整。这样的处理不仅能保留更多的图像细节，也使得图像更适合进行压缩编码。GDN的数学表示形式为：

$$
u_{i}^{(k+1)}(m, n)=\frac{w_{i}^{(k)}(m, n)}{\left(\beta_{k, i}+\sum_{j} \gamma_{k, i j}\left(w_{j}^{(k)}(m, n)\right)^{2}\right)^{\frac{1}{2}}}
$$
具体来说，$w^{(k)}_{j}(m,n)$  为第 $k$ 层第 $j$ 个通道在空间位置$(m,n)$处的激活值；$\beta_{k,i}$ 为可学习的偏置参数，防止分母为零并控制基础响应强度；$\gamma_{k,ij}$ 为可学习的交互权重矩阵，表征通道 $j$ 对通道 $i$ 的抑制强度。其中，分母中的 $\sum_j \gamma_{k,ij}(w^{(k)}_j)^2$ 项实现了跨通道的非线性交互。

为了简化硬件实现，本研究在每层卷积之后采用ReLU作为非线性单元，而不使用GDN。因为GDN涉及大量的浮点计算且复杂度较高。为了解决量化的不可微函数引入的梯度无法传递的问题，能够以端到端的方式优化不可微的量化过程，在反向传播阶段，使用直通估计器（Straight-Through Estimator，STE) 来保留梯度信息。具体来说，在前向传播中执行不可微的量化操作，将连续的实数值映射为离散的整数值；在反向传播中，忽略该离散化操作的不可导特性，直接将梯度从后层传递回前层，即将量化函数视为恒等映射，从而使得网络可以继续进行参数更新。

本研究基于DCVC编码模型，结合可学习步长量化模型 LSQ（Learned Step Quantization）[]构建量化基线模型。LSQ通过学习确定量化的步长 $s(b)$（即量化间隔）从而优化整个网络的量化过程。针对所有卷积权重和激活值，本研究中采用更细粒度的逐通道量化（Channel-wise Quantization)，这是由于权重和激活在不同通道之间的数值范围存在较大差异，特别是在深度超分辨、视频编码等生成模型中，需尽可能保持高质量，同时对噪声较为敏感。因此，为每个通道的权重和激活提供独立的量化参数，使其能够依据各自的数据分布独立选择最优的量化范围和零点，从而减少信息损失，提升模型精度。

对于特征量化，其量化过程可以表示为：
$$
\boldsymbol{x}_q = Q_b(\boldsymbol{x})=\lfloor\operatorname{clip}(\boldsymbol{x}\cdot\frac{s(b_x)}{\alpha_x}, -Q_N^{{b_x}}, Q_P^{{b_x}})\rceil \cdot \frac{\alpha_x}{s(b_x)},
$$
量化函数 $Q_b(x)$ 将输入特征 $x$ 量化为量化特征 $x_q$。裁剪操作 $\operatorname{clip}(z,q_1,q_2)$ 将量化值 $z$ 约束于边界 $q_1$ 和 $q_2$ 之间。$\frac{\alpha_x}{s(b_x)}$ 为可学习的量化步长，$b_x$ 表示量化位宽，$s(b_x)=2^{b_x-1}$ 表示量化后整数值的范围，用于将数值映射到量化区间，$\alpha_x$ 为可学习的尺度参数[]。采用对称量化，量化后整数范围上下界分别为 $Q_N^{b_x}=-2^{b_x-1}$ 和 $Q_P^{b_x}=2^{b_x-1}-1$，$\lfloor\cdot\rceil$ 表示四舍五入取整操作。为支持端到端训练中不可导量化过程的优化，通过直通估计器（STE）对梯度进行近似： 将舍入函数$(\lfloor\cdot\rfloor)$的梯度视为直通操作（即梯度直接回传）。

类似地，权重量化的过程可表述为：
$$
\boldsymbol{w}_q\ = Q_b(\boldsymbol{w})=\lfloor\operatorname{clip}(\boldsymbol{w}\cdot\frac{s(b_w)}{\alpha_w}, -Q_N^{{b_w}}, Q_P^{{b_w}})\rceil · \frac{\alpha_w}{s(b_w)}
$$
参考LSQ论文中的参数设置，特征和权重的可学习量化步长可初始化为：
$$
\alpha=\frac{2\cdot\mathrm{mean}(|\boldsymbol{v}|)}{\sqrt{Q_{P}^{b}}}
$$
LSQ通过计算输入数据 $v$ 的平均幅值进行初始化，而不是固定设定量化步长，使得量化范围适应具体数据分布；并且引入 $\sqrt{Q_P^{b}}$ 归一化项确保不同比特宽度下量化步长保持合理的尺度，使训练更加稳定。



### 特征动态量化

已有的量化工作通常没有考虑到视频片段的多样性，以及帧内不同区域的复杂性，大多数的量化位宽分配方法通常是静态的，没有考虑到由于人眼对于帧内不同区域的关注度不同，如果对ROI区域保留更多计算资源，并且适当降低非ROI区域的重建精度，可以在不牺牲人眼视觉感受质量的同时，降低模型整体的计算复杂度，达到模型编解码质量和计算效率间的更优权衡。为了精确分配计算资源，本研究中引入动态分配，根据输入调整关键区域和非关键区域的量化位宽。

假设 ROI 区域和非ROI 区域均有 K 个不同的量化位宽候选 $b^1，b^2，\ldots，b^K$，位宽分配器将根据制定的量化策略为输入的区域特征 $x$ 分别分配一个最优的量化位宽。每一个量化位宽 $b^k$ 对应的特征量化函数为：
$$
Q_{b^k}(\boldsymbol{x})=\lfloor\operatorname{clip}(\boldsymbol{x}\cdot\frac{s(b^k)}{\alpha^k},-Q_N^{b^k},Q_P^{b^k})\rceil\cdot\frac{\alpha^k}{s(b^k)},
$$
其中 $\alpha^k$ 表示第 k 个位宽对应的可学习尺度参数，$s(b^k) = 2^{b^k-1}$ 表示 K 个量化位宽候选项中任意一项 $b^k$ 的量化后整数范围。为了实现动态的帧内区域量化，通过位宽分配器为每一个量化位宽分配一个选择概率，因此ROI和非ROI区域的动态量化可写作：
$$
\boldsymbol{x}_{q}=\sum^KQ_{b^k}(\boldsymbol{x})\cdot P^{k}(\boldsymbol{x})\\
s.t. \sum_{k=1}^KP^k(\boldsymbol{x})=1.
$$
其中 $P^k(\boldsymbol{x})$ 表示分配给候选位宽 $b^k$ 的概率。

### 位宽分配器

在上一节中，介绍了多个候选位宽下的特征动态量化，这一节中本文将进一步分析动态位宽选择的动机以及可行性。由于深度学习量化编码网络的所输出的视频重建帧的不同区域存在不同的量化敏感度。量化灵敏度是用由模型量化而导致的帧重建能力的退化（例如，量化编码网络的重构帧和全精度网络的重构帧间的峰值信噪比（PSNR）差值）来衡量的。以基于LSQ方式的8-bit静态量化编码网络DCVC为例，设置50个视频序列，分别计算全精度DCVC和量化DCVC的重构帧PSNR差异，并分别统计原始图像的真实图像梯度幅度和特征标准差。图xxx展示了统计特征与量化敏感度的分布散点图。从图xx左图中可以观察到，图像梯度的平均大小与其量化敏感度存在相关性。结构更为复杂的区域特征表现出更高的图像梯度幅度，量化后的PSNR衰退更为明显，而结构较为简单的区域特征具有较小梯度幅度，量化后PSNR衰退较轻微。此外，从图xx右图可以发现，特征标准差与量化敏感度间也存在强相关性。具有更高特征标准差的图像在量化后具有更高的PSNR下降。已有的量化视频编码工作多采用固定比特量化方式 []，具有局限性，此外，由于量化敏感度与区域特征的复杂度有直接的相关性，因此，设计一个轻量的位宽分配器，根据区域特征的复杂度动态分配不同区域间的位宽，同时不额外增加模型计算复杂度。

位宽选择器能够为具有高量化敏感度的特征分配更高概率到高位宽，为具有较低量化敏感度分配更高概率到低比特。因为无法直接预测每个输入特征的量化敏感度，因此采用输入特征和光流的通道级标准差以及水平方向和垂直方向梯度的平均大小来综合表示特征的结构复杂度，以估计量化敏感度。



在上一节中，本文介绍了多种候选位宽下的特征动态量化方法。本节将进一步探讨动态位宽选择的动机与可行性。由于量化编码网络在不同区域的重建帧表现出不同的量化敏感度，因此固定位宽的量化方式存在一定局限。量化敏感度可通过量化引起的重建性能退化程度来衡量，常用指标包括量化模型与全精度模型在重建帧峰值信噪比（PSNR）上的差值。

以基于LSQ的8-bit静态量化编码网络DCVC为例，在50个视频序列上进行了实验，分别计算了全精度DCVC与量化DCVC的重建帧PSNR差值，并统计了对应图像区域的真实图像梯度幅度与特征标准差。图XXX展示了量化敏感度与图像特征的相关性散点图。从图xxx中左图可以观察到，图像梯度的平均值与量化敏感度之间具有明显的相关性：结构复杂区域通常具有较大的图像梯度，其量化后PSNR衰减显著；而结构简单区域则梯度幅度较小，对量化更为鲁棒。进一步，从图XXX右图中也可发现，特征标准差与量化敏感度之间存在强相关性——特征标准差越大，量化引起的PSNR下降越明显。

然而，现有大多数视频量化编码方法仍采用固定比特量化方式，无法有效适应区域间结构复杂度的差异。由于量化敏感度与图像区域特征复杂度存在直接相关性，本文提出设计一种轻量的位宽分配器，根据区域特征复杂度动态分配位宽，从而在不显著增加计算成本的前提下提高量化效率。

具体而言，位宽分配器能够根据特征的量化敏感度趋势，为敏感度高的特征分配更高位宽的概率，为敏感度低的特征分配更低位宽的概率。由于无法直接预测每个输入特征的真实量化敏感度，因此采用输入特征及其对应光流的通道级标准差，以及水平方向与垂直方向的图像梯度平均值，作为衡量结构复杂度的间接指标，进而估计量化敏感度。

 为实现动态位宽选择，本文设计了基于区域复杂度的轻量化位宽分配器。具体策略为：分别为ROI区域与非ROI区域配置独立的位宽分配器，其中ROI区域具有更高精度的候选位宽范围。受到[引用文献]的启发，我们结合帧内结构复杂度与运动复杂度，为不同区域制定相应的量化策略。对于时刻 $t$ 的输入帧区域特征 $x^R_t$，位宽分配器输出由各候选位宽对应概率 $\pi^k(\boldsymbol{x})$ 组成的概率向量，以此指导区域量化位宽的选择





为了进行量化位宽的选择，本研究设计一个轻量的位宽分配器，根据图像区域特征的复杂度来分配量化位宽，具体来说，分别为ROI区域和非ROI区域配置两个位宽分配器，ROI区域相比非ROI区域具有更高精度的位宽候选。受到[]的启发，以帧内的结构复杂度和运动复杂度分别为两个区域制定量化策略。对于 $t$ 时刻的输入帧区域特征 $x^R_t$，位宽分配器将输出由每个位宽候选项的概率 $\pi^k(\boldsymbol{x})$ 组成的概率向量：
$$
\pi(\boldsymbol{x_t^R}) = \mathrm{Softmax}{\Big(fc\big(\sigma(\boldsymbol{x_t^R}),G(\boldsymbol{x_t^R}),\sigma(\boldsymbol{v_t^R}),G(\boldsymbol{v_t^R})\big)\Big)}
$$
其中 $\sigma(\boldsymbol{x^R_t})$ 和 $\sigma(\boldsymbol{v^R_t})$ 分别表示输入特征和光流的通道级标准差，$G(\boldsymbol{x^R_t})$ 表示区域特征 $x^R_t$ 灰度图像水平方向和垂直方向梯度的平均大小；$G(\boldsymbol{v^R_t})$ 表示光流在两个通道上的平均梯度大小。这几个指标评估了图像的纹理结构复杂度和运动复杂度，复杂结构或具有高运动内容的区域将分配更高位宽。随后，将这几个指标进行拼接，拼接融合后的特征信息传递给一个全连接层 $fc: \mathbb{R}^{3+2+2+2}\Rightarrow\mathbb{R}^K$，其中K表示候选位宽的数量。之后通过Softmax后获得 $K$ 个位宽候选项对应的概率分数 $\pi^1,\pi^2,\dots,\pi^k$。最后通过 $argmax$ 选择具有最高概率分数的量化位宽：
$$
\boldsymbol{x}_{t,q}^R =Q_{b^k}(\boldsymbol{x}_{t,q}^R)=\arg\max_{Q_{b^k}(\boldsymbol{x})}\pi^k(\boldsymbol{x}_{t,q}^R).
$$
由于从离散分布$\pi(x)$的采样过程是不可微的，为了使优化问题完全可微，采用 Gumbel-Softmax[]方法来提供一个采样 argmax 的可微公式：
$$
P^k(\boldsymbol{x}_t^R)=\frac{\exp\left((\log\pi^k(\boldsymbol{x}_t^R)+g^k)/\tau\right)}{\sum_{j\in\Omega}\exp\left((\log\pi^j(\boldsymbol{x}_t^R)+g^j)/\tau\right)}
$$
$P^k(x)$表示位宽 $b^k$ 的软分配概率，$g^k$ 是从 $Gumbel(0,1)$分布中抽取的随机噪声。$\tau$ 是控制输出分布离散性的温度参数。随着 $\tau$ 接近 0，输出逼近硬最大函数，结果向量接近于 one-hot 编码。当 $\tau$ 增加时，分布变得更加均匀，意味着概率在候选项之间更加均匀地分布。根据[13]中的设定，在训练阶段，将 $\tau$ 参数初始化为 5，并通过退火过程逐渐将其降至0。

### 渐进式训练方法

深度视频编码通过率失真优化平衡压缩比和视频质量，以最小化编码后的视频码率和失真。受到[]的启发，本研究采用一个参数 $\beta$，来控制ROI和非ROI区域重建质量比例。给定ROI 掩码 $m_t$，整体的编码框架通过优化以下率失真权衡来实现：
$$
\begin{aligned}
L_t = \lambda D_t + R_t &= \lambda \left[ d_{\text{ROI}} + d_{\text{non-ROI}} \right] + \left[ H(\hat{y}_t) + H(\hat{g}_t) \right] \\
d_{\text{ROI}} &= d\left( m_t \odot (x_t, \hat{x}_t) \right) \\
d_{\text{non-ROI}} &= \beta \, d\left( (1 - m_t) \odot (x_t, \hat{x}_t) \right)
\end{aligned}
$$
$L_t$为当前时间步$t$的总损失函数，由重建失真项$D_t$和比特率项$R_t$两部分组成。$d(x_t，\hat{x}_t)$测量原始帧$x_t$和重建帧$\hat{x}_t$之间的失真。失真项可以使用均方误差（Mean Square Error，MSE）和多尺度结构相似度（Multi Scale Structure Similarity，MS-SSIM）这两个指标来衡量。$\lambda$ 是一个拉格朗日乘数，控制失真$D_t$与比特率$R_t$之间的权衡。$H(\hat{y}_t)$和$H(\hat{g}_t)$表示隐变量$\hat{y}_t$和$\hat{g}_t$的比特率。

为了弥补使用ReLU替换GDN后造成的编码模型高斯、去噪和采样能力的不足，对学生网络与教师网络之间的运动信息编码器和上下文编码器输出的比特流的分布计算其KL散度（Kullback-Leibler divergence)，并作为蒸馏损失 $L_{KD}$ 通过知识蒸馏来向低精度学生网络转移全精度网络教师模型的知识，进一步微调优化学生网络的模型性能。

| Stage   | Training Modules |                             Loss                             | Steps | LR        | Frames |
| ------- | ---------------- | :----------------------------------------------------------: | ----- | --------- | ------ |
| Stage 1 | MV-E/D           |      $\lambda D_\text{warp}+ R_\text{mv} + L_{KD}^{MV}$      | 250k  | $1E^{-4}$ | 2      |
| Stage 2 | MV-E/D，ME       |       $\lambda D_\text{warp}+R_\text{mv}+ L_{KD}^{MV}$       | 200k  | $1E^{-4}$ | 2      |
| Stage 3 | Contextual-E/D   |      $\lambda D_\text{recon}+R+ L_{KD}^\text{context}$       | 300k  | $1E^{-4}$ | 2      |
| Stage 4 | All              | $\lambda D_\text{recon}+R+ L_{KD}^{MV}+L_{KD}^\text{context}$ | 400k  | $1E^{-4}$ | 2      |
| Stage 5 | All              | $\frac{1}{T}\sum^T{(\lambda D_t+R^t+L_{KD}^{MV}+L_{KD}^\text{context})}$ | 800k  | $1E^{-4}$ | 5      |

与第三章的训练思路相似，本研究也采用渐进式训练策略训练整个编码框架，分为5个阶段，如表xxx所示。首先，固定其他网络模块参数，只训练运动信息（Motion Vector，MV） 编码器（Encoder，E)/解码器（Decoder，D)；之后，联合训练MV-E/D和运动估计网络（Motion Estimation，ME）；第三阶段中，固定MV-E/D和MV网络参数，开放并训练上下文编解码器（Contextual-E/D）模块网络参数；最后，开放网络所有参数，整个网络联合以端到端方式训练。

在前两阶段，率失真损失函数中的失真项，是参考帧 $\hat{x}_{t-1}$ 经过重构运动信息 $\hat{v}_t$ 的warp操作后得到的特征 $\ddot{x}_t$ 产生的失真，码率项是压缩运动信息 $g_t$ 的比特率，$L_{KD}^{MV}$ 项是学生网络与教师网络的运动信息编码器输出的比特流分布差异：
$$
\begin{equation}
\begin{gathered}
L_t = \lambda D_\text{warp} + R_\text{mv} \\[-0.6ex]
  = \lambda [d_{ROI}(x_t, \ddot{x}_t)+d_{non-ROI}(x_t, \ddot{x}_t)] + H(\hat{g}_t) + L_{KD}^{MV}
\end{gathered}
\end{equation}
$$
第二阶段之后，运动估计网络ME和运动编解码网络MV-E/D已收敛，固定已收敛的网络参数，开放并对条件编解码 Contextual-E/D 的网络参数进行训练。损失函数由重构帧 $\hat{x}_t$ 和原始帧 $x_t$ 之间的失真、所有隐变量的比特率、运动信息分布差异项 $L_{KD}^{MV}$ 和上下文编码器输出的比特流分布差异项 $L_{KD}^\text{context}$ 组成：
$$
L_t = \lambda D_\text{recon} + R = \lambda [d_\text{ROI}(x_t, \hat{x}_t)+d_\text{non-ROI}(x_t, \hat{x}_t)] + [H(\hat{g}_t)+ H(\hat{y}_t)] + L_{KD}^{MV}+L_{KD}^\text{context}
$$
在前4个阶段中，训练只涉及连续两个时间步的帧，忽略了当前重构帧的重构误差 $\hat{x}_t$ 对于下一帧 $x_{t+1}$ 的潜在误差影响，从而导致误差传播。因此在最后一个阶段，将 $\hat{x}_t$ 作为 $x_{t+1}$ 编码阶段的参考帧，获得多个连续时间间隔的损失：
$$
L^T=\frac{1}{T}\sum_{t=1}^TL_t=\frac{1}{T}\sum_{t=1}^T\{\lambda [d_{ROI}+d_{non-ROI}]+[H(\hat{g}_t)+ H(\hat{y}_t)]+ L_{KD}^{MV}+L_{KD}^\text{context}\}
$$
其中T表示时间间隔，本节中将T设置为6，即采用连续6个时间步的帧进行损失计算。



### 实验设置及结果

#### 实验设置

**数据集** 训练数据集采用Vimeo-90k数据集。Vimeo-90k是一个用于底层视频处理的大规模高质量视频数据集，包含89800个视频片段，涵盖各种场景和动作，每个片段由连续7帧组成。在训练过程中，视频帧经过随机裁剪和翻转后生成 $256 \times 256$ 分辨率大小的视频图像。测试数据集来自联合视频专家小组（Joint Video Experts Team，JVET）制定的通用测试条件（Common Test Conditions，CTC），包括HEVC 标准测试序列的以下类别：Class B（1080P)，Class C（480P)，Class D（240P)，Class E（720P) 。此外，还选用了 MCL-JCV（1080P） 和 UVG（1080P） 数据集进行测试，以确保模型在不同分辨率和场景下的泛化性能。

**实验细节** 本研究提出的量化策略应用于上下文视频压缩框架，包括DCVC- DC[23]、DCVC-HEM[22]和DCVC[21]。为了评估该方法相较于传统编码码框架的性能表现，在测试数据集上使用 FFmpeg 中的标准编解码器 x265 和 x264 对H.265/HEVC 和 H.264/AVC进行编码测试，并设置为 veryslow 编码模式，以提供更具代表性的比较基线。此外，为验证所提动态量化方法在不同比特宽度下的有效性，本文基于 LSQ实现了 8-bit 与 4-bit 静态量化模型，以进行R-D性能的对比分析。采用BD-Rate（Bjøntegaard-Delta rate）指标衡量压缩性能差异，用于评估不同模型之间的码率-失真（R-D）表现，其中 BD-Rate 为负值表示相较于参考模型实现了码率节省，正值则表示码率增加。采用渐进式的训练框架，选用Adam优化器，其中$\beta_1$和$\beta_2$分别设为 0.9和0.999；训练过程共划分为 5 个阶段，每个阶段的初始学习率均设置为 le-4，当损失趋于稳定后学习率调整为 1e-5。训练阶段的Batch size 设为 4。为覆盖不同的目标码率输出需求，采用 4 个不同的$\lambda$值训练 4 个模型 \{MSE：256，512，1024，2048；MS-SSIM：8，16，32，64\}。遵循[OpenDVC]的训练思路，首先在完整的训练阶段基于PSNR进行训练；在此基础上，针对MS-SSIM失真的模型，采用 $D_t = 1- MSSSIM(x_t, \hat{x}_t)$ 作为失真项对网络进行微调，重复训练的最后两个阶段，以进一步优化模型在主观视觉感知上的表现。针对控制 ROI 和 非ROI 区域之间的重建质量平衡问题，引入控制因子 $\beta$ ，用于调节非 ROI 区域的损失权重，实验中统一设定为 0.5。本文采用 PyTorch 深度学习框架，并在 2张 NVIDIA RTX 3090 GPU 上完成所有实验。

**编码设置** 由于深度编码框架对编码序列的分辨率有特定要求，需确保长宽为 64 的倍数，因此对测试序列进行适当裁剪。例如，对于 HEVC Class E 720p 分辨率视频，将其从 1280 $\times$ 720  裁剪为 1280×704。对于测试数据集的GOP参数设置，HEVC数据集GOP设置为10，非HEVC数据集GOP为12，均采用 I帧 + (GOP-1)P帧的编码结构。其中，I 帧采用 Cheng等人提出的离散混合高斯似然图像编码模型[cheng2020]进行编码，而P帧则使用条件视频编码模型进行编码。

**位宽候选** 本节实验中，为ROI和非ROI区域特征分别设置了两组量化位宽候选项，分为较低精度组和较高精度组， 较低精度的量化位宽集合为 \{非ROI: 2，3，4；ROI：4，5，6\}，平均量化位宽设置为4 bit；较高精度的量化位宽集合为\{非ROI：6，7，8；ROI：8，9，10\}，平均量化位宽设置为8 bit。对于权重，采用固定的量化位宽，与区域特征的平均量化位宽保持一致。

#### 实验结果

**率失真性能** 



\begin{table}[]
\centering
\caption{\new{BD-Rate(\%) comparison measured with PSNR. The anchor is full-precision model.}}\label{tab2}
\begin{tabular}{ccccccccc}
\hline

Model                     & Bits      & MCL-JCV & UVG  & HEVC-B & HEVC-C & HEVC-D & HEVC-E & Average \\
\hline
\multirow{5}{*}{DCVC-DC}  & 32        & 0.0     & 0.0  & 0.0    & 0.0    & 0.0    & 0.0    & 0.0     \\
                          & 8-static  & 17.2    & 16.3 & 18.2   & 21.4   & 18.0   & 12.6   & 17.3    \\
                          & 8-dynamic & 4.3     & 6.6  & 5.4    & 10.7    & 4.8    & 3.9    & 5.3     \\
                          & 4-static  & 24.3    & 27.5 & 27.2   & 29.8   & 27.3   & 19.7   & 26.3    \\
                          & 4-dynamic & 10.1    & 11.2 & 12.4   & 17.5   & 14.3   & 7.1   & 12.1    \\
\hline
\multirow{5}{*}{DCVC-HEM} & 32        & 0.0     & 0.0  & 0.0    & 0.0    & 0.0    & 0.0    & 0.0     \\
                          & 8-static  & 14.3    & 16.5 & 16.2   & 23.9   & 22.1   & 11.6   & 17.8    \\
                          & 8-dynamic & 4.2     & 4.1  & 5.3    & 11.7    & 7.2    & 4.6    & 5.5     \\
                          & 4-static  & 30.2    & 31.3 & 28.2   & 27.1   & 28.5   & 30.8   & 29.4    \\
                          & 4-dynamic & 13.4    & 11.1 & 10.5   & 18.2   & 6.6   & 14.3   & 12.4    \\
\hline
\multirow{5}{*}{DCVC}     & 32        & 0.0     & 0.0  & 0.0    & 0.0    & 0.0    & 0.0    & 0.0     \\
                          & 8-static  & 13.3    & 16.1 & 20.1   & 25.1   & 21.3   & 15.6   & 18.6    \\
                          & 8-dynamic & 6.3     & 5.9  & 6.8    & 9.1    & 5.2    & 4.1    & 6.2     \\
                          & 4-static  & 13.5    & 28.4 & 30.6   & 33.2   & 37.1   & 20.7   & 27.3    \\
                          & 4-dynamic & 7.5     & 18.4 & 13.1   & 14.0   & 10.2   & 15.3   & 13.1   \\
\hline
\end{tabular}
\end{table}





\begin{table}[]
\centering
\caption{\new{BD-Rate(\%) comparison measured with PSNR. The anchor is 8/4-bit static quantization model.} }\label{tab3}
\begin{tabular}{ccccccccc}
\hline
Model                     & Bits      & MCL-JCV & UVG   & HEVC-B & HEVC-C & HEVC-D & HEVC-E & Average \\
\hline
\multirow{2}{*}{DCVC-DC}  & 8-dynamic & -11.3   & -13.6 & -12.4  & -6.7  & -11.8  & -17.9  & -12.3   \\
                          & 4-dynamic & -12.4   & -12.5 & -13.7  & -9.8  & -10.6  & -15.4  & -12.4   \\
\hline                    
\multirow{2}{*}{DCVC-HEM} & 8-dynamic & -15.4   & -16.3 & -15.5  & -12.9  & -15.4  & -18.8  & -15.7   \\
                          & 4-dynamic & -15.6   & -14.4 & -12.7  & -13.3  & -13.8  & -17.5  & -14.5   \\
\hline                          
\multirow{2}{*}{DCVC}     & 8-dynamic & -16.9   & -15.5 & -17.4  & -11.7  & -14.8  & -18.7  & -15.8   \\
                            & 4-dynamic & -11.6   & -24.5 & -17.2  & -16.1  & -14.3  & -19.4  & -17.2  \\
\hline                          
\end{tabular}
\end{table}



为了评价所提出的显著区域感知的编码模型量化方法的性能，需要考虑多个编码结果：全精度上下文模型的编码后结果、静态量化模型的编码后结果以及基于本文动态量化方法的编码后结果。表 \ref{psnr2} 和表 \ref{ssim2} 分别展示了基于 PSNR 和MS-SSIM度量的三种上下文编码模型的 BD-Rate 比较结果，基准模型为全精度模型。从表 \ref{psnr2} 中可观察到，相较于静态量化方案，所提出的动态量化方法在各数据集上均实现了显著的率失真性能提升。进一步观察表 \ref{psnr2} 可知，静态量化在 8-bit 和 4-bit 模型中均会导致性能下降，且在 4-bit 模型中的退化现象更为明显。以全精度模型为基准，8-bit和4-bit静态量化方案分别产生17.9\%和27.7\%的码率增加，而8-bit和4-bit动态量化模型分别只产生5.7\%和12.6\%的码率增加，所提动态量化仅造成了较小的码率增加。表 \ref{psnr3} 展示了以静态量化模型为基准的BD-Rate比较结果，可以发现，动态量化方法分别在 8-bit 和 4-bit 模型上实现了平均 14.6\% 和 14.7\% 的码率节省。因此，通过所提出的动态量化位宽分配，量化编码模型的率失真性能得到了显著提升。

图 \ref{psnr_mul} 展示了三种上下文压缩模型在 8-bit 动态量化与静态量化条件下的率失真（R-D）曲线对比。图 \ref{psnr_detail} 和图 \ref{ssim_detail} 则进一步展示了 DCVC 模型在不同量化设置下的详细 R-D 曲线表现。 从图中可以观察到，8-bit 动态量化模型的率失真性能接近全精度模型，同时 4-bit 动态量化的性能也得到了显著提升，可以在低比特量化的同时保证较高人眼视觉感知质量。这表明所提出的动态位宽分配策略在低比特条件下，能够有效保障人眼感知质量。 结合 BD-Rate 对比结果与 R-D 曲线分析，所提方法在多种视频内容类型和不同分辨率下均实现了显著的性能提升。在提升推理效率、降低计算成本的同时保证更高的重建质量。

针对 DCVC 的量化后性能进行更深入的分析，并对ROI区域与非 ROI 区域的量化后R-D 性能进行比较，结果如图 \ref{roi} 所示。图中展示了 32-bit 基线模型、8-bit 动态量化、量化后ROI 区域和非 ROI 区域的 R-D 曲线。从图中可以看出，虽然动态量化后整体图像质量略有下降，但 ROI 区域的R-D性能保持接近全精度模型的水平，而非ROI区域的重建质量相对降低，这是因为由于ROI区域只占画面的一小部分，整体的重建质量主要受非ROI区域的影响。这表明位宽分配器能够精确分配计算资源，使显著的前景区域获得更多码率分配，而背景等次要区域的码率分配则相对减少。总体而言，所提出的方法在 ROI 区域实现了可观的编码增益，有效提升了视频内容在实际观看条件下的人眼感知质量。

**复杂度分析**

为了准确衡量量化后编码模型的计算复杂度，本文采用BitOps（位操作数）作为评估指标，以更好反映反映计算资源的消耗。具体而言，当前 CPU 能够并行执行按位 XNOR 和位计数（bit-count）操作，对于一个权重量化为 $b_w$ 位、特征量化为 $b_x$ 位的卷积层，其 BitOPs 可表示为：
$$
\frac{||b_x\cdot b_w||_2}{32}* 2*C_{in}*C_{out}*F^2*B*H*W
$$
其中 $C_{\text{in}}$ 和 $C_{\text{out}}$ 分别为输入和输出通道数，$F$ 表示卷积核大小，$B$、$H$ 和 $W$ 分别表示 batch size、高度和宽度。根据BitOPs计算的复杂度对比结果如表 \ref{complex} 所示。考虑到不同区域的视觉注意力显著性存在差异，ROI区域和非 ROI 区域被分配了不同的计算资源，从而在低时延场景下提高视频编解码的推理效率。相较于 32-bit 基线模型，8-bit 动态量化减少 79.3\% 的 Bit-FLOPs，同时仅造成较小的码率增长。此外，与 8-bit 静态量化模型相比，所提出的框架在 Bit-FLOPs 上减少了约 21.2\%，并显著降低了帧的重建失真。

为了更直观验证所提方法的推理加速效果，本研究基于配备支持8/4位加速的Tensor Cores的NVIDIA Tesla T4 GPU上对推理时间进行实测。在之前的实验中，采用位宽候选集合如 \{3，4，5\} 来计算理论模型性能，在实际实现中，由于3、5、7等位宽在T4上不受硬件支持，我们将ROI和非ROI区域的位宽候选集合分别替换为 \{8，16\}和 \{4，8\}，权重量化位宽固定为8-bit，推理时间测试结果如表 \ref{complex} 所示 。使用32-bit全精度模型作为基准，可以发现所提动态量化方法的编解码速度大大提升，实现了约2.4倍的FPS增长。与静态量化方法相比，动态量化的推理时间平均减少了约15.6%。上述实验证明，本文方法在低延迟编解码场景中具备显著优势。

\begin{table}[]
\centering
\caption{\new{Complexity analysis and decoding speed of DCVC-DC, DCVC-HEM and DCVC. }}\label{tab4}
\begin{tabular}{ccccccccc}
\hline
\multirow{2}{*}{Model}    & \multirow{2}{*}{Bits} & \multirow{2}{*}{Size(MB)} & \multirow{2}{*}{Bit-Flops(G)} & \multicolumn{4}{c}{FPS}    & \multirow{2}{*}{BD-Rate(\%)} \\
                          &                       &                           &                               & 1080p & 720p & 480p & 360p &                              \\
\hline                        
\multirow{3}{*}{DCVC-DC}  & 32                    & 79.3                      & 2660                          & 4.1   & 9.3  & 21.5 & 36.9 & 0.0                          \\
                          & 8-static              & 20.2                      & 671                           & 7.8   & 18.2 & 39.8 & 69.3 & 17.3                         \\
                          & 8-dynamic             & 17.8                      & 586                           & 9.1   & 20.7 & 46.2 & 77.3 & 6.2                          \\
\hline                          
\multirow{3}{*}{DCVC-HEM} & 32                    & 70.2                      & 3166                          & 3.4   & 7.7  & 18.4 & 31.2 & 0.0                          \\
                          & 8-static              & 19.4                      & 787                           & 7.2   & 15.6 & 35.4 & 60.3 & 17.8                         \\
                          & 8-dynamic             & 17.6                      & 682                           & 8.2   & 18.3 & 40.2 & 69.2 & 6.4                          \\
\hline                          
\multirow{3}{*}{DCVC}     & 32                    & 52.4                      & 2225                          & 1.3   & 3.7  & 8.4  & 13.5 & 0.0                          \\
                          & 8-static              & 14.4                      & 542                           & 2.9   & 8.1  & 17.8 & 29.7 & 18.1                         \\
                          & 8-dynamic             & 13.6                      & 462                           & 3.4   & 9.5  & 20.1 & 34.8 & 5.8            \\
\hline                          
\end{tabular}
\end{table}



**主观结果对比**

为了更好地验证位宽分配器采用的量化策略，对8位动态量化在几个数据集上的位宽分布进行了可视化，如图 \ref{visual3} 所示。对于前景复杂背景简单的HEVC-E数据集，47\%的帧分配10位到ROI区域；对于具有复杂纹理和较大运动的HEVC-C数据集，位分配器倾向于为ROI和非ROI区域分配更高的位宽度。图 \ref{visual4} 显示了HEVC-B数据集中分配给“Kimono”和“BasketballDrive”帧的比特宽度。位宽分配器根据视频的内容精确地分配位宽度。具体来说，复杂的图像、肖像或具有显著运动的场景获得更高的位宽，以提高重建质量。相反，以最小的光流变化或更简单的结构为特征的区域被分配较低的位宽以节省计算资源。这种有针对性的分配有助于在高复杂度区域保留更多的资源，有效地减少了由于累积量化误差导致的帧失真加剧。

为了更好地验证位宽分配器采用的量化策略，对8位动态量化在几个数据集上的位宽分布进行了可视化，如图 \ref{visual3} 所示。对于前景变换复杂的动态场景数据集（如HEVC-B），位分配器倾向于为ROI和非ROI区域分配更高的位宽度；而对于前景相对固定、背景简单的静态数据集（如HEVC-E），ROI和非ROI区域的平均分配位宽则更低。图 \ref{clip} 显示了位宽分配器为HEVC-B数据集中“Kimono”和“BasketballDrive”视频序列中各帧分配的比特宽度。从图中可以发现，复杂的肖像前景或具有显著运动的场景获得更高的位宽，以提高重建质量。相反，具有较小光流变化或简单结构特征的区域被分配较低的位宽以节省计算资源。这种有针对性的分配有助于在高复杂度区域保留更多的资源，有效地减少了由于累积量化误差导致的帧失真加剧。

**定义**：
$$
L = D + \lambda \cdot R
$$

- $D$：失真（Distortion），常用MSE、PSNR、SSIM等衡量
- $R$：码率（Rate），单位通常是bits或bits per pixel/frame
- $\lambda$：拉格朗日乘子（Lagrange multiplier），控制D和R的权衡程度



### 本章小节

为了解决端到端视频编码模型在计算复杂度和存储开销方面的挑战，本章提出了一种显著区域感知的动态量化方法，旨在根据输入视频区域的人眼关注度，自适应地调整量化比特宽度，以在保证重构质量的同时降低计算负担。现有深度学习视频编码框架的量化方案大多采用固定比特宽度，未能充分利用视频内容的时空特性进行计算资源优化。一方面，固定量化策略对非显著区域的冗余计算增加了不必要的计算开销；另一方面，对所有区域采用相同位宽可能导致显著区域的细节损失，从而影响主观视觉质量。此外，现有的动态量化方法通常依赖于推理时的实时统计信息，计算开销较高，难以满足低延迟应用（如移动端流媒体、视频会议等）的需求。因此，本章提出了一种结合视觉显著性检测与动态比特分配的优化方法，实现计算效率与重建质量的平衡。

本研究中引入轻量级的位宽分配器，根据输入区域的特征、光流梯度幅值以及通道级标准差，自适应地估计不同区域的候选量化比特，并动态分配最优位宽。其中，ROI 区域采用高比特量化以保留细节信息，而非 ROI 区域采用低比特量化以降低计算负载。对于权重量化，采用固定比特宽度，以减少比特流中的额外信息传输。此外，为保障硬件兼容性，使用 ReLU 替代广义除数归一化（GDN），并通过知识蒸馏进行补偿，以弥补量化过程中的非线性表达能力损失。

实验结果表明，该方法能够有效降低计算复杂度和存储开销，在不影响视觉质量的前提下减少计算消耗。与传统的固定位宽量化方法相比，该方法能更好地保证主观重建质量，为移动端实时视频处理和低延迟流媒体应用提供了更高效的解决方案。

## 总结与展望

### 全文工作总结

随着超高清视频技术的广泛应用，视频数据量的持续增长对存储、传输和计算资源提出了更高的挑战。传统的视频编码标准基于混合编码框架，实现了较高的压缩率，但由于依赖固定的人工设计特征，其性能提升逐渐趋于瓶颈。近年来，端到端优化的深度学习视频编码方法在率失真优化方面展现出较大潜力。然而，现有方法大多没有考虑到人眼的关注特性，对整帧采用统一的码率分配策略，影响了率失真性能；此外，大多数方法难以在单一模型内实现连续可变码率调节，增加了实际部署的复杂性和存储开销；除此之外，现有编码模型的量化方案采用固定位宽，忽略了视频内容的动态特性，量化后关键区域的重建精度损失较高。针对上述问题，本文围绕内容感知的视频编码优化展开研究，并提出了两项关键优化策略，主要研究内容如下：

（1）提出了一种内容感知的可变码率视频编码方法，旨在根据人眼视觉系统的非均匀感知特性，动态调整 ROI和非 ROI 区域的比特分配，并通过分级可变码率调控机制，实现连续可变的码率控制。分级可变码率涉及三个粒度级别，全局级通过条件卷积模块调整整体码率分布，通道级基于上下文编码隐变量的特征信息自适应缩放量化权重，局部级结合显著性检测网络优化不同区域的失真控制。实验结果表明，与基线模型相比，显著性编码框架在多种数据集上的ROI区域重建后平均可实现25.7\%～44.4\%的码率节省，能够在保证人眼视觉感知的质量的前提下，进一步提升压缩率，并在单一模型中实现连续可变码率调节，更灵活地适应不同的带宽需求。

（2）提出了一种显著区域感知的视频编码模型量化方案，针对现有深度学习编码模型量化方案未能充分考虑视频内容的动态特性的问题，提出基于人眼显著性的区域自适应量化策略。首先，通过显著性检测模型生成 ROI掩膜，将视频帧划分为显著区域和非显著区域。设计轻量级位宽分配器，结合输入区域的特征、光流梯度幅值和通道级标准差，动态分配最优位宽，从而优化计算资源分配。为了增强硬件兼容性，使用ReLU 替代 GDN 并结合知识蒸馏策略，补偿缺乏GDN可能引入的非线性表达能力损失。实验结果表明，相比于基线静态量化模型，所提量化方法分别在 8-bit 和 4-bit 模型上实现了平均 14.6\% 和 14.7\% 的码率节省，并且ROI区域重建质量与全精度模型保持相当的水平。该方法在降低计算复杂度和存储开销的同时，能更好地保证主观重建质量，满足低延迟场景下的实际应用需求。

### 后续工作展望

本文围绕端到端的深度学习视频编码内容感知优化进行了研究，并取得了一些进展。然而，还有很多方面需要进一步研究，以下是本文对未来工作的展望：

（1）设计更高效、轻量的模型结构以适应高帧率应用场景。尽管本文通过动态量化降低了计算复杂度，但在高帧率（如 120FPS 及以上）的应用场景中，推理速度仍然受到模型规模和计算资源的限制，难以满足超高清视频、实时游戏视频流等场景的需求。现有神经视频压缩框架大多采用复杂的深度卷积神经网络，在保证高压缩率的同时，引入了较高的计算开销。因此，未来的研究可探索更轻量化的网络架构，以减少冗余计算。另一方面，熵模型的优化也是提升推理效率的重要方向。当前端到端视频编码框架多采用自回归 熵模型进行概率估计，虽然能够提升编码精度，但由于自回归特性，导致解码过程难以并行化，增加了计算延迟。未来可以探索更高效的熵模型，在保留较高估计精度的同时，实现更高效的比特率建模，进而优化整体编码效率。

（2）解决端到端视频编码的跨平台不一致问题。端到端视频编码的部署通常涉及不同硬件平台（如 CPU、GPU、NPU）和不同软件环境，但由于深度学习模型的推理框架、计算精度和硬件优化方式存在差异，可能导致不同平台上的解码结果存在不一致性。例如，在某些硬件上，浮点计算精度不同会影响模型推理结果，从而导致重建视频的细微偏差，这对编码标准化和大规模应用部署带来了挑战。因此，未来可以研究跨平台一致性优化方法，确保模型在不同设备上具有统一的推理结果；或引入模型标准化层，减少不同硬件架构下的计算偏差，提升端到端视频编码在跨平台应用中的稳定性和适用性。

（3）扩展数据集多样性以提升模型的适应性与泛化能力。当前端到端视频编码的研究主要基于自然视频场景，如 UVG、MCL-JCV、Vimeo-90K 等，这些数据集涵盖了常见的自然场景、人物和运动模式。然而，在实际应用中，视频内容的种类更为多样，包括医学影像视频、工业检测视频、低光环境视频、计算机合成动画（CGI）、遥感视频等，这些视频往往具有独特的视觉特征，与传统自然视频有较大区别。因此，基于这些单一类型数据集训练的模型在面对其他类型视频时泛化能力较弱，压缩性能下降，影响其在实际场景中的应用效果。未来的研究可以构建更加多样化的训练数据集，覆盖不同场景、帧率、分辨率、运动模式和光照条件，或者通过探索自监督学习和迁移学习，以增强模型在不同视频场景中的适应性。



本论文的完成离不开许多人的关心与帮助，在此我谨向所有给予我支持和鼓励的人表示诚挚的感谢。

首先，我要衷心感谢我的导师\textbf{XXX教授}，在论文的选题、研究过程中，XXX教授给予了我极大的指导和帮助。无论是在学术思路的启发，还是在写作结构的把握上，XXX教授都耐心细致地给予指导，使我在不断思考与修改中得以提升。

同时，我也要感谢本专业的各位任课教师，您们的严谨治学态度和深厚学识让我受益匪浅。感谢实验室的同门和同学们，在学习与生活中给予我许多宝贵的建议与鼓励，让我在科研道路上不再孤单。

此外，我还要特别感谢我的家人，感谢他们在背后默默地支持与理解，是你们给予我前行的动力，让我能够专注于学业，顺利完成学业任务。

最后，向所有在我学习与生活过程中给予过我帮助的人致以最真诚的谢意！



在本论文的撰写过程中，我有幸得到许多老师、家人和同学的关心与帮助。在此，我怀着无比感激的心情，向所有给予我支持和鼓励的人致以最诚挚的谢意。

首先，我要衷心感谢我的导师曹先彬老师和张宝昌老师，在专业知识上给予我细致入微的指导，在学术道路上不断引领我前行。在课题选择、实验设计、论文撰写等各个阶段，始终给予我悉心指导和耐心帮助，使我在科研能力和学术视野上都获得了长足进步。在此，衷心祝愿老师身体健康、万事顺意！

同时，我要感谢我的父母。感谢你们始终如一的理解与支持，是你们无条件的信任和鼓励让我有勇气坚持初心、不断前行。无论我面对怎样的困难，你们永远是我最坚强的后盾，是我不断追求理想的源动力。

此外，我要特别感谢我的同学和朋友们。能够与你们一同学习、共同成长，是我人生中一段宝贵的经历。感谢王一诺同学对我一直以来的坚定支持，感谢肖双赢、张维佳、赵敏同学对我的帮助和鼓励，是你们的陪伴让这段旅程充满力量和温情。

再次感谢所有在我求学期间给予我帮助的人，是你们让我以更加坚定的步伐迈向人生的新阶段。

最后，诚挚感谢在百忙之中审阅本论文的各位老师，以及所有参与毕业答辩工作的老师们，您们辛苦了！
