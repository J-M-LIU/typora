# Review



## 基本概念

$$
Q(r) = round(\frac{r}{scale})+zero\_point
$$

​	

### 模型量化的对象

**a. 权重-weight**

​	weight的量化是最常规也是最常见的。量化weight可达到减少模型大小内存和占用空间。可以观察到神经网络权值的分布通常在一个很小的范围内，非常接近于0。下图显示了Alexnet模型和MobileNet v1模型中一些卷积和全连接层的权重分布。这个有限的范围使得映射到较低精度的数据类型更不容易出现量化误差。

<img src="https://img-blog.csdnimg.cn/img_convert/1e5b4fbad07286f13ce60114adef254c.png" style="zoom:80%;" />

**b. 激活输出-activation/特征图/神经元输出**

​	实际中activation往往是占内存使用的大头，因此量化activation不仅可以大大减少内存占用，更重要的是，结合weight的量化可以充分利用整数计算获得性能提升。与模型的权重不同，神经网络层的激活根据提供给模型的输入数据而变化，并且为了估计激活范围，需要一组具有代表性的输入数据样本。因此量化激活是一个依赖于数据的过程，需要额外的数据来校准神经网络层的输出。可以使用不同的方法来确定比例因子和模型权重和激活的零点。

**c. 梯度-gradient**

​	主要用于训练。它主要作用是在分布式计算中减少通信开销，单机训练时也可以减少backward时的开销。

### Symmetric and Asymmetric Quantization

​	量化过程需要选取合适的放缩比例，$S=\frac{\beta-\alpha}{2^b-1}$, $[\alpha,\beta]$ 是选取的真实值的区间，根据真实值的不同clip range剪切区间，可以分为对称量化和非对称量化。

**对称**

​	使用对称量化，简化了量化函数 $Q(r) = Int(r/S)-Z$，使得零点Z=0；
$$
-\alpha = \beta = max(|r_{min}|,|r_{max}|)\\
$$
<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230221154827498.png" alt="image-20230221154827498" style="zoom:70%;" />

**非对称**

​	非对称量化中常选择 $\alpha = r_{min},\beta = r_{max}$。与对称量化相比，非对称量化通常会导致更窄的剪辑范围。当目标权重或激活不平衡时，这尤其重要，例如，ReLU后的激活总是具有非负值。

​	利用min/max值进行对称/非对称量化易受到激活异常数据的影响。解决这个问题的**方法一**是使用百分位数而不是min/max值。也就是说，β / α采用第i个min/max值代替最大/最小值。**方法二**是通过最小化真实值和量化值之间的**KL散度**来选择α和β。

​	对称量化：适用于 weight 量化，因为零点Z=0，减少推理期间的计算成本；非对称：适用于真实值范围不对称的情况，适用于激活函数的量化。

**具体公式如下**

非对称量化
$$
\begin{aligned}
x_{\text {int }} & =\operatorname{round}\left(\frac{x}{\Delta}\right)+z \\
x_{Q} & =\operatorname{clamp}\left(0, N_{\text {levels }}-1, x_{\text {int }}\right) 
\end{aligned}
$$
对称量化
$$
\begin{array}{rlr}
x_{i n t} & =\operatorname{round}\left(\frac{x}{\Delta}\right) \\
x_{Q} & =\operatorname{clamp}\left(-N_{\text {levels }} / 2, N_{\text {levels }} / 2-1, x_{i n t}\right) & \text { if signed } \\
x_{Q} & =\operatorname{clamp}\left(0, N_{\text {levels }}-1, x_{i n t}\right) & \text { if un-signed }
\end{array}
$$
反量化过程：$x_{out} = (x_Q-Z)\Delta、$ $x_{out} = x_Q\Delta$

### 量化方式

已知提速概率较大的量化方法主要有三类：

**二值化**，其可以用简单的位运算来同时计算大量的数。对比从nvdia gpu到x86平台，1bit计算分别有5到128倍的理论性能提升。且其只会引入一个额外的量化操作，该操作可以享受到SIMD（单指令多数据流）的加速收益。

**线性量化**，又可细分为非对称，对称和ristretto几种。在nvdia gpu，x86和arm平台上，均支持8bit的计算，效率提升从1倍到16倍不等，其中tensor core甚至支持4bit计算，这也是非常有潜力的方向。由于线性量化引入的额外量化/反量化计算都是标准的向量操作，也可以使用SIMD进行加速，带来的额外计算耗时不大。

**对数量化**，一个比较特殊的量化方法。可以想象一下，两个同底的幂指数进行相乘，那么等价于其指数相加，降低了计算强度。同时加法也被转变为索引计算。但没有看到有在三大平台上实现对数量化的加速库，可能其实现的加速效果不明显。只有一些专用芯片上使用了对数量化。

**聚类量化**

​	采用kmeans算法来将权值进行聚类，在每一个类中，所有的权值共享该类的聚类质心，因此最终存储的结果就是一个码书和索引表。

![](https://img-blog.csdnimg.cn/img_convert/c291eb5910848f3aee1b67bb19151f03.png)

### 动态/静态量化 在线/离线?

​	为什么要量化激活输出? 权重在推理期间是固定的，其clip range可以静态地计算；而激活输出/feature map会随输入样本的变化而变化。神经网络里面的激活函数都是针对实数域的，比如leaky relu里面的alpha值一般是小数，或者tanh的输入一般也是小数，但量化里面数值一般都是整型，也就是输入是0、1、2...，这个时候普通的tanh函数是没法使用的，因为不管输入是什么，输出都基本一样（参考tanh的函数图像）。而量化tanh的目的就是希望能继续保持tanh的映射关系。

​	动态量化中，在运行时为每个激活映射动态计算 scale 和 zero_point。这种方法需要实时计算统计量(最小值、最大值、百分位数等)，这可能会有很高的开销。然而，动态量化通常会导致更高的精度，因为每个输入的范围是精确计算的。

​	静态量化中clip range scale zero_point 是预先计算的，在推理过程中是静态的。这种方法不会增加任何计算开销，但会导致较低的精度。常用的预计算方法是输入校准数据（calibration）来计算激活范围。如最小化真实值权重分布与相应量化值之间的均方误差；另一种方法在神经网络训练中学习/施加clip range。如LQNets[^1]， PACT [^2]， LSQ[^3]和LSQ+[^4]，它们在训练期间联合优化神经网络中的clip range和权重。



### 量化粒度

​	根据不同的模型量化参数选择范围，可将模型量化的粒度分为：

- 分层量化（per-tensor/per-layer）：一个 tensor 中所有的 kernel 的权重都用同一个区间范围[α,β]、同一个 scale 和 zero point。虽然这种方法实现简单，但通常会导致次优解，因为每个 kernel 的范围可能会有很大的变化，会使某些参数范围相对较窄的卷积核失去quantization resolution。

<img src="https://pic1.zhimg.com/80/v2-6539935ce12a79b789a4d574d5d9e810_1440w.webp" style="zoom:35%;" />

- 通道量化（per-axis/per-channel）实践中，per-channel 量化中的 feature 仍是整个 tensor 共用一个 scale 和 zeropoint，但每个 kernel 会单独统计一个 scale 和 zeropoint（注意是每个 kernel，而不是 kernel 的每个 channel）。



<img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2qjmTSQaKuwGAMsXaSOMgOoZeiamcyzLnS7MLNRria0icSEiavj1e14AsCibLiaXveQUnTXIrcl7fWLcmkA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style="zoom:35%;" />

- 分组量化：与 per-channel 量化相比，分组量化是将层内的通道分组后，每一组单独统计 scale 和 zero point。

### 量化后的 fine-tuning方法

​	根据在量化之后是否需要重新训练模型可以将量化的方法分为：量化感知训练（QAT） 和 训练后量化（PTQ）。

**量化感知训练（QAT）**给定一个训练好的模型，量化可能会给训练好的模型参数带来扰动，这可能会使模型偏离它在用浮点精度训练时收敛的点。可以通过用量化的参数重新训练网络模型来解决这个问题，这样模型就可以收敛到一个有更好损失的点。一种常用的方法称为量化感知训练（QAT），其通常在前向和反向传递过程中以浮点数进行，但在每次梯度更新后，模型参数会被量化。反向传播中的一个很关键的地方在于如何处理不可微分的量化算子，这个算子的梯度几乎在任何地方都是零，一般使用直通估计器（STE）来近似这个算子的梯度。

​	尽管STE是粗略的近似，但QAT已经被证明是有效的。然而，QAT的主要缺点是计算成本太高。因为其需要重新训练网络模型，这种训练可能需要花费几百个epoch，对于低比特的量化，更是如此。当前，高效的量化部署很重要，很多时候重新训练整个量化模型被认为是过于耗时和没必要的。

**训练后量化（PTQ）**： 训练后量化（PTQ），它执行量化后没有任何微调，计算开销非常低。与需要足够数量的训练数据进行再训练的QAT不同，PTQ有一个额外的优势，即它可以在数据有限或无标签的情况下应用。然而，与QAT相比，这往往是以较低的精度为代价的，特别是对于低精度量化。

​	在PTQ中，所有的权重和激活值量化参数的确定都不需要对NN模型进行任何重新训练。因此，PTQ是一种非常快速的量化NN模型的方法。然而，与QAT相比，这往往是以较低的精度为代价的。

#### PyTorch的三种模型量化方法

**训练后动态量化：** 这是最简单的量化形式，其中权重被提前量化，而激活在推理过程中被动态量化。这种方法用于模型执行时间由从内存加载权重而不是计算矩阵乘法所支配的情况，这适用于批量较小的 LSTM 和 Transformer 类型。

**训练后静态量化：** 这是最常用的量化形式，其中权重是提前量化的，并且基于在校准过程中观察模型的行为来预先计算激活张量的比例因子和偏差。CNN 是一个典型的用例，训练后量化通常是在内存带宽和计算节省都很重要的情况下进行的。

**训练时量化：** 在极少数情况下，训练后量化不能提供足够的准确性，可以插入 torch.quantization.FakeQuantize() 模块执行训练时量化。计算将在 FP32 中进行，但将值取整并四舍五入以模拟 INT8 的量化效果。



**AdaRound**

**AdaQuant**



**零样本量化 Zero-shot Quantization**



### 随机量化 Stochastic Quantization

​	前面所述的量化都是确定性的量化方案，有一些工作探索了随机量化。一种直觉是，与确定的量化相比，随机量化可能会让NN探索更多。主流观点认为：小的权重更新可能不会导致任何权重变化，因为舍入操作可能总是返回相同的权重。然而，启用随机四舍五入可能为NN提供一个逃脱的机会，从而更新其参数。具体来说：随机量化将浮点数值向上或向下映射的概率与权重更新的幅度相关。具体如下式：
$$
Int(x)=
\begin{cases}
\lfloor x \rfloor, \quad with\ probability\  \lceil x \rceil-x \\
\lceil x \rceil, \quad with \ probability\  x-\lfloor x \rfloor
\end{cases}
\tag{1}
$$
​	上式在二值量化中不适用，进一步改写：$\sigma(x)$是sigmoid函数
$$
Binary(x)=
\begin{cases}
-1, \quad with\ probability\  1- \sigma(x)\\
+1, \quad with \ probability\  \sigma(x)
\end{cases}
\tag{1}
$$



### 生产量化模型

L1：data free，直接将浮点参数根据某种手工规则转成量化，一般会带来很大的精度损失。
L2：calibration，通过少量输入数据进行统计分析，基于数据校准的方案，很多芯片都会提供这样的功能，比如tensorRT，高通，寒武纪等。它需要转模型的时候提供一些真实的计算数据。
L3：finetune，将量化误差在训练时仿真建模，调整权重使其更适合量化，好处是可以带来更大的精度提升，缺点是需要修改训练代码，实施周期比较长。



## Advanced Concepts

### 模拟量化 Simulated Quantization/纯整数量化 Integer-only Quantization

​	伪量化保存模型每一层时，利用低精度来保存每一个网络参数，同时保存拉伸比例scale和零值对应的浮点数zero_point。推理阶段，将网络参数还原为32bit浮点。

​	在模拟量化中，量化后的模型参数以低精度存储，但操作（如矩阵乘法和卷积）是以浮点运算进行的。因此，量化后的参数需要在浮点运算之前进行反量化。因此，人们不能完全受益于模拟量化的快速有效的低精度逻辑。为什么推理时不仍然使用低精度呢？这是因为一方面框架层有些算子只支持浮点运算，需要专门实现算子定点化才行。另一方面，高精度推理准确率相对高一些。伪量化可以实现模型压缩，但对模型加速没有多大效果。

​	纯整数量化中，所有的操作都是使用低精度的整数运算，这允许用高效的整数算术进行整个推理，而不需要对任何参数或激活值进行浮点反量化。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230226173430490.png" alt="image-20230226173430490" style="zoom:40%;" />

### 混合精度量化 Mixed-Precision Quantization

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230226211253875.png" alt="image-20230226211253875" style="zoom:50%;" />

​	当使用较低精度的量化时，硬件性能会提高。然而，将一个模型均匀量化为超低精度会导致精度显著下降。 可以通过混合精度量化来解决这个问题。如下图所示，每层都以不同的比特精度进行量化。这种方法的难点是，选择这种不同bit位设置的搜索空间是层数的指数级。为每一层选择这种混合精度基本上是一个搜索问题。



### 蒸馏辅助量化

​	量化中一个有趣的工作是结合模型蒸馏来提高量化精度[^5]。模型蒸馏是一种使用精度较高的大型模型作为教师来帮助训练紧凑的学生模型的方法。在学生模型的训练过程中，模型蒸馏提出利用教师产生的软概率，而不是仅仅使用ground-truth类标签，它可能包含更多的输入信息。



### 极端量化（二值量化）

BinaryConnect

### 矢量量化


















## 参考文献

[^1]:Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: **Learned quantization for highly accurate and compact deep neural networks.** In European conference on computer vision (ECCV), 2018.
[^2]: Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. **Pact: Parameterized clipping activation for quantized neural networks.** arXiv preprint arXiv:1805.06085, 2018.
[^3]: Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. **Learned step size quantization**. arXiv preprint arXiv:1902.08153, 2019.
[^4]: Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. **Lsq+: Improving low-bit quantization through learnable offsets and better initialization.** In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 696–697, 2020.
[^5]: Antonio Polino, Razvan Pascanu, and Dan Alistarh. **Model compression via distillation and quantization.** arXiv preprint arXiv:1802.05668, 2018.
