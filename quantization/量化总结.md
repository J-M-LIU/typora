# Review



## 基本概念

$$
Q(r) = round(\frac{r}{scale})+zero\_point
$$

### 模型量化的对象

**a. 权重-weight**

​	weight的量化是最常规也是最常见的。量化weight可达到减少模型大小内存和占用空间。可以观察到神经网络权值的分布通常在一个很小的范围内，非常接近于0。下图显示了Alexnet模型和MobileNet v1模型中一些卷积和全连接层的权重分布。这个有限的范围使得映射到较低精度的数据类型更不容易出现量化误差。

<img src="https://img-blog.csdnimg.cn/img_convert/1e5b4fbad07286f13ce60114adef254c.png" style="zoom:80%;" />

**b. 激活输出-activation/特征图**

​	实际中activation往往是占内存使用的大头，因此量化activation不仅可以大大减少内存占用，更重要的是，结合weight的量化可以充分利用整数计算获得性能提升。与模型的权重不同，神经网络层的激活根据提供给模型的输入数据而变化，并且为了估计激活范围，需要一组具有代表性的输入数据样本。因此量化激活是一个依赖于数据的过程，需要额外的数据来校准神经网络层的输出。

**c. 梯度-gradient**

​	主要用于训练。它主要作用是在分布式计算中减少通信开销，单机训练时也可以减少backward时的开销。

### Symmetric and Asymmetric Quantization

​	量化过程需要选取合适的缩放系数，$S=\frac{\beta-\alpha}{2^b-1}$, $[\alpha,\beta]$ 是选取的真实值的区间 clipping range，根据clipping range的选取方式，可以分为对称量化和非对称量化。**选取clipping range的过程又叫校准 calibration**

**对称**

​	使用对称量化，简化了量化函数 $Q(r) = Int(r/S)-Z$，使得零点Z=0；
$$
-\alpha = \beta = max(|r_{min}|,|r_{max}|)\\
$$
<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230221154827498.png" alt="image-20230221154827498" style="zoom:70%;" />

**非对称**

​	非对称量化中常选择 $\alpha = r_{min},\beta = r_{max}$。与对称量化相比，非对称量化通常会导致更窄的剪辑范围。当目标权重或激活不平衡时，这尤其重要，例如，ReLU后的激活总是具有非负值。

​	利用min/max值进行对称/非对称量化易受到激活异常数据的影响。解决这个问题的**方法一**是使用百分位数而不是min/max值。也就是说，β / α采用第i个min/max值代替最大/最小值。**方法二**是通过最小化真实值和量化值之间的**KL散度**来选择α和β。

==对称量化：适用于 weight 量化，因为零点Z=0，减少推理期间的计算成本；==
==非对称：适用于真实值范围不对称的情况，适用于激活函数的量化==。</u>

**具体公式如下**

非对称量化
$$
\begin{aligned}
x_{\text {int }} & =\operatorname{round}\left(\frac{x}{\Delta}\right)+z \\
x_{Q} & =\operatorname{clamp}\left(0, N_{\text {levels }}-1, x_{\text {int }}\right) 
\end{aligned}
$$
对称量化 $N_{levels}$是bit-width为N的量化range，如若bit-width为8，则从float映射到int的范围为：0～2^8^-1，或者 -2^7^～2^7^-1.
$$
\begin{array}{rlr}
x_{i n t} & =\operatorname{round}\left(\frac{x}{\Delta}\right) \\
x_{Q} & =\operatorname{clamp}\left(-N_{\text {levels }} / 2, N_{\text {levels }} / 2-1, x_{i n t}\right) & \text { if signed } \\
x_{Q} & =\operatorname{clamp}\left(0, N_{\text {levels }}-1, x_{i n t}\right) & \text { if un-signed }
\end{array}
$$
反量化过程：$x_{out} = (x_Q-Z)\Delta、$ $x_{out} = x_Q\Delta$

### calibration 校准

Calibration是用来选模型参数和激活值的缩放系数，并对real value的区间做截断。

**校准策略**

**max-min**：tensor的最大值和最小值，这个策略没有截断，常用于权重量化。

**KL散度（KL divergence）**：最小化量化后int8与float32数据的KL散度，**TensorRT**采用这个策略。

**百分比（Percentile**）：选取 tensor 的99%或其他百分比数值，其余的截断，常用于激活量化。

**MSE**：定义量化损失为量化前后的权重或激活的最小均方误差(MSE)。



### 量化方式

已知提速概率较大的量化方法主要有三类：

**二值化**，其可以用简单的位运算来同时计算大量的数。对比从nvdia gpu到x86平台，1bit计算分别有5到128倍的理论性能提升。且其只会引入一个额外的量化操作，该操作可以享受到SIMD（单指令多数据流）的加速收益。

**线性量化**，又可细分为**非对称，对称和ristretto**几种。在nvdia gpu，x86和arm平台上，均支持8bit的计算，效率提升从1倍到16倍不等，其中tensor core甚至支持4bit计算，这也是非常有潜力的方向。由于线性量化引入的额外量化/反量化计算都是标准的向量操作，也可以使用SIMD进行加速，带来的额外计算耗时不大。

**对数量化**，一个比较特殊的量化方法。可以想象一下，两个同底的幂指数进行相乘，那么等价于其指数相加，降低了计算强度。同时加法也被转变为索引计算。但没有看到有在三大平台上实现对数量化的加速库，可能其实现的加速效果不明显。只有一些专用芯片上使用了对数量化。

### 权值共享和定点量化

​	**权值共享采用聚类等方式将权重进行分类，即聚类量化**，采用了 K-means 聚类算法对每个卷积层的权重进行聚类，每个簇的权重都用该簇的聚类中心的值表示。存储时只需要存储对应簇的编号和查找表并 结合哈夫曼编码进行进一步的压缩，在使用时通过查表得到对应的值。同时通过 fine-tune 微调恢复因量化带来的精度损失。然而这种查表的方式对硬件的利用率很低。为了进行高效的推理，需要将索引值恢复为浮点数进行运算，因此一般不能实现加速效果。

![](https://img-blog.csdnimg.cn/img_convert/c291eb5910848f3aee1b67bb19151f03.png)

**定点量化**

​	对于浮点数，可以采用IEEE规定的浮点数格式，但那种浮点数运算的开销太大，对于小数位数比较少的浮点数，资源浪费又太多，可以采用定点数的形式来进行运算。所谓定点数就是将小数点的位置固定，也就是说，整数部分和小数部分的位数固定，我们用整数来表示这个定点小数。

举例说明。12.918，定点量化时对整数部分和小数部分分别量化。

（1）整数部分12，最少使用4位量化，4-bit表示范围0~15；

（2）小数部分0.918，假设12.918整体使用12位量化，整数已使用4-bit，则小数部分还能使用8-bit量化，能够表示2的8次方共计256个刻度，每个刻度的间隔是1/256 = 0.00390625，这个值是量化精度。

小数0.918需要多少个刻度来表示？ 0.918/(1/256) = 0.918256 = 235.008，四舍五入取整，则使用235个刻度来表示，误差是0.008个刻度，误差大小是0.008(1/256) = 0.00003125。

量化误差小于量化精度的一半，认为是“无损量化”。由于量化后误差0.00003125肯定是小于精度0.00390625的一半，所以这个误差小到可以认为是无损量化。

### 动态/静态量化 在线/离线?

​	为什么要量化激活输出? 权重在推理期间是固定的，其clip range可以静态地计算；而激活输出/feature map会随输入样本的变化而变化。神经网络里面的激活函数都是针对实数域的，比如leaky relu里面的alpha值一般是小数，或者tanh的输入一般也是小数，但量化里面数值一般都是整型，也就是输入是0、1、2...，这个时候普通的tanh函数是没法使用的，因为不管输入是什么，输出都基本一样（参考tanh的函数图像）。而量化tanh的目的就是希望能继续保持tanh的映射关系。

​	动态量化中，在运行时为每个激活映射动态计算 scale 和 zero_point。这种方法需要实时计算统计量(最小值、最大值、百分位数等)，这可能会有很高的开销。然而，动态量化通常会导致更高的精度，因为每个输入的范围是精确计算的。

​	静态量化中clip range scale zero_point 是预先计算的，在推理过程中是静态的。这种方法不会增加任何计算开销，但会导致较低的精度。常用的预计算方法是输入校准数据（calibration）来计算激活范围。如最小化真实值权重分布与相应量化值之间的均方误差；另一种方法在神经网络训练中学习/施加clip range。如LQNets[^1]， PACT [^2]， LSQ[^3]和LSQ+[^4]，它们在训练期间联合优化神经网络中的clip range和权重。



### 量化粒度

​	根据不同的模型量化参数选择范围，可将模型量化的粒度分为：

- 分层量化（per-tensor/per-layer）：一个 tensor 中所有的 kernel 的权重都用同一个区间范围[α,β]、同一个 scale 和 zero point。虽然这种方法实现简单，但通常会导致次优解，因为每个 kernel 的范围可能会有很大的变化，会使某些参数范围相对较窄的卷积核失去**quantization resolution**。

<img src="https://pic1.zhimg.com/80/v2-6539935ce12a79b789a4d574d5d9e810_1440w.webp" style="zoom:28%;" />

- 通道量化（per-channe/per-axisl）实践中，per-channel 量化中的 feature 仍是整个 tensor 共用一个 scale 和 zeropoint，但每个 kernel 会单独统计一个 scale 和 zeropoint（注意是每个 kernel，而不是 kernel 的每个 channel）。

<img src="https://mmbiz.qpic.cn/mmbiz_jpg/bdpnCavfx2qjmTSQaKuwGAMsXaSOMgOoZeiamcyzLnS7MLNRria0icSEiavj1e14AsCibLiaXveQUnTXIrcl7fWLcmkA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style="zoom:30%;" />

- 分组量化：与 per-channel 量化相比，分组量化是将层内的通道分组后，每一组单独统计 scale 和 zero point。

### 量化模型是否finetune

​	根据量化是否需要重新训练模型/finetune 可以将量化的方法分为：量化感知训练（QAT） 和 训练后量化（PTQ）。

**量化感知训练（QAT）**量化训练让模型感知量化运算对模型精度带来的影响，使用大量有标签的数据，通过 finetune 训练降低量化误差。

​	QAT通常在前向和反向传递过程中以浮点数进行，但在每次梯度更新后，模型参数会被量化。反向传播中的一个很关键的地方在于如何处理不可微分的量化算子，这个算子的梯度几乎在任何地方都是零，一般使用直通估计器（STE）来近似这个算子的梯度。尽管STE是粗略的近似，但QAT已经被证明是有效的。然而，QAT的主要缺点是计算成本太高。因为其需要重新训练网络模型，这种训练可能需要花费几百个epoch，对于低比特的量化更是如此。

**后训练量化（PTQ）**： 即对训练好的模型做量化，但不finetune。与需要足够数量的训练数据进行再训练的QAT不同，PTQ有一个额外的优势，即它可以在数据有限或无标签的情况下应用。然而，与QAT相比，这往往是以较低的精度为代价的，特别是对于低精度量化。在PTQ中，所有的权重和激活值量化参数的确定都不需要对NN模型进行任何重新训练。

​	所谓后训练量化，**就是给少量训练数据或测试数据，在模型结构不变、且不重新训练的情况下，得到或者接近浮点模型的精度**。这里的目标是获得和浮点模型精度接近的量化模型 (其实就是量化参数)，而约束条件可以归纳为以下几个：

1. 只给少量的样本数据做参数校准，而没有完整的训练集数据；
2. 不能做模型训练，或者严格地说，不能像训练浮点模型那样训练量化模型；
3. 量化的比特数最好小于等于 8，这样实际部署的时候才会有足够的收益；
4. 量化方式要方便部署，比如最好采用线性量化等等。

#### PyTorch的三种模型量化方法

**动态离线量化(PTQ Dynamic)：** 无需额外样本数据，仅将模型中特定算子的权重从浮点类型映射成整数类型。

**静态离线量化(PTQ Static)：** 静态离线量化使用少量无标签校准数据，采用 KL 散度等方法计算量化比例因子。

**量化训练 (Quant Aware Training, QAT)：** 需要完整数据进行finetune。



#### 零样本量化 Zero-shot Quantization

两种不同的策略：

1. 无数据（Zero-Shot Quantization，ZSQ）+无微调（PTQ）
2. 无数据（ZSQ）+需要训练微调（QAT）

​	ZSQ中一个流行的研究分支是生成与目标预训练模型训练的真实数据相似的合成数据。然后，合成数据用于校准和/或微调量化模型。早期工作利用生成对抗网络(GANs)进行合成数据生成。使用预训练的模型作为鉴别器，它训练生成器，使其输出可以被鉴别器很好地分类。然后，使用从生成器收集的合成数据样本，可以通过从全精度对应对象中蒸馏知识来对量化模型进行微调。然而，这种方法无法捕获真实数据的内部统计数据(例如，中间层激活的分布)，因为它只使用模型的最终输出生成。没有考虑内部统计的合成数据可能不能正确地代表真实的数据分布。为了解决这个问题，许多后续工作使用批处理规范化(BatchNorm)中存储的统计数据，即通道平均值和方差，以生成更真实的合成数据。其中[85]通过直接最小化内部统计量的KL散度来生成数据，并利用合成数据对量化模型进行校准和微调。

### 随机量化 Stochastic Quantization

​	前面所述的量化都是确定性的量化方案，有一些工作探索了随机量化。一种直觉是，与确定的量化相比，随机量化可能会让NN探索更多。主流观点认为：小的权重更新可能不会导致任何权重变化，因为舍入操作可能总是返回相同的权重。然而，启用随机四舍五入可能为NN提供一个逃脱的机会，从而更新其参数。具体来说：随机量化将浮点数值向上或向下映射的概率与权重更新的幅度相关。具体如下式：
$$
Int(x)=
\begin{cases}
\lfloor x \rfloor, \quad with\ probability\  \lceil x \rceil-x \\
\lceil x \rceil, \quad with \ probability\  x-\lfloor x \rfloor
\end{cases}
\tag{1}
$$
​	上式在二值量化中不适用，进一步改写：$\sigma(x)$是sigmoid函数
$$
Binary(x)=
\begin{cases}
-1, \quad with\ probability\  1- \sigma(x)\\
+1, \quad with \ probability\  \sigma(x)
\end{cases}
\tag{1}
$$



### 生产量化模型

L1：data free，直接将浮点参数根据某种手工规则转成量化，一般会带来很大的精度损失。
L2：calibration，通过少量输入数据进行统计分析，基于数据校准的方案，很多芯片都会提供这样的功能，比如tensorRT，高通，寒武纪等。它需要转模型的时候提供一些真实的计算数据。
L3：finetune，将量化误差在训练时仿真建模，调整权重使其更适合量化，好处是可以带来更大的精度提升，缺点是需要修改训练代码，实施周期比较长。



## Advanced Concepts

### 模拟量化 Simulated Quantization/纯整数量化 Integer-only Quantization

​	伪量化保存模型每一层时，利用低精度来保存每一个网络参数，同时保存比例scale和零值对应的浮点数zero_point。推理阶段，将网络参数还原为32bit浮点。

​	在模拟量化中，量化后的模型参数以低精度存储，但操作（如矩阵乘法和卷积）是以浮点运算进行的。因此，量化后的参数需要在浮点运算之前进行反量化。因此，人们不能完全受益于模拟量化的快速有效的低精度逻辑。为什么推理时不仍然使用低精度呢？这是因为一方面框架层有些算子只支持浮点运算，需要专门实现算子定点化才行。另一方面，高精度推理准确率相对高一些。伪量化可以实现模型压缩，但对模型加速没有多大效果。

​	纯整数量化中，所有的操作都是使用低精度的整数运算，这允许用高效的整数算术进行整个推理，而不需要对任何参数或激活值进行浮点反量化。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230226173430490.png" alt="image-20230226173430490" style="zoom:40%;" />

### 混合精度量化 Mixed-Precision Quantization

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230226211253875.png" alt="image-20230226211253875" style="zoom:50%;" />

​	当使用较低精度的量化时，硬件性能会提高。然而，将一个模型均匀量化为超低精度会导致精度显著下降。 可以通过混合精度量化来解决这个问题。如下图所示，每层都以不同的比特精度进行量化。这种方法的难点是，选择这种不同bit位设置的搜索空间是层数的指数级。为每一层选择这种混合精度基本上是一个搜索问题。



### 蒸馏辅助量化

​	量化中一个有趣的工作是结合模型蒸馏来提高量化精度[^5]。模型蒸馏是一种使用精度较高的大型模型作为教师来帮助训练紧凑的学生模型的方法。在学生模型的训练过程中，模型蒸馏提出利用教师产生的软概率，而不是仅仅使用ground-truth类标签，它可能包含更多的输入信息。



### 极端量化（二值量化）

BinaryConnect

### Vector Quantization



## TensorRT原理

- 权重量化：对权重使用对称最大值量化（不饱和映射），并忽略bias项；
- 激活量化：对称饱和量化；因为激活值通常分布不均匀，直接使用非饱和量化会使得量化后的值都挤在一个很小的范围从而浪费了INT8范围内的其他空间，也就是说没有充分利用INT8（-128～+127）的值域；而进行饱和量化后，使得映射后的-128~+127范围内分布相对均匀，这相当于去掉了一些不重要的因素，保留了主要成分。

<img src="https://img-blog.csdnimg.cn/20200618150924204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29ZZVpob3U=,size_16,color_FFFFFF,t_70" style="zoom:60%;" />

### 对称量化中的映射方式

​	分为不饱和映射和饱和映射，两种映射方的区别就是`FP32`张量的值在映射后是否能够大致均匀分布在`0`的左右。如果分布不均匀，量化之后将不能够充分利用`INT8`的数据表示能力。饱和映射：寻找一个阈值 **T** 使得

<img src="https://img-blog.csdnimg.cn/20200618145355799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29ZZVpob3U=,size_16,color_FFFFFF,t_70" style="zoom:70%;" />

​	上图表明权重可直接使用非饱和映射，而激活值使用饱和映射能调高性能：因为权重分别通常较为均匀，通过非饱和映射寻找阈值后，量化后的分布很可能变化微小；而激活值分布不均，寻找一个合适的阈值进行饱和映射就比较重要了；选择一个合适的阈值T，来对原来的分布进行一个截取，将±T区间内的值保存，之外的值忽略掉。

### 非饱和映射选取阈值

<img src="https://img-blog.csdnimg.cn/20200618151521573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L29ZZVpob3U=,size_16,color_FFFFFF,t_70" style="zoom:67%;" />

​	可以看出，不同模型的激活值分布差异很大，这就需要进行动态的量化，也即针对每一个模型，寻找一个对它来说最合适的T。于是，NVIDIA就选择了**KL散度**（相对熵）来对量化前后的激活值分布进行评价，找出使量化后INT8分布相对于原来的FP32分布信息损失最小的那个阈值。

**量化流程**

- 先在一个校准数据集上跑一遍原FP32的模型；
- 对每一层：
  - 收集激活值的直方图；
  - 生成在不同阈值下的饱和量化分布；
  - 找出使得KL散度最小的阈值T。

## BNN

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20230323112816384.png" alt="image-20230323112816384" style="zoom:45%;" />







## 参考文献

[^1]:Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: **Learned quantization for highly accurate and compact deep neural networks.** In European conference on computer vision (ECCV), 2018.
[^2]: Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. **Pact: Parameterized clipping activation for quantized neural networks.** arXiv preprint arXiv:1805.06085, 2018.
[^3]: Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. **Learned step size quantization**. arXiv preprint arXiv:1902.08153, 2019.
[^4]: Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. **Lsq+: Improving low-bit quantization through learnable offsets and better initialization.** In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 696–697, 2020.
[^5]: Antonio Polino, Razvan Pascanu, and Dan Alistarh. **Model compression via distillation and quantization.** arXiv preprint arXiv:1802.05668, 2018.
