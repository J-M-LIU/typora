基于变压器的检测器(detr)由于其稀疏的训练范式和去除后处理操作而引起了极大的关注，但庞大的模型计算耗时长，难以部署在现实应用中。为了解决这一问题，可以利用知识蒸馏(KD)对庞大的模型进行压缩，构建一个通用的师生学习框架。与传统CNN检测器通过特征图自然对齐蒸馏目标不同，DETR将目标检测视为集合预测问题，导致蒸馏过程中师生关系不明确。在本文中，我们提出了DETRDistill，一种新的知识蒸馏，致力于detr族。我们首先探索了一种具有渐进逐级实例蒸馏的稀疏匹配范式。针对不同DETRs采用的不同注意机制，提出了注意不可知的特征蒸馏模块，以克服传统特征模仿的不足。最后，为了充分利用来自教师的中间产品，我们引入了教师辅助的分配蒸馏，它使用教师的对象查询和分配结果作为附加指导。大量的实验表明，我们的蒸馏方法取得了显著的改进，在各种竞争的DETR方法，没有引入额外的消耗在推断阶段。据我们所知，这是第一次系统地研究了detr型检测器的一般蒸馏方法。

## Introduction

Object detection aims to locate and classify visual objects from an input image. In the early works, the task was typically achieved by incorporating convolution neural networks (CNNs) to process the regional features of the input image [16, 21], in which a bunch of inductive biases were included, for example, anchors [21], label assignment [35] and duplicate removal [1]. Recently, transformer-based ob-ject detectors like DETR [2] have been proposed where detection is treated as a set prediction task and can be trained end-to-end. They greatly simplify the detection pipeline, relieving the users from the tedious tuning of the hand-craft components, e.g., anchor sizes and ratios.

Although the transformer-based detectors have achieved state-of-the-art accuracy [10, 14, 18], they still suffer from the expensive computation problem, making them difﬁcult to be deployed in real-time applications. In order to acquire a fast and accurate detector, knowledge distillation [13] (KD) is often needed. It allows us to train a light and fast model with the help of a heavy-weighted but powerful teacher model. As the result, we can get an object detector with fast inference speed while keeping a good accuracy.

目标检测旨在从输入图像中定位和分类视觉目标。在早期的工作中，该任务通常是通过结合卷积神经网络(CNNs)来处理输入图像的区域特征[16,21]，其中包括一组归纳偏差，例如锚[21]，标签分配[35]和重复删除[1]。最近，已经提出了基于变压器的对象检测器，如DETR[2]，其中检测被视为一组预测任务，可以端到端进行训练。它们极大地简化了检测流程，将用户从手工组件(例如锚的大小和比例)的繁琐调优中解脱出来。

尽管基于变压器的探测器已经达到了最先进的精度[10,14,18]，但它们仍然存在昂贵的计算问题，这使得它们难以部署在实时应用中。为了获得快速准确的检测器，经常需要知识蒸馏[13](KD)。它允许我们在一个权重大但功能强大的教师模型的帮助下训练一个轻而快速的模型。因此，我们可以得到一个推理速度快的目标检测器，同时保持良好的精度。

Knowledge distillation has achieved great success in the traditional CNN-based object detectors [29, 30]. However, designing a good KD strategy for transformer-based detectors is nontrivial. Firstly, it is difﬁcult to directly apply the CNN-based KD algorithms to transformer-based models because of their architectural differences. Specifically, in transformer-based detectors, the object information is largely encoded in the object query vectors while such information mainly lies in the image feature maps in CNN-based ones, causing signiﬁcantly different feature distributions in these two families of detectors. For example, in Fig. 2 we compare the backbone feature activation of two detectors: ATSS [35] (CNN-based) and AdaMixer [10] (transformer-based). We observe that the feature activation of ATSS is mainly around the objects while the background areas are also largely activated in AdaMixer. As the result, the weighting strategies proposed for CNN-based detectors [26, 29, 30] would not be suitable for transformerbased ones. On the other hand, the set prediction design [2] of transformer-based detectors also makes knowledge distillation challenging: the different query-object matching makes it difﬁcult to transfer instance-level information from the teacher to the student model. In addition, the different cross-attention designs in those DETR variants [2, 10, 14, 18, 37] further impede us from developing a uniﬁed KD strategy.

知识蒸馏在传统的基于cnn的目标检测器中取得了很大的成功[29,30]。然而，为基于变压器的探测器设计一个好的KD策略并非易事。首先，基于cnn的KD算法难以直接应用于基于变压器的模型，因为它们在架构上存在差异。具体而言，在基于变压器的检测器中，目标信息主要编码在目标查询向量中，而在基于cnn的检测器中，这些信息主要存在于图像特征映射中，这导致两类检测器的特征分布存在显著差异。例如，在图2中，我们比较了两种检测器的骨干特征激活:ATSS[35](基于cnn)和AdaMixer[10](基于变压器)。我们观察到ATSS的特征激活主要集中在物体周围，而在AdaMixer中背景区域也被大量激活。因此，针对基于cnn的检测器提出的加权策略[26,29,30]并不适用于基于变压器的检测器。另一方面，基于变压器的检测器的集合预测设计[2]也给知识蒸馏带来了挑战:不同的查询对象匹配使得实例级信息难以从教师模型传递到学生模型。此外，这些DETR变体中不同的交叉注意设计[2,10,14,18,37]进一步阻碍了我们制定统一的KD策略。

To address the above challenges, we propose DETRDistill, a knowledge distillation framework speciﬁcally designed for transformer-based detectors. To the best of our knowledge, our work is the ﬁrst systematic study exploring the knowledge distillation for DETR-style detectors. DETRDistill is based on three components: (1) Progressive Instance Distillation: we establish prediction matching between object queries with progressive stage-by-stage instance distillation to gradually transfer useful knowledge to the student. At the same time, we supervise the relationship of content queries between the teacher and student. (2) Attention-Agnostic Feature Distillation: instead of the conventional imitation of features, we use content queries aggregated from each decoder layer to restore an attention mask for feature-level distillation, making it agnostic to a multifarious cross-attention mechanism. (3) Teacher-Assisted Assignment Distillation: to fully leverage the query embedding and corresponding bipartite graph matching results trained by the teacher model, we view them as additional training ﬂow  to provide more samples with ﬁxed matching.

We apply the proposed DETRDistill on the widely used MS COCO [17] benchmark. The experiment results validate the effectiveness and generalization ability of our method. DETRDistill archives state-of-the-art performance with large advantages over the previous object detection KD approaches. As shown in Fig. 1, DETRDistill improves the student performance by 2.2 AP, 2.5 AP, and 2.4 AP on three competitive transformer-based detectors, which even surpasses their respective teacher models.

为了解决上述挑战，我们提出了DETRDistill，这是一种专门为基于变压器的检测器设计的知识蒸馏框架。据我们所知，我们的工作是第一个探索detr式检测器知识蒸馏的系统研究。DETRDistill基于三个组成部分:(1)渐进实例蒸馏:我们通过渐进的逐级实例蒸馏在对象查询之间建立预测匹配，逐步将有用的知识传递给学生。同时，我们监督师生之间的内容查询关系。(2)注意不可知的特征蒸馏:我们使用从每个解码器层聚合的内容查询来恢复特征级蒸馏的注意掩码，使其对各种交叉注意机制不可知。(3)教师辅助作业蒸馏:为了充分利用教师模型训练的查询嵌入和相应的二部图匹配结果，我们将其视为学生提供更多固定匹配样本的额外训练流程。

我们将提出的DETRDistill应用于广泛使用的MS COCO[17]基准。实验结果验证了该方法的有效性和泛化能力。DETRDistill存档最先进的性能，与以前的目标检测KD方法相比有很大的优势。如图1所示，DETRDistill在三种基于变压器的竞争检测器上分别提高了2.2 AP、2.5 AP和2.4 AP，甚至超过了各自的教师模型。

## Method

In this section, we ﬁrst review the basic architecture of DETR and then introduce the concrete implementation of our proposed DETRDistill, which consists of three components: (i) Progressive Instance Distillation, (ii) AttentionAgnostic Feature Distillation, and (iii) Teacher-Assisted Assignment Distillation. Fig. 3 illustrates the overall architecture of DETRDistill.

在本节中，我们首先回顾了DETR的基本架构，然后介绍了我们提出的DETR蒸馏的具体实现，它由三个部分组成:(i)渐进实例蒸馏，(ii)注意力不确定特征蒸馏，(iii)教师辅助分配蒸馏。图3说明了DETRDistill的总体架构。

