## 线性神经网络



### 线性回归的基本元素



#### 预测结果表示为

$$
\hat{y}^{i}=w_1x_1^{i}+w_2x_2^{i}+...+w_nx_n^{i}+b
$$

将所有特征放到向量 $X\subseteq R^n$, 并将所有权重放到向量 $W\subseteq R^n$, 可以用点积分表示预测模型：
$$
\hat{y}^i=W^TX+b
$$

#### 损失函数

为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的损失均值:
$$
L(W,b)=\frac{1}{2n}\sum_{i=0}^n(W^TX^{(i)}+b-y^{(i)})^2
$$

#### 随机梯度下降

梯度下降最简单的用法是计算损失函数(数据集中所有样本的损失均值)关于模型参数的导数(在这里也可以称为梯度)。但实际中的执行可能会非常慢:因为在每一次更新参数之前，我们必须遍历整个数据集。因此， 我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做**小批量随机梯度下降**(minibatch stochastic gradient descent).
$$
W:=W-\frac{\alpha}{\Beta}\sum_{i\in\beta}X^{(i)}(W^TX^{(i)}+b-y^{(i)})
$$

$$
b:=b-\frac{\alpha}{\Beta}\sum_{i\in\beta}(W^TX^{(i)}+b-y^{(i)})
$$

其中 $\alpha$为学习率，$|\Beta|$为批量大小。批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为**超参数**(hyperparameter)。调参(hyperparameter tuning)是选择超参数的过程。超参数通常是我们根据训练迭代结果来调整的，而训练迭代结果是在独立的==验证数据集==(validation dataset)上评估得到的。

