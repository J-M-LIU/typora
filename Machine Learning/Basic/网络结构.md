## skip connect/residual connections 残差连接

**skip connect的思想**：将输出表述为输入和输入的一个非线性变换的线性叠加。F包含了卷积、激活等操作。

<img src="https://img-blog.csdnimg.cn/20200502093404478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODUyNjc2,size_16,color_FFFFFF,t_70" style="zoom: 33%;" />

**为什么要skip connect**

首先大家已经形成了一个通识，在一定程度上，网络越深表达能力越强，性能越好。但随着网络深度的增加，带来了许多问题：梯度消失、梯度爆炸。在Resnet出来之前尝试过各类优化方法：更好的初始化策略，BN层，Relu等各种激活函数，但改善问题的能力有限，直到残差连接被广泛使用。

深度学习依靠误差的链式反向传播来进行参数更新，假如有这样一个函数，$fgk$ 分别为卷积、激活、分类器：
$$
\mathbf{z} = f(\mathbf{x},\mathbf{w_f})\\
\mathbf{h}=g(\mathbf{z})\\
\mathbf{\hat{y}}=k(\mathbf{h})\\
L = l(\mathbf{y},\mathbf{\hat{y}})
$$
其中 $L$ 对 $\mathbf{w_f}$ 的导数为：
$$
\frac{\partial{l}}{\partial{\mathbf{\hat{y}}}} \times \frac{\partial{\mathbf{\hat{y}}}}{\partial{\mathbf{h}}} \times \frac{\partial{\mathbf{h}}}{\partial{\mathbf{z}}} \times \frac{\partial{\mathbf{z}}}{\partial{{\mathbf{w_f}}}}
$$
一旦其中某一个导数很小，多次连乘后梯度可能越来越小，这就是常说的梯度消失，对于深层网络，传到浅层几乎就没了。但是如果使用了残差，每一个导数就加上了一个恒等项1，此时就算原来的导数df/dx很小，这时候误差仍然能够有效的反向传播。

假设残差网络 $H =F(\mathbf{x})+\mathbf{x}$，非残差网络 $G = F(\mathbf{x})$，则
$$
\frac{\partial{H}}{\partial{\mathbf{x}}} = \frac{\partial F(\mathbf{x})}{\partial{\mathbf{x}}}+1\\
\frac{\partial{G}}{\partial{\mathbf{x}}} = \frac{\partial F(\mathbf{x})}{\partial{\mathbf{x}}}
$$

## Group Conv 分组卷积

<img src="https://pic4.zhimg.com/80/v2-7fe2aef064142470a0d1f2c220021d73_1440w.jpg" style="zoom:70%;" />

1. 将输入特征图按通道均分为g组，对每一组进行常规卷积；
2. 由于分组后，每组输入特征图通道数变为原来的 $1/g$，因此每个卷积核通道数也变成 $1/g$，因此分组卷积的运算量和参数量减少；
3. 由于每组内进行的是常规卷积，所以每组至少需要一个卷积核，即分组卷积输出通道数至少为g；

**分组卷积作用**

1. 减少运算量和参数量，相同输入输出大小的情况下，减少为原来的 $1/g$ .

2. 隔绝不同组的信息交换。
   1. 在某些情况下，如每个输出与输入的一部分特征图相关联时，分组卷积可以取得比常规卷积更好的性能，如输出通道为2，它们分别只与输入的1，2和3，4通道相关，这时最好使用g=2的分组卷积，相当于直接让模型将不相关的输入通道权重设置为零，加快模型收敛。
   2. 但对于需要考虑所有输入特征图信息的情况，分组卷积会降低模型的性能，对于这个问题，常常在两个分组卷积之间加入Channel_Shuffle模块打乱通道顺序，从而实现不同分组间的信息交换。

## Depthwise Conv 深度可分离卷积

也叫逐通道卷积。即将通道数为C的特征图将其拆分为C组，并用分别对他们进行单通道卷积，最后输出特征图通道数仍为C。

由于深度可分离卷积每个输出通道仅由输入的一个通道得来，缺乏了输入通道之间的信息交换，所以通常在后面加一个**1x1卷积**来实现通道间的信息交换，这个1x1卷积被称为Pointwise Convolution，这是MobileNet中提出来的结构，但是1X1卷积的运算量在轻量化模型中的比重很大，于是有的方法在设计去除1x1卷积的情况下，实现通道间的信息融合，如LiteHRNet,使用类似于通道注意力模块的方法来实现通道间的信息交换。

<img src="https://pic2.zhimg.com/80/v2-ed0a18edda90268493c6ce1a27dbc685_1440w.webp" style="zoom:67%;" />



## $1\times 1$ conv/Pointwise Conv 逐点卷积

在论文 Network In Network 中，提出了一个重要的方法：1×1 卷积。这个方法也在后面比较火的方法，如 googLeNet、ResNet、DenseNet ，中得到了非常广泛的应用。特别是在 googLeNet 的Inception中，发挥的淋漓尽致。

<img src="https://pic4.zhimg.com/80/v2-9fa17784edcb8483099e95920799c357_1440w.webp" style="zoom:60%;" />

如上图，当输入feature map为 $6\times 6 \times 32$，使用一个 $1\times 1\times 32$ 的卷积核，卷积核的通道数32是为了匹配输入 feature map，得到输出为 $6\times 6\times1$ 的特征图，起到了降维（改变通道数）的作用。

- 改变通道（channels）
  - 降维 / 升维
  - 数据融合
  - 减少计算量 
- 非线性操作：使整个模型相对更加的复杂，提升网络的表达能力。



## Channel Shuffle(ShuffleNet v1中提出)

为了降低计算量，当前先进的卷积网络通常在3×3卷积之前增加一个1×1卷积，用于通道间的信息流通与降维。然而在ResNeXt、MobileNet等高性能的网络中，1×1卷积却占用了大量的计算资源。 2017年的ShuffleNet v1从优化网络结构的角度出发，利用组卷积与通道混洗（Channel Shuffle）的操作有效降低了1×1逐点卷积的计算量，是一个极为高效的轻量化网络。

### 为什么要通道混洗

当前先进的轻量化网络大都使用**深度可分离卷积或者组卷积，以降低网络的计算量**，但这两种操作**都无法改变特征的通道数，因此还需要使用1×1卷积。** 总体来讲，**逐点的1×1卷积有以下特性：**

- 可以促进通道之间信息的融合，改变通道至指定维度。
- 轻量化网络中1×1卷积占据了大量的计算，并且致使通道之间充满约束，一定程度上降低了模型的精度。 为了进一步降低计算量，ShuffleNet提出了通道混洗的操作，通过通道混洗也可以完成通道之间信息的融合，具体结构如下图所示。

<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b159a6157f764f3e8a4fc364c7793569~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.image" style="zoom:50%;" />

上图中a图代表了**常规的两个分组卷积操作**，可以看到，**如果没有逐点的1×1卷积或者通道混洗，最终输出的特征仅由一部分输入通道的特征计算得出，这种操作阻碍了信息的流通，进而降低了特征的表达能力。** 因此，我们希望在一个组卷积之后，能够将特征图之间的通道信息进行融合，类似于图中b的操作，将每一个组的特征分散到不同的组之后，再进行下一个组卷积，这样输出的特征就能够包含每一个组的特征，而通道混洗恰好可以实现这个过程，如图的c图所示。 通道混洗可以通过几个常规的张量操作巧妙地实现，即 reshape、transpose、flatten.

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/5f165247dbe34078a18d02e7394a7570~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.png" style="zoom:60%;" />
