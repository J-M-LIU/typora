## feature map、卷积核、卷积核个数、filter、channel的概念解释

### feather map的理解

在cnn的每个卷积层，数据都是以三维形式存在的。你可以把它看成许多个二维图片叠在一起（像豆腐皮一样），其中每一个称为一个feature map。

### feather map 是怎么生成的？

**输入层**：在输入层，如果是灰度图片，那就只有一个feature map；如果是彩色图片，一般就是3个feature map（红绿蓝）。

**其它层**：层与层之间会有若干个卷积核（kernel）（也称为过滤器），上一层每个feature map跟每个卷积核做卷积，都会产生下一层的一个feature map，有N个卷积核，下层就会产生N个feather map。

### 多个feather map的作用是什么？

在卷积神经网络中，我们希望用一个网络模拟视觉通路的特性，分层的概念是自底向上构造简单到复杂的神经元。楼主关心的是同一层，那就说说同一层。
我们希望构造一组基，这组基能够形成对于一个事物完备的描述，例如描述一个人时我们通过描述身高/体重/相貌等，在卷积网中也是如此。在同一层，我们希望得到对于一张图片多种角度的描述，具体来讲就是用多种不同的卷积核对图像进行卷，得到不同核（这里的核可以理解为描述）上的响应，作为图像的特征。他们的联系在于形成图像在同一层次不同基上的描述。

下层的核主要是一些简单的边缘检测器（也可以理解为生理学上的simple cell）。

上层的核主要是一些简单核的叠加（或者用其他词更贴切），可以理解为complex cell。

多少个Feature Map？真的不好说，简单问题少，复杂问题多，但是自底向上一般是核的数量在逐渐变多（当然也有例外，如Alexnet），主要靠经验。

### 卷积核的理解

卷积核在有的文档里也称为==过滤器==（filter）：  

- 每个卷积核具有长宽深三个维度；

- 在某个卷积层中，可以有多个卷积核：下一层需要多少个feather map，本层就需要多少个卷积核。

#### 卷积核的形状

- 每个卷积核具有长、宽、深三个维度。在CNN的一个卷积层中：

- 卷积核的长、宽都是人为指定的，长X宽也被称为卷积核的尺寸，常用的尺寸为3X3，5X5等；卷积核的深度与当前图像的深度（feather map的张数）相同，所以指定卷积核时，只需指定其长和宽 两个参数。例如，在原始图像层 （输入层），如果图像是灰度图像，其feather map数量为1，则卷积核的深度也就是1；如果图像是grb图像，其feather map数量为3，则卷积核的深度也就是3.

## 什么是深度学习的卷积

### 2D卷积

2D卷积是一个相当简单的操作：我们先从一个小小的**权重矩阵**，也就是**卷积核（kernel）**开始，让它逐步在二维输入数据上“扫描”。卷积核“滑动”的同时，计算权重矩阵和扫描所得的数据矩阵的乘积，然后把结果汇总成一个输出像素。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/v2-f35f041aec9cbbf26b98af649536cc90_b.gif" style="zoom:50%;" />

那么为什么输出特征的会落入这个“大致区域”呢？这取决于卷积核的大小。卷积核的大小直接决定了在生成输出特征时，它合并了多少输入特征，也就是说：**卷积核越小，输入输出的位置越接近；卷积核越大，距离就越远**。



#### 常用技巧

##### Padding

上文中我们把5×5的特征矩阵转换成了3×3的特征矩阵，输入图像的边缘被“修剪”掉了，原始图像的边界丢失了许多有用信息.这是因为边缘上的像素永远不会位于卷积核中心，而卷积核也没法扩展到边缘区域以外。这是不理想的，通常我们都希望输入和输出的大小应该保持一致。

<img src="https://pic3.zhimg.com/v2-2a2307d5c20551f1a3e8458c7070cf16_b.webp" style="zoom:50%;" />

Padding就是针对这个问题提出的一个解决方案：它会用额外的“假”像素填充边缘（值一般为0）；因此当卷积核扫描输入数据时，它能延伸到边缘以外的伪像素，从而使输出和输入大小相同。



##### Strides

如果说Padding的作用是使输出与输入同高宽，那么在卷积层中，有时我们会需要一个尺寸小于输入的输出。那这该怎么办呢？这其实是卷积神经网络中的一种常见应用，当通道数量增加时，我们需要降低特征空间维度。实现这一目标有两种方法，一是使用池化层，二是使用Stride（步幅）。

![](https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/v2-294159b043a917ea622e1794b4857a34_b.gif)

滑动卷积核时，我们会先从输入的左上角开始，每次往右滑动一列或者往下滑动一行逐一计算输出，我们将每次滑动的行数和列数称为Stride，上图中，Stride=2。Stride的作用是成倍缩小尺寸，而这个参数的值就是缩小的具体倍数，如步幅为2，输出就是输入的1/2；步幅为3，输出就是输入的1/3。以此类推。

在一些目前比较先进的网络架构中，如ResNet，它们都选择**使用较少的池化层，在有缩小输出需要时选择步幅卷积**。



### 多通道卷积

大多数输入图像都有3个RGB通道，而通道数的增加意味着网络深度的增加。为了方便理解，我们可以把不同通道看成观察全图的不同“视角”，它或许会忽略某些特征，但一定也会强调某些特征。

![](https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/v2-82509ef17a589196264a0338ce003a61_r.jpg)

**卷积核和filter**

在只有一个通道的情况下，“卷积核”就相当于“filter”，这两个概念是可以互换的；但在一般情况下，它们是两个完全不同的概念。**每个“filter”实际上恰好是“卷积核”的一个集合**，在当前层，每个通道都对应一个卷积核，且这个卷积核是独一无二的。

卷积层中的每个filter有且只有一个输出通道——当filter中的各个卷积核在输入数据上滑动时，它们会输出不同的处理结果，其中一些卷积核的权重可能更高，而它相应通道的数据也会被更加重视（例：如果红色通道的卷积核权重更高，filter就会更关注这个通道的特征差异）。

![](https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/v2-c140fa14aea249cf34d6475f26b9d243_b.gif)

卷积核处理完数据后，形成了三个版本的处理结果，这时，filter再把它们加在一起形成一个总的通道。简而言之，卷积核处理的是不同通道的不同版本，而filter则是作为一个整体，产生一个整体的输出。

![](https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/v2-cdcbd860ec3e941858c1e092e7871d92_b.gif)



