## 什么是深度学习的卷积



<img src="https://pic3.zhimg.com/v2-b20dcefb0683dbe24ba6cf4598613242_b.webp" style="zoom:67%;" />

### 2D卷积

2D卷积是一个相当简单的操作：我们先从一个小小的**权重矩阵**，也就是**卷积核（kernel）**开始，让它逐步在二维输入数据上“扫描”。卷积核“滑动”的同时，计算权重矩阵和扫描所得的数据矩阵的乘积，然后把结果汇总成一个输出像素。

<img src="https://pic1.zhimg.com/v2-f35f041aec9cbbf26b98af649536cc90_b.webp" style="zoom:67%;" />

那么为什么输出特征的会落入这个“大致区域”呢？这取决于卷积核的大小。卷积核的大小直接决定了在生成输出特征时，它合并了多少输入特征，也就是说：**卷积核越小，输入输出的位置越接近；卷积核越大，距离就越远**。



#### 常用技巧

##### Padding

上文中我们把5×5的特征矩阵转换成了3×3的特征矩阵，输入图像的边缘被“修剪”掉了，原始图像的边界丢失了许多有用信息.这是因为边缘上的像素永远不会位于卷积核中心，而卷积核也没法扩展到边缘区域以外。这是不理想的，通常我们都希望输入和输出的大小应该保持一致。

<img src="https://pic3.zhimg.com/v2-2a2307d5c20551f1a3e8458c7070cf16_b.webp" style="zoom:50%;" />

Padding就是针对这个问题提出的一个解决方案：它会用额外的“假”像素填充边缘（值一般为0）；因此当卷积核扫描输入数据时，它能延伸到边缘以外的伪像素，从而使输出和输入大小相同。



##### Strides

如果说Padding的作用是使输出与输入同高宽，那么在卷积层中，有时我们会需要一个尺寸小于输入的输出。那这该怎么办呢？这其实是卷积神经网络中的一种常见应用，当通道数量增加时，我们需要降低特征空间维度。实现这一目标有两种方法，一是使用池化层，二是使用Stride（步幅）。

![](https://pic1.zhimg.com/v2-294159b043a917ea622e1794b4857a34_b.webp)

滑动卷积核时，我们会先从输入的左上角开始，每次往右滑动一列或者往下滑动一行逐一计算输出，我们将每次滑动的行数和列数称为Stride，上图中，Stride=2。Stride的作用是成倍缩小尺寸，而这个参数的值就是缩小的具体倍数，如步幅为2，输出就是输入的1/2；步幅为3，输出就是输入的1/3。以此类推。

在一些目前比较先进的网络架构中，如ResNet，它们都选择**使用较少的池化层，在有缩小输出需要时选择步幅卷积**。



### 多通道卷积

大多数输入图像都有3个RGB通道，而通道数的增加意味着网络深度的增加。为了方便理解，我们可以把不同通道看成观察全图的不同“视角”，它或许会忽略某些特征，但一定也会强调某些特征。

![](https://pic2.zhimg.com/v2-82509ef17a589196264a0338ce003a61_r.jpg)

**卷积核和filter**

在只有一个通道的情况下，“卷积核”就相当于“filter”，这两个概念是可以互换的；但在一般情况下，它们是两个完全不同的概念。**每个“filter”实际上恰好是“卷积核”的一个集合**，在当前层，每个通道都对应一个卷积核，且这个卷积核是独一无二的。

卷积层中的每个filter有且只有一个输出通道——当filter中的各个卷积核在输入数据上滑动时，它们会输出不同的处理结果，其中一些卷积核的权重可能更高，而它相应通道的数据也会被更加重视（例：如果红色通道的卷积核权重更高，filter就会更关注这个通道的特征差异）。

![](https://pic4.zhimg.com/v2-c140fa14aea249cf34d6475f26b9d243_b.webp)

卷积核处理完数据后，形成了三个版本的处理结果，这时，filter再把它们加在一起形成一个总的通道。简而言之，卷积核处理的是不同通道的不同版本，而filter则是作为一个整体，产生一个整体的输出。

![](https://pic3.zhimg.com/v2-cdcbd860ec3e941858c1e092e7871d92_b.webp)



