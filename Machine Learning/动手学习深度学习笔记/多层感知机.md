# Multilayer Perceptron

### 在网络中加入隐藏层

通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。



<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220920104010534.png" alt="image-20220920104010534" style="zoom:50%;" />

### 从线性到非线性

假设有n个样本，d个输入特征，q个输出类别，隐藏层中有h个隐藏单元。

隐藏层权重：$W^{(1)}\in R^{d\times h}$, 偏置：$b^{(1)} \in 1\times h $

输出层权重：$W^{(2)}\in R^{h\times q}$, 偏置：$b^{(2)} \in 1\times q $
$$
\begin{gathered}
H = XW^{(1)}+b^{(1)}\\
O = HW^{(2)}+b^{(2)}
\end{gathered}
$$
合并隐藏层后，不难发现叠加多层线性模型，最后仍得到的是等价的线性模型；仿射函数的仿射函数本身就是仿射函数，但是我们之前的 线性模型已经能够表示任何仿射函数：

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220920115301105.png" style="zoom:50%;" />

为了发挥多层架构的潜力，我们还需要一个额外的关键要素:在仿射变换之后对每个隐藏单元应用非线性的激活函数(activation function) $\sigma$. 有了激活函数，就不可能再将我们的多层感知机退化成线性模型.
$$
\begin{gathered}
H^{(1)} = \sigma_1 (XW^{(1)}+b^{(1)})\\
H^{(2)} = \sigma_2 (H^{(1)}W^{(2)}+b^{(2)})\\
O = H^{(2)}W^{(3)}+b^{(3)}
\end{gathered}
$$

### 激活函数

激活函数(activation function)通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。

#### ReLU函数：Rectified linear unit

Relu函数提供了简单的非线性变换，给定元素 $x$，ReLU函数被定义为元素x与0的最大值：
$$
ReLU(x) = max(x,0)
$$
通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220920164639979.png" alt="image-20220920164639979" style="zoom:50%;" />

##### 导数

当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。注意，当输入值精确等于0时， ReLU函数不可导。在此时，我们默认使用左侧的导数，即当输入为0时导数为0。我们可以忽略这种情况，因 为输入可能永远都不会是0。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220920164943723.png" alt="image-20220920164943723" style="zoom:50%;" />

**使用ReLU的原因**：**它求导表现得特别好:要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。**

#### sigmoid函数

对于一个定义域在$R$中的输入，sigmoid将输入压缩变换为区间(0,1)上的输出。可讲sigmoid看作softmax的特例。
$$
sigmoid(x) = \frac{1}{1+exp(-x)}
$$
<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220920165940322.png" alt="image-20220920165940322" style="zoom:50%;" />

- 点那个输入接近0，sigmoid函数接近线性变换
- 目前基本使用ReLU代替sigmoid

##### 导数

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220920170135578.png" alt="image-20220920170135578" style="zoom:50%;" />

#### tanh函数

将输入压缩变换到区间(-1,1)上。
$$
tanh(x) = \frac{1-exp(-2x)}{1+exp(-2x)}
$$

- 当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数， 不同的是tanh函数关于坐标系原点中心对称。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220920170407073.png" alt="image-20220920170407073" style="zoom:50%;" />



### 模型选择、欠拟合和过拟合

如何发现可以泛化到模式是机器学习的根本问题。

> ```
> 困难在于，当我们训练模型时，我们只能访问数据中的小部分样本。最大的公开图像数据集包含大约一百万
> 张图像。而在大部分时候，我们只能从数千或数万个数据样本中学习。在大型医院系统中，我们可能会访问
> 数十万份医疗记录。当我们使用有限的样本时，可能会遇到这样的问题:当收集到更多的数据时，会发现之
> 前找到的明显关系并不成立。
> ```

