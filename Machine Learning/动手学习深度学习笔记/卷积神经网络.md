# 卷积神经网络 Convolutional neural networks



### 从全连接层到卷积



#### 不变性

**平移不变性(translation invariance)**: 不管检测对象出现在图像中的哪个位置，神经网络的前面几层 应该对相同的图像区域具有相似的反应；

**局部性(locality)**: 神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。





#### 卷积

数学中，两个函数 $g，f$之间的卷积定义为：
$$
(f*g)(\mathbf{x}) = \int f(\mathbf{z})g(\mathbf{x}-\mathbf{z})d\mathbf{z}
$$
也就是说，卷积是当把一个函数“翻转”并移位$x$时，测量f 和$g$之间的重叠.

离散对象中：
$$
(f*g)(i) = \sum_a f(a)g(i-a)
$$
对于二维张量：
$$
(f*g)(i,j) = \sum_a\sum_b f(a,b)g(i-a,j-b)
$$

#### 互相关运算 cross-correlation

卷积核的高度和宽度都是2

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220922233521867.png" alt="image-20220922233521867" style="zoom:50%;" />

卷积窗口从输入张量的左上⻆开始，从左到右、从上到下滑动。当卷积窗口滑动到新 一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘。

输入长 X 宽：$n_h \times n_w$ ,卷积核长 X 宽：$k_h\times k_w$, 输出大小为：
$$
(n_h - k_h+1)\times (n_w - k_w + 1)
$$
