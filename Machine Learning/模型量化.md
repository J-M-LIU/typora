## 量化



### 模型压缩



#### 模型剪枝



#### 模型量化



#### 知识蒸馏



### 量化



#### 量化分类

<img src="https://user-images.githubusercontent.com/52520497/95644539-e7f23500-0ae9-11eb-80a8-596cfb285e17.png" style="zoom:50%;" />



##### 动态离线量化



##### 静态离线量化



##### 量化感知训练



#### 后训练量化 Post Training Quantization



##### 基本原理

我们通常会将一张 uint8 类型、数值范围在 0~255 的图片归一成 float32 类型、数值范围在 0.0~1.0 的张量，这个过程就是**反量化**。类似地，我们经常将网络输出的范围在 0.0~1.0 之间的张量调整成数值为 0~255、uint8 类型的图片数据，这个过程就是**量化**。所以量化本质上只是对数值范围的重新调整，可以「粗略」理解为是一种线性映射。

显然，反量化一般没有信息损失，而量化一般都会有精度损失。这也非常好理解，float32 能保存的数值范围本身就比 uint8 多，因此必定有大量数值无法用 uint8 表示，只能四舍五入成 uint8 型的数值。量化模型和全精度模型的误差也来自四舍五入的 clip 操作。

量化其实就是将训练好的深度神经网络的权值，激活值等从高精度转化成低精度的操作过程，并保证精度不下降的过程。如何从高精度转到低精度呢？**在定点与浮点等数据之间建立一种数据映射关系，将信号的连续取值近似为有限多个离散值，并使得以较小的精度损失代价获得了较好的收益。**

**模型量化以损失推理精度为代价，将网络中连续取值或离散取值的浮点型参数（ 权重或张量）线性映射为定点近似（int8 / uint8）的离散值，取代原有的 float32 格式数据，同时保持输入输出为浮点型，从而达到减少模型尺寸大小、减少模型内存消耗及加快模型推理速度等目标。定点量化如下图：**

<img src="https://edit.wpgdadawant.com/uploads/news_file/blog/2020/2007/tinymce/1_1.jpg" style="zoom:50%;" />

用 $r$ 表示浮点实数，$q$ 表示量化后的定点整数。浮点和整型之间的换算公式为：
$$
r = S(q-Z)\\
q = round(\frac{r}{S}+Z)
$$
$S$ 是 scale，表示实数和整数之间的比例关系，$Z$ 是 zero point，表示实数中的 0 经过量化后对应的整数，它们的计算方法为：
$$
S = \frac{r_{max}-r_{min}}{q_{max}-q_{min}}\\
Z = round(q_{max} - \frac{r_{max}}{S})
$$


##### 矩阵运算的量化

假设 $r_1$、$r_2$是浮点实数上的两个 $N\times N$矩阵，$r_3$为其相乘后的结果矩阵：
$$
r_3^{i,k} = \sum_{j=1}^N r_1^{i,j}r_2^{j,k}
$$
假设 $S_1$ $S_2$、$Z_1$ $Z_2$ 是矩阵对应的 scale 和 zero point：
$$
S_3(q_3^{i,k}-Z_3) = \sum_{j=1}^N S_1(q_1^{i,j}-Z_1) S_2(q_2^{i,k}-Z_2)
$$
所以：
$$
q_3^{i,k} = \frac{S_1 S_2}{S_3} \sum_{j=1}^N (q_1^{i,j}-Z_1) (q_2^{i,k}-Z_2) + Z_3
$$


##### 卷积网络的量化

定义一个这样的网络，将conv、relu、fc三个模块量化。假设conv、fc的参数是 $w_1, w_2$，中间层的feature map为 $a_1,a_2$。



<img src="https://pic1.zhimg.com/80/v2-8aeb50d76358ee1f6e88c33916b57200_1440w.webp" style="zoom:50%;" />
$$
a_1^{i,k} = \sum_{j=1}^N x^{i,j}w_1^{j,k}
$$
进而得到
$$
q_{a_1}^{i,k} = \frac{S_x S_{w_1}}{S_{a_1}} \sum_{j=1}^N (q_x^{i,j}-Z_x) (q_{w_1}^{i,k}-Z_{w_1}) + Z_{a_1}
$$
对于量化的relu，$q_{a_2}=max(q_{a_1},Z_{a_1})$, 并且 $Z_{a_1} = Z_{a_2}$，$S_{a_1}=S_{a_1}$，量化后的fc层为：
$$
q_y^{i,k} = \frac{S_{a_2} S_{w_2}}{S_y} \sum_{j=1}^N (q_{a_2}^{i,j}-Z_{a_2}) (q_{w_2}^{i,k}-Z_{w_2}) + Z_y
$$
以FP32到int8为例，核心思想就是将浮点数区间的参数映射到INT8的离散区间中；其中 $r_{min},r_{max}$是fp32浮点数的最小值和最大值，$q_{max}=128$ 、 ${q_{min}=-127}$ 分别为量化后的最大值与最小值.



##### 确定合适的量化参数

有了权重和特征的数值范围后，一种很直接的方法就是根据数值范围的大小来确定 $r_{min}$、$r_{max}$。这种方法容易受到噪声的影响，比如，有些 weight 或者 feature 中可能存在离群点，数值较大，但对结果影响又很小，如果把这些数值也统计到 min、max 里，就容易造成浪费。比如，某个 weight 数值是 [-0.1, 0.2, 0.3, 255.1]，则统计出来的 min、max 为 -0.1 和 255.1，如此一来，0.2、0.3 这样的数值就会被映射到同一个定点数，信息损失相当严重，而它们对结果影响可能远大于 255.1。因此在这种情况下，我们宁愿把 255.1 损失掉，也希望尽可能把 0.2、0.3 保持下来。



###### 公式重写

**非对称量化**
$$
x_{int} = round(x/ \Delta)+z\\
x_Q = clamp(0,N_{levels}-1,x_{int}), \ \ \ if\ unsigned
$$
de-quantization is:
$$
x_{float} = (x_Q-z)\Delta
$$


**对称量化**
$$
x_{int} = round(x/ \Delta)\\
x_Q = clamp(0,N_{levels}-2,x_{int}), \ \ \ if \ unsigned\\
x_Q = clamp\big(-(N_{levels}/2)-1,(N_{levels}/2)-1, x_{int}\big),\ \ \ if \ signed
$$
de-quantization is:
$$
x_{out} = x_q\Delta 
$$


#### 量化感知训练 Quantization Aware Training

即在量化的过程中，对网络进行训练，从而让网络参数能更好地适应量化带来的信息损失。



##### Straight Through Estimate: STE 直通估计

在后训练量化中，前向传播中对weight进行量化，存在`round`函数。

```python
def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False):
    if signed:
        qmin = - 2. ** (num_bits - 1)
        qmax = 2. ** (num_bits - 1) - 1
    else:
        qmin = 0.
        qmax = 2.**num_bits - 1.
 
    q_x = zero_point + x / scale
    q_x.clamp_(qmin, qmax).round_()
    
    return q_x.float()
```

函数图像如下：

<img src="https://pic3.zhimg.com/80/v2-d3ad7130bfa530d5f7bafa57e0446266_1440w.webp" style="zoom: 80%;" />

函数几乎每一处的梯度都是 0，如果网络中存在该函数，会导致反向传播的梯度也变成 0，会导致反向传播梯度无法更新，不能训练下去。如何解决这个问题呢？直接跳过伪量化的过程，避开 `round`。直接把卷积层的梯度回传到伪量化之前的 weight 上。这样一来，由于卷积中用的 weight 是经过伪量化操作的，因此可以模拟量化误差，把这些误差的梯度回传到原来的 weight，又可以更新权重，使其适应量化产生的误差，量化训练则可以正常进行。

<img src="https://pic1.zhimg.com/80/v2-1219d3fbb6b97ffc083acb62848b9134_1440w.webp" style="zoom:80%;" />



即通过量化->反量化，相当于做了一遍float->uint->float，最后得到的数值依然是float。二值网络中，前向传播的过程中，先用sign函数对浮点型参数二值化处理然后再参与到运算，而此时并没有把这个浮点型数值抛弃掉，而是暂时在内存中保存起来。前向传播完之后，用STE将二值参数的梯度作为对应的浮点型参数的梯度来更新浮点参数。

```python
class FakeQuantize(Function):

    @staticmethod
    def forward(ctx, x, qparam):
        x = qparam.quantize_tensor(x)
        x = qparam.dequantize_tensor(x)
        return x

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output, None

class QConv2d(QModule):
    def forward(self, x):
        if hasattr(self, 'qi'):
            self.qi.update(x)
            x = FakeQuantize.apply(x, self.qi)
            ......
```

在 backward 中，直接返回后一层传过来的梯度 `grad_output`，相当于直接跳过了伪量化这一层的梯度计算，让梯度直接流到前一层 (Straight Through).

https://blog.csdn.net/m0_50617544/article/details/121596406
