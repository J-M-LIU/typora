## 模型量化



### 模型压缩



#### 模型剪枝



#### 模型量化



#### 知识蒸馏



### 量化



#### 量化分类

<img src="https://user-images.githubusercontent.com/52520497/95644539-e7f23500-0ae9-11eb-80a8-596cfb285e17.png" style="zoom:50%;" />



##### 动态离线量化



##### 静态离线量化



##### 量化感知训练



#### 量化方法



##### 非对称量化



##### 对称量化



##### 随机量化



#### 后训练量化



##### 基本原理

我们通常会将一张 uint8 类型、数值范围在 0~255 的图片归一成 float32 类型、数值范围在 0.0~1.0 的张量，这个过程就是**反量化**。类似地，我们经常将网络输出的范围在 0.0~1.0 之间的张量调整成数值为 0~255、uint8 类型的图片数据，这个过程就是**量化**。所以量化本质上只是对数值范围的重新调整，可以「粗略」理解为是一种线性映射。

显然，反量化一般没有信息损失，而量化一般都会有精度损失。这也非常好理解，float32 能保存的数值范围本身就比 uint8 多，因此必定有大量数值无法用 uint8 表示，只能四舍五入成 uint8 型的数值。量化模型和全精度模型的误差也来自四舍五入的 clip 操作。

量化其实就是将训练好的深度神经网络的权值，激活值等从高精度转化成低精度的操作过程，并保证精度不下降的过程。如何从高精度转到低精度呢？**在定点与浮点等数据之间建立一种数据映射关系，将信号的连续取值近似为有限多个离散值，并使得以较小的精度损失代价获得了较好的收益。**

**模型量化以损失推理精度为代价，将网络中连续取值或离散取值的浮点型参数（ 权重或张量）线性映射为定点近似（int8 / uint8）的离散值，取代原有的 float32 格式数据，同时保持输入输出为浮点型，从而达到减少模型尺寸大小、减少模型内存消耗及加快模型推理速度等目标。定点量化如下图：**

<img src="https://edit.wpgdadawant.com/uploads/news_file/blog/2020/2007/tinymce/1_1.jpg" style="zoom:50%;" />

用 $r$ 表示浮点实数，$q$ 表示量化后的定点整数。浮点和整型之间的换算公式为：
$$
r = S(q-Z)\\
q = round(\frac{r}{S}+Z)
$$
$S$ 是 scale，表示实数和整数之间的比例关系，$Z$ 是 zero point，表示实数中的 0 经过量化后对应的整数，它们的计算方法为：
$$
S = \frac{r_{max}-r_{min}}{q_{max}-q_{min}}\\
Z = round(q_{max} - \frac{r_{max}}{S})
$$


##### 矩阵运算的量化

假设 $r_1$、$r_2$是浮点实数上的两个 $N\times N$矩阵，$r_3$为其相乘后的结果矩阵：
$$
r_3^{i,k} = \sum_{j=1}^N r_1^{i,j}r_2^{j,k}
$$
假设 $S_1$ $S_2$、$Z_1$ $Z_2$ 是矩阵对应的 scale 和 zero point：
$$
S_3(q_3^{i,k}-Z_3) = \sum_{j=1}^N S_1(q_1^{i,j}-Z_1) S_2(q_2^{i,k}-Z_2)
$$
所以：
$$
q_3^{i,k} = \frac{S_1 S_2}{S_3} \sum_{j=1}^N (q_1^{i,j}-Z_1) (q_2^{i,k}-Z_2) + Z_3
$$


##### 卷积网络的量化

定义一个这样的网络，将conv、relu、fc三个模块量化。假设conv、fc的参数是 $w_1, w_2$，中间层的feature map为 $a_1,a_2$。



<img src="https://pic1.zhimg.com/80/v2-8aeb50d76358ee1f6e88c33916b57200_1440w.webp" style="zoom:50%;" />
$$
a_1^{i,k} = \sum_{j=1}^N x^{i,j}w_1^{j,k}
$$
进而得到
$$
q_{a_1}^{i,k} = \frac{S_x S_{w_1}}{S_{a_1}} \sum_{j=1}^N (q_x^{i,j}-Z_x) (q_{w_1}^{i,k}-Z_{w_1}) + Z_{a_1}
$$
对于量化的relu，$q_{a_2}=max(q_{a_1},Z_{a_1})$, 并且 $Z_{a_1} = Z_{a_2}$，$S_{a_1}=S_{a_1}$，量化后的fc层为：
$$
q_y^{i,k} = \frac{S_{a_2} S_{w_2}}{S_y} \sum_{j=1}^N (q_{a_2}^{i,j}-Z_{a_2}) (q_{w_2}^{i,k}-Z_{w_2}) + Z_y
$$
以FP32到int8为例，核心思想就是将浮点数区间的参数映射到INT8的离散区间中；其中 $r_{min},r_{max}$是fp32浮点数的最小值和最大值，$q_{max}=128$ 、 ${q_{min}=-127}$ 分别为量化后的最大值与最小值.



https://blog.csdn.net/m0_50617544/article/details/121596406
