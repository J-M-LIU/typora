# 卷积神经网络 Convolutional neural networks



## 从全连接层到卷积



### 不变性

​	**平移不变性(translation invariance)**: 不管检测对象出现在图像中的哪个位置，神经网络的前面几层 应该对相同的图像区域具有相似的反应；

​	**局部性(locality)**: 神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。



#### 卷积

数学中，两个函数 $g，f$之间的卷积定义为：
$$
(f*g)(\mathbf{x}) = \int f(\mathbf{z})g(\mathbf{x}-\mathbf{z})d\mathbf{z}
$$
也就是说，卷积是当把一个函数“翻转”并移位$x$时，测量f 和$g$之间的重叠.

离散对象中：
$$
(f*g)(i) = \sum_a f(a)g(i-a)
$$
对于二维张量：
$$
(f*g)(i,j) = \sum_a\sum_b f(a,b)g(i-a,j-b)
$$

#### 互相关运算 cross-correlation

卷积核的高度和宽度都是2

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220922233521867.png" alt="image-20220922233521867" style="zoom:50%;" />

​	卷积窗口从输入张量的左上⻆开始，从左到右、从上到下滑动。当卷积窗口滑动到新 一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘。

输入长 X 宽：$n_h \times n_w$ ,卷积核长 X 宽：$k_h\times k_w$, 输出大小为：
$$
(n_h - k_h+1)\times (n_w - k_w + 1)
$$



### Padding

​	通常，如果我们添加$p_h$ 行填充(大约一半在顶部，一半在底部)和$p_w$ 列填充(左侧大约一半，右侧一半)，则输出形状将为
$$
(n_h −k_h +p_h +1)×(n_w −k_w +p_w +1)
$$
<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220922235600677.png" alt="image-20220922235600677" style="zoom:50%;" />

​	即输出高度和宽度分别增加 $p_h$和 $p_w$。

​	在许多情况下，我们需要设置$p_h = k_h − 1$和$p_w = k_w − 1$，使输入和输出具有相同的高度和宽度。这样可以在 构建网络时更容易地预测每个图层的输出形状。假设$k_h$ 是奇数，我们将在高度的两侧填充$p_h /2$行。如果$k_h$ 是 偶数，则一种可能性是在输入顶部填充<br>⌈$p_h /2$⌉行，在底部填充⌊$p_h /2$⌋行。同理，我们填充宽度的两侧。



### Stride

通常，当垂直步幅为$s_h$ 、水平步幅为$s_w$ 时，输出形状为:
$$
⌊(n_h −k_h +p_h +s_h)/s_h⌋×⌊(n_w −k_w +p_w +s_w)/s_w⌋
$$
在实践中，我们很少使用不一致的步幅或填充

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220923095020904.png" alt="image-20220923095020904" style="zoom:50%;" />

**通用公式**
$$
feature\ map\ size:((n_h −k_h +2* p_h)/s_h + 1)×((n_w −k_w + 2 * p_w )/s_w + 1)
$$
padding = "same" ，仅当stride=1时，padding = $(k-1)/2$



### 汇聚层 Pooling

​	池化层不包含参数，其运算是确定性的，通常计算汇聚窗口中所有元素的最大值或平均值。分别称为**最大汇聚层(maximum pooling)**和**平均汇聚层(average pooling)**。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20220923101626049.png" alt="image-20220923101626049" style="zoom:50%;" />



### 批量规范化

- 需要标准化输入特征 $\mathbf{X}$，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与优化器配合使用，因为它可以将参数的量级进行统一。
- 随着训练学习率的补偿调整
- 更深层网络更容易过拟合，**正则化**。

​	每次训练迭代中，首先规范化输 入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。之后应用比例系数和比例偏移。即批量规范化。而选择合适大小的批量也很重要。
$$
\hat{\mu}_{\Beta} = \frac{1}{|\Beta|}\sum_{\mathbf{x}\in \Beta}\mathbf{x} \\
\hat{\sigma}_{\Beta} = \frac{1}{|\Beta|}\sum_{\mathbf{x}\in \Beta}(\mathbf{x}-\hat{\mu}_{\Beta})^2 + \epsilon
$$
批量规范化BN根据以下式子转换 $\mathbf{x}$ , $\gamma$-拉伸参数，$\beta$-偏移量。
$$
BN(\mathbf{x}) = \gamma ·\ \frac{\mathbf{x}-\hat{\mu}_{\Beta}}{\hat{\sigma}_{\Beta}} + \beta
$$


### 残差网络 ResNet



#### 为什么不能简单地增加网络层数

​	对于传统的CNN网络，简单的增加网络的深度，容易**导致梯度消失和爆炸**。针对梯度消失和爆炸的解决方法一般是**正则初始化(**normalized initialization**)**和**中间的正则化层(**intermediate normalization layers**)，**但是这会导致另一个问题，**退化问题**，随着网络层数的增加，在**训练集上的准确率却饱和甚至下降**了。这个和过拟合不一样，因为过拟合在训练集上的表现会更加出色。

​	按照常理更深层的网络结构的解空间是包括浅层的网络结构的解空间的，也就是说深层的网络结构能够得到更优的解，性能会比浅层网络更佳。但是实际上并非如此，深层网络无论从训练误差或是测试误差来看，都有可能比浅层误差更差，这也证明了并非是由于过拟合的原因。导致这个原因可能是因为**随机梯度下降的策略**，往往解到的并不是全局最优解，而是局部最优解，**由于深层网络的结构更加复杂，所以梯度下降算法得到局部最优解的可能性就会更大**。

#### 如何解决退化问题

既然深层网络相比于浅层网络具有退化问题，那么是否可以保留深层网络的深度，又可以有浅层网络的优势去避免退化问题呢？如果将深层网络的后面若干层学习成恒等映射 $h(x)=x$ ，那么模型就退化成浅层网络。但是直接去学习这个恒等映射是很困难的，那么就换一种方式，把网络设计成：
$$
H(\mathbf{x}) = F(\mathbf{x}) + \mathbf{x} \Longrightarrow F(\mathbf{x}) = H(\mathbf{x}) - \mathbf{x}
$$
$\mathbf{x}$为输入，$F(\mathbf{x})$为经过线性变换和激活后的输出，

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221018092235296.png" alt="image-20221018092235296" style="zoom:50%;" />

[resnet 解析](https://zhuanlan.zhihu.com/p/72679537?utm_source=wechat_session)
