## skip connect/residual connections 残差连接

**skip connect的思想**：将输出表述为输入和输入的一个非线性变换的线性叠加。F包含了卷积、激活等操作。

<img src="https://img-blog.csdnimg.cn/20200502093404478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODUyNjc2,size_16,color_FFFFFF,t_70" style="zoom:40%;" />

**为什么要skip connect**

首先大家已经形成了一个通识，在一定程度上，网络越深表达能力越强，性能越好。但随着网络深度的增加，带来了许多问题：梯度消失、梯度爆炸。在Resnet出来之前尝试过各类优化方法：更好的初始化策略，BN层，Relu等各种激活函数，但改善问题的能力有限，直到残差连接被广泛使用。

深度学习依靠误差的链式反向传播来进行参数更新，假如有这样一个函数，$fgk$ 分别为卷积、激活、分类器
$$
\mathbf{z} = f(\mathbf{x},\mathbf{w_f})\\
\mathbf{h}=g(\mathbf{z})\\
\mathbf{\hat{y}}=k(\mathbf{h})\\
L = l(\mathbf{y},\mathbf{\hat{y}})
$$
其中 $L$ 对 $\mathbf{w_f}$ 的导数为：
$$
\frac{\partial{l}}{\partial{\mathbf{\hat{y}}}} \times \frac{\partial{\mathbf{\hat{y}}}}{\partial{\mathbf{h}}} \times \frac{\partial{\mathbf{h}}}{\partial{\mathbf{z}}} \times \frac{\partial{\mathbf{z}}}{\partial{{\mathbf{w_f}}}}
$$
一旦其中某一个导数很小，多次连乘后梯度可能越来越小，这就是常说的梯度消失，对于深层网络，传到浅层几乎就没了。但是如果使用了残差，每一个导数就加上了一个恒等项1，此时就算原来的导数df/dx很小，这时候误差仍然能够有效的反向传播。

假设残差网络 $H =F(\mathbf{x})+\mathbf{x}$，非残差网络 $G = F(\mathbf{x})$，则
$$
\frac{\partial{H}}{\partial{\mathbf{x}}} = \frac{\partial F(\mathbf{x})}{\partial{\mathbf{x}}}+1\\
\frac{\partial{G}}{\partial{\mathbf{x}}} = \frac{\partial F(\mathbf{x})}{\partial{\mathbf{x}}}
$$
