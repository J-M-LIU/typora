# 最优化方法课程大作业



## Task1: 核范数，矩阵补全，条件梯度

### 条件梯度法	

条件梯度法的思想是在每一次迭代中通过求解一个线性的子问题得到最优 的下降方向，在更新步时与上一步的迭代点凸组合得到下一步的迭代点。其算法描述如下:
$$
𝑚𝑖𝑛_𝑥 = 𝑓(𝑥)
$$
在当前迭代点 $x^{(k)}$​处，该算法考虑目标函数的线性化，并求解出该线性函数的最小值(定义域相同)。迭代算法为：
$$
𝑥_{𝑘+1} = 𝑥_𝑘 + 𝜂_𝑘(\hat{x}_𝑘 − 𝑥_𝑘)
$$
其中: $\hat{x}_𝑘 = argmin _{𝑥∈Ω}〈∇𝑓(𝑥_𝑘),𝑥〉$

### 核范数

核范数即为 1-范数或者迹范数，可以由矩阵 A 的奇异值分解来计算，其计算方法为:
$$
‖𝐴‖_∗ =\sum_i \sigma_𝑖(𝐴)
$$
核范数的单位球定义为:
$$
B_*^{m\times n} = \{A\in R^{m\times n} ||A||_* \leq1 \}
$$
将矩阵 A(核范数球外)投影到核范数球内:
$$
\prod_{B_{*}}(A)=\underset{X \in B_{*}}{\arg \min }\|A-X\|_{F}^{2}
$$
可以将该问题化为 A 矩阵 SVD 分解后的奇异值向量在单纯形上的投影:
$$
\begin{array}{c}
A=U S V^{T} \\
S=\operatorname{diag}\left\{\sigma_{1, \cdots, \sigma_{k}}\right\} \\
\Delta_{n}=\left\{x \in R^{k}: \sum_{i=1}^{k} x_{i}=1, x \geq 0\right\} \\
\prod_{\Delta_{n}}(S)=\underset{X \epsilon_{\Delta_{n}}}{\arg \min } \frac{1}{2}\|S-X\|_{2}^{2}
\end{array}
$$
该问题为凸规划问题，只要求 KKT 点即可:
$$
\begin{array}{c}
L(S, \theta)=\frac{1}{2}\|S-X\|_{2}^{2}+\theta \sum_{i=1}^{k} x_{i} \\
S-X+\theta \underbrace{[1, \cdots, 1]^{T}}_{k}=0 \\
\sum_{i=1}^{k} x_{i}=1
\end{array}
$$
解得
$$
\begin{array}{c}
\theta=\frac{1}{n}\left(\sum_{i=1}^{k} \sigma_{i}-1\right) \\
x_{i}=\sigma_{i}-\theta=\sigma_{i}-\frac{1}{n}\left(\sum_{i=1}^{k} \sigma_{i}-1\right)
\end{array}
$$
但由于奇异值需要为正，在计算得到投影结果后需要将 $x_i$ 小于 0 的数进行截断。 最终得到在核范数球里的投影为
$$
\Pi_{B *}(A)=U X V^{T}
$$

### 投影梯度下降法

​	梯度投影法(gradient projection method)利用梯度的投影技巧求约束非线性 规划问题最优解的一种方法。
​	求带线性约束的非线性规划问题更为有效。它是从一个基本可行解开始，由 约束条件确定出凸约束集边界上梯度的投影，以便求出下次的搜索方向和步长。 每次搜索后，都要进行检验，直到满足精度要求为止。这种方法是罗森于 1960 年提出的，戈德福布和拉匹塔斯于 1968 年作了改进。
​	本次课程作业使用的是投影梯度下降法(PGDA)，方法主要用于在凸集上极小化凸函数，算法复杂性为 $O(1/\sqrt{t})$。因为每轮梯度下降迭代的点可能在投影区域外，要多加一个投影步将其投影回凸集约束上。因此投影梯度法分为两步，梯度步和投影步。
​	在该问题中投影梯度法主要分为以下两步:
​	梯度步：  $Y_{t+1}=X_{t}-\eta \nabla f\left(X{t}\right)$ 
​	投影步：  $X{t+1}=\Pi_{B .}\left(Y_{t+1}\right)$ 
​	由于高维矩阵投影在核范数单位球上所花费的时间较长，这也进一步拖慢了 优化问题的求解。而根据之前的分析，条件梯度法无需计算投影步，只需要求解 线性最优化 oracle，能有效缩短时间。

### 条件梯度法

​	条件梯度法，也被称为 FrankWolfe(FW)算法，该方法的出现是为了解决 在某些情况下，投影梯度下降法中的存在低效投影步骤的问题。该方法是投影梯 度下降法的替代版本，因此该方法也是用于在有界凸集上极小化函数。并且该算法的复杂性在函数光滑凸的条件下为 $O(1/t)$。
​	在该问题中投影梯度法主要分为以下两步:
​	线性优化步: $\bar{X}_{t}=\underset{X \in B^{m n n}}{\arg \max }\langle Z, X\rangle$
​	更新步 : $ X_{t+1}=X_{t}+\eta_{t}\left(\bar{X}_{t}-X_{t}\right)$

​	而对于矩阵补全问题，线性优化问题转变为如下所示：
$$
\begin{array}{c}
\bar{X}_{t} \in \underset{X \in B_{*}}{\operatorname{argmin}} \nabla f\left(X_{t}\right)^{\top} X \\
\nabla f(X)=Y-X \odot O
\end{array}
$$
根据对核范数单位球是秩-1 矩阵的凸包的分析，为了解决这个线性优化问 题，仅需要计算矩阵最大左奇异值的方法，为此引入经典幂法，如下所示：
$$
Pick\ a \ random\  unit \ vector \  x_{1} \  and \ let \  y_{1}=A^{\top} x /\left\|A^{\top} x\right\| .\\
From  \ k=1  \ to \  k=T-1  :\\
- Put \  x_{k+1}=\frac{A y_{k}}{\left\|A y_{k}\right\|} \\
- Put \  y_{k+1}=\frac{A^{\top} x_{k+1}}{\left\|A^{\top} x_{k+1}\right\|} \\
Return \  x_{T}\   and \  y_{T} \  as \ approximate \ top\  left\  and \ right \ singular\  vectors.
$$

### 实验和结果

#### 核范数球投影时间分析

​	随机生成 1000、2000、3000 和 4000 维的矩阵，分别投影到单位核范数球内。1000 维矩阵的投影时间为：0.93s，2000 维为 4.61s，3000 维为 13.53s，4000 维为 30.28s，如图所示。从图中可以看出，随着维数增加，投影步所需要的时间增加， 当矩阵维数过大时，采用投影梯度法进行优化的时间将过长。
​	核范数求解不是一个简单的操作。它需要输入矩阵的完整 SVD，通常需要 步数的立方倍的迭代次数。以程序 5-1-nuclear norm projection 为例子说 明，即使在较小的步数大小上，单个投影步骤也是耗时的。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218140301938.png" alt="image-20221218140301938" style="zoom:40%;" />

#### 低阶矩阵补全

​	利用随机数生成秩为 10，维数为 1000 的矩阵，将其表示为A=USU^T^ 的形式，S 为由奇异值组成的对角阵，对 A 进行数据采集，创建损失函数计算函数。

```python
# random rank-10 matrix normalized to have nuclear norm 1 U = np.random.normal(0, 1, (n, k))
U = np.linalg.qr(U)[0]
S = np.diag(np.random.uniform(0, 1, k))
S /= np.sum(S)
A = U.dot(S.dot(U.T))
# pick which entries we observe uniformly at random O = np.random.randint(0,2, (n, n))
# multiply A by O coordinate-wise
Y = np.multiply(A, O)
def mc_objective(Y, O, X):
  """Matrix completion objective."""
  return 0.5 * np.linalg.norm(Y-np.multiply(X, O), 'fro')**2
def mc_gradient(Y, O, X):
"""Gradient of matrix completion objective.""" 
	return np.multiply(X, O) - Y
```

​	利用投影梯度下降算法作为优化器对预测矩阵进行优化:

```python
# start from random matrix of nuclear norm 1
X0 = np.random.normal(0,1, (n,n))
X0 = nuclear_projection(X0.dot(X0.T))
objective = lambda X: mc_objective(Y, O, X)
gradient = lambda X: mc_gradient(Y, O, X)
Xs = gradient_descent(X0, [0.2]*40, gradient, nuclear_projection)
```

​	得到的范数误差如下:

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218140816506.png" alt="image-20221218140816506" style="zoom:50%;" />

​	从上图中可以看出投影梯度法优化过程中，目标函数值递减，并且补全矩阵和原矩阵的误差不断缩小。接下来进一步分析补全矩阵的奇异值，如图下图所示。从图中可以看到补全矩阵在奇异值的还原上并不理想，原矩阵 10 个奇异值，补全矩阵 第 11 个奇异值开始还具有一定的值，并未趋于 0。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218141116268.png" alt="image-20221218141116268" style="zoom:50%;" />

​	利用条件梯度下降算法作为优化器对预测矩阵进行优化:

```python
def conditional_gradient(initial, steps, oracle):
  xs = [initial]
  for step in steps:
  xs.append(xs[-1] + step*(oracle(xs[-1])-xs[-1])) return xs
def power_method(A, num_steps=10): m, n = A.shape
  x = np.random.normal(0,1, m) x /= np.linalg.norm(x)
  y = A.T.dot(x)
  y /= np.linalg.norm(y)
  for _ in range(num_steps):
    x = A.dot(y)
    x /= np.linalg.norm(x) y = A.T.dot(x)
    y /= np.linalg.norm(y)
    return x, y
```

​	线性优化搜索归结为计算矩阵负梯度的最佳一阶近似.

```python
def mc_oracle(Y, O, X):
    """Linear optimization oracle for matrix completion.""" G = mc_gradient(Y, O, X)
    x, y = power_method(-G)
    return x.reshape((len(x), 1)).dot(y.reshape((1,len(y))))
n, k = 1000, 10
# random rank-10 matrix normalized to have nuclear norm 1
U = np.random.normal(0, 1, (n, k))
U = np.linalg.qr(U)[0]
S = np.diag(np.random.uniform(0, 1, k))
S /= np.sum(S)
A = U.dot(S.dot(U.T))
# pick which entries we observe uniformly at random O = np.random.randint(0,2, (n, n))
# multiply A by O coordinate-wise
Y = np.multiply(A, O)
X0 = np.random.normal(0,1, (n,n))
X0 = nuclear_projection(X0.dot(X0.T))
oracle = lambda X: mc_oracle(Y, O, X) steps = [2./(k+2.) for k in range(1, 40)] Xs = conditional_gradient(X0, steps, oracle)
```

​	得到的范数误差如下:

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218141449489.png" alt="image-20221218141449489" style="zoom:50%;" />

​	对奇异值分解后的最大奇异值进行比较:	

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218141529224.png" alt="image-20221218141529224" style="zoom:40%;" />

​	比较矩阵范数误差和奇异值的误差发现两个优化器效果都很好，误差较小比较两者优化计算所需时间:	

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218141627213.png" alt="image-20221218141627213" style="zoom:40%;" />

​	从图中可知，条件梯度法优化过程中，目标函数值和实际误差总体上是减小的，虽然具有上升的波动，但最终误差收敛到一个较低的水平。同样的迭代步数下和 投影梯度法相比，条件梯度法的最终误差大体相大，但所花费时间要明显的短。 可以看出补全矩阵的奇异值与原矩阵具有一致性，特别是从第 11 个奇异值开始值趋于 0，较好还原了矩阵的低秩特性。因此，在该问题优化求解中条件梯度法无论是时间还是结果精度上都要优于投影梯度法，且输入维数越大效果越明显。



## Task2：交替极小化与 EM 算法

### 问题背景

​	在一些具体任务中(例如图像处理、用户评价)，往往无法收集所有的信息，但这些信息又呈 现高度的相关性，因此想找到低秩矩阵，尽可能去逼近原有的带有缺失值的矩阵。在上一次作业中， 我们采用了核范数凸松弛的方法，在条件投影法下减少了计算复杂度。而采用交替极小化方法便不 需要对问题进行凸松弛了。

### 低秩分解

​	如果X是一个m行n列的矩阵，rank(X)是X的秩，假如rank(X)远小于m和n，则称X为低 秩矩阵。低秩矩阵包含了大量冗余信息。低秩分解目的即为去除冗余，并减少权值参数。

​	在低秩分解中，我们常常定义如下目标函数：
$$
\min _{\operatorname{rank}(M) \leq k} f(M)
$$
​	$f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$。事实上，对于低秩矩阵 M, rankM = k 而言，一定可以做分解如下:
$$
M=X Y^{T}, X \in \mathbb{R}^{m \times k}, Y \in \mathbb{R}^{n \times k}
$$
因此给定初始值，交替极小化的过程便是:
$$
\begin{array}{l}
X_{t}=\arg \min _{X} f\left(X Y_{t-1}^{\top}\right) \\
Y_{t}=\arg \min _{Y} f\left(X_{t} Y^{\top}\right)
\end{array}
$$
​	具体我们需要解决的问题是，对于一个含有缺失值的并且秩较低的矩阵 $A \in \mathbb{R}^{m \times n}$ (缺失值由集合 Ω 描述，表示相应位矩阵元素为 0 的矩阵空间)，我们想要优化以下的目标:
$$
\min _{X \in \mathbb{R}^{m \times k}, Y \in \mathbb{R}^{n \times k}} \frac{1}{2}\left\|P_{\Omega}\left(A-X Y^{\top}\right)\right\|_{F}^{2}
$$
其中，$P_{\Omega}$ 表示一个矩阵到缺失矩阵空间 Ω 的投影。在处理问题之前，需要做出两个假设:
1. 缺失的元素都是独立且随机的;
2. 矩阵 A 的奇异向量的 $l_{\infty}$ 值是很小的。这表明，矩阵的元素都是比较“分散”的。

​	观察我们需要优化的目标函数，发现对于固定的矩阵 X 而言，矩阵 Y 的第 i 行向量 $y_i$ 只与 $P_{\Omega}\left(A-X Y^{\top}\right)$ 的第 i 列有关，F 范数是对每个元素的平方和直接相加的。因此我们可以把这个问题拆分为 n 个子问题:	

​	其中，$a_i$ 是矩阵 A 的第 i 列向量，$s_i$ 是矩阵 A 的第 i 列对应的缺失值位置的向量描述，是一个 0-1 的已知向量。每一个子问题得到的 $y_i$ 都使得最终目标矩阵的列向量的 2-范数最小，把 n 个子问题相加，得到的解也一定是最优的。而这个子问题相当于求解 k 维输入 m 维输出最小二乘问题。 因此此问题的便得到了解决方法。

### 高斯混合模型

当期望极大化算法应用于概率参数化模型 $p(X,Z|\theta)$ 时，即最大化
$$
\max _{\theta} E\left[\log p_{\theta}(X, Z) \mid X\right]
$$
其中， *X* 为样本数据， *Z* 为模型隐含变量，$\theta$ 为概率模型优化变量(例如均值和标准差)。考虑混合高斯分布，即 K 个高斯分布的线性组合，隐含变量 Z 表示样本 所属的第Z个高斯分布， $\theta=\left\{\mu_{i}, \Sigma_{i}, \pi_{i}\right\}_{i=1}^{K}$，其中 $\mu_i$ 为均值，$\sum_i$ 为协方差，$\pi_i$ 为混合高斯分布的权重。因此
$$
\begin{array}{c}
p_{\theta}\left(x_{i}\right)=\sum_{z=1}^{K} \pi_{z} N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right) \\
N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)=\frac{1}{(2 \pi)^{\frac{D}{2}}\left|\Sigma_{z}\right|^{\frac{1}{2}}} \exp \left\{-\frac{\left(x_{i}-\mu_{z}\right)^{T} \Sigma_{z}^{-1}\left(x_{i}-\mu_{z}\right)}{2}\right\}
\end{array}
$$
其中 D 为样本向量维度，由于样本独立同分布，对数形式的最大似然函数为:所有样本概率连乘后取对数
$$
\theta=\underset{s . t \sum \pi_{z}=1}{\arg \max } f(\theta)=\underset{s . t \sum \pi_{z}=1}{\arg \max } \sum_{i=1}^{n} \ln \left[p_{\theta}\left(x_{i}\right)\right]
$$
对该函数求最大值，引入拉格朗日函数
$$
L(\theta, \lambda)=\sum_{i=1}^{n} \ln \left[p_{\theta}\left(x_{i}\right)\right]+\lambda\left[\sum \pi_{z}-1\right]
$$
分别针对 $\theta$ 三个参数求导
$$
\begin{array}{c}
\nabla L_{\pi_{z}}(\theta, \lambda)=\sum_{i=1}^{n} \frac{N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)}{\sum_{z=1}^{K} \pi_{z} N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)}+\lambda=0 \\
\nabla L_{\mu_{z}}(\theta, \lambda)=-\sum_{i=1}^{n} \frac{\pi_{z} N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)}{\sum_{z=1}^{K} \pi_{z} N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)} \Sigma_{z}^{-1}\left(x_{i}-\mu_{z}\right)=0 \\
\nabla L_{\Sigma_{z}}(\theta, \lambda)=\sum_{i=1}^{n} \frac{\pi_{z} N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)}{K}\left(-\frac{1}{2 \Sigma_{z}}+\frac{\left(x_{i}-\mu_{z}\right)^{T}\left(x_{i}-\mu_{z}\right)}{2 \Sigma_{z}^{2}}\right)=0
\end{array}
$$
由贝叶斯定理得：
$$
p_{\theta}\left(z \mid x_{i}\right)=\frac{\pi_{z} N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)}{\sum_{z=1}^{K} \pi_{z} N\left(\mu_{z}, \Sigma_{z}\right)\left(x_{i}\right)}
$$
因此
$$
\begin{array}{l}
\pi_{z}=\frac{\sum_{i=1}^{n} p_{\theta}\left(z \mid x_{i}\right)}{\sum_{z=1}^{K} \sum_{i=1}^{n} p_{\theta}\left(z \mid x_{i}\right)} \\
\mu_{z}=\frac{\sum_{i=1}^{n} p_{\theta}\left(z \mid x_{i}\right) x_{i}}{\sum_{i=1}^{n} p_{\theta}\left(z \mid x_{i}\right)} \\
\Sigma_{z}=\frac{\sum_{i=1}^{n} p_{\theta}\left(z \mid x_{i}\right)\left(x_{i}-\mu_{z}\right)^{T}\left(x_{i}-\mu_{z}\right)}{\sum_{i=1}^{n} p_{\theta}\left(z \mid x_{i}\right)} \\
\end{array}
$$

### 优化方法

#### 交替极小化

​	当秩为k的矩阵M可以被分解为 $M=XY^{T}$ 的形式时 ($X \in \mathbb{R}^{m \times k}, \quad Y \in \mathbb{R}^{n \times k}$),给定初始参数矩阵 $X_0$ 和 $Y_0$ ，交替优化矩阵 *X* 和*Y*
$$
\begin{array}{l}
X_{t}=\arg \min _{X} f\left(X Y_{t-1}^{T}\right) \\
Y_{t}=\arg \min _{Y} f\left(X_{t} Y^{T}\right)
\end{array}
$$
​	以固定X为例：
$$
Y_{t}=\underset{Y \in \mathbb{R}^{n k}}{\arg \min } \frac{1}{2}\left\|P_{\Omega}\left(A-X_{t} Y^{T}\right)\right\|_{F}^{2}
$$
​	由于矩阵的F范数的平方为各元素的平方和，Y中的第i行只出现在 $P_{\Omega}\left(A-X_{t} Y^{T}\right)$​ 中的第 i 列，该问题可以按列进行优化，依此得到 Y 每行的元素，最后组合起来 形成 Y，因此第 i 列的优化问题为
$$
\min _{y_{i} \in \mathbb{R}^{k}}\left\|s_{i} \times\left(a_{i}-X_{t} y_{i}\right)\right\|_{2}^{2}
$$
​	将优化问题转化为最小二乘解的问题。

#### 最大期望法 EM算法

​	EM算法主要分为两步：

1. 求解期望

$$
\text { 根据当前的 } \pi_{z}{ }^{(t)} 、 \mu_{z}{ }^{(t)} \text { 和 } \Sigma_{z}{ }^{(t)} \text { 求期望 } p_{\theta^{(t)}}\left(z \mid x_{i}\right)
$$

2. 极大化

$$
\begin{array}{l}
\pi_{z}^{(t+1)}=\frac{\sum_{i=1}^{n} p_{\theta^{(t)}}\left(z \mid x_{i}\right)}{\sum_{z=1}^{K} \sum_{i=1}^{n} p_{\theta^{(t)}}\left(z \mid x_{i}\right)} \\
\mu_{z}^{(t+1)}=\frac{\sum_{i=1}^{n} p_{\theta^{(t)}}\left(z \mid x_{i}\right) x_{i}}{\sum_{i=1}^{n} p_{\theta^{(t)}}\left(z \mid x_{i}\right)} \\
\Sigma_{z}^{(t+1)}=\frac{\sum_{i=1}^{n} p_{\theta^{(t)}}\left(z \mid x_{i}\right)\left(x_{i}-\mu_{z}^{(t+1)}\right)^{T}\left(x_{i}-\mu_{z}^{(t+1)}\right)}{\sum_{i=1}^{n} p_{\theta^{(t)}}\left(z \mid x_{i}\right)} \\
\end{array}
$$



### 实验和结果

#### 采用交替极小化进行低秩矩阵补全

​	首先生成需要分解的低秩矩阵A，如下：

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218150732800.png" alt="image-20221218150732800" style="zoom:70%;" />

要还原的矩阵如下:

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218150916176.png" alt="image-20221218150916176" style="zoom:40%;" />

| 迭代次数 | 结果                                                         |
| -------- | ------------------------------------------------------------ |
| 1        | <img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151117495.png" alt="image-20221218151117495" style="zoom:40%;" /> |
| 2        | <img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151225888.png" alt="image-20221218151225888" style="zoom:40%;" /> |
| 3        | <img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151248935.png" alt="image-20221218151248935" style="zoom:40%;" /> |
| 4        | <img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151310271.png" alt="image-20221218151310271" style="zoom:40%;" /> |
| 5        | <img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151321626.png" alt="image-20221218151321626" style="zoom:40%;" /> |
| 6        | <img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151333057.png" alt="image-20221218151333057" style="zoom:40%;" /> |
| 7        | <img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151543204.png" alt="image-20221218151543204" style="zoom:40%;" /> |

​	对提供的文档中的矩阵进行分析发现，对于秩为 2 的矩阵，迭代 5 即 可达到较好的还原效果，因此在自己的仿真实验中只进行 6 次迭代，可以发现矩 阵的交替极小化补全有很好的效果，还原的矩阵与原矩阵几乎完全相同。将交替极小化的结果与核范数的结果进行比较，交替极小化结果如下:

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151754621.png" alt="image-20221218151754621" style="zoom:50%;" />

​	核范数结果如下:

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218151852961.png" alt="image-20221218151852961" style="zoom:50%;" />





#### EM算法参数估计

​	生成高斯分布模型的样本直方图如下：

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218153345345.png" alt="image-20221218153345345" style="zoom:50%;" />

采用EM方法得到对数最大似然函数值的变化曲线：

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218153505973.png" alt="image-20221218153505973" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218153547369.png" alt="image-20221218153547369" style="zoom:40%;" />

采用考虑 2 维 4 均值(4-mean)高斯分布的混合模型，可以看出四个灰色区域较好分隔开了生 成的样本，EM 方法较好估计了高斯模型的参数。

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221218153715618.png" alt="image-20221218153715618" style="zoom:50%;" />