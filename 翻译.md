## abstract

原文：The recent detection transformer (DETR) has advanced object detection, but its application on resourceconstrained devices requires massive computation and memory resources. Quantization stands out as a solution by representing the network in low-bit parameters and operations. However, there is a significant performance drop when performing low-bit quantized DETR (Q-DETR) with existing quantization methods. We find that the bottlenecks of Q-DETR come from the query information distortion through our empirical analyses. This paper addresses this problem based on a ==distribution rectification distillation== (DRD). We formulate our DRD as a bi-level optimization problem, which can be derived by generalizing the ==information bottleneck== (IB) principle to the learning of QDETR. At the inner level, we conduct a distribution alignment for the queries to maximize the self-information entropy. At the upper level, we introduce a new foregroundaware query matching scheme to effectively transfer the teacher information to distillation-desired features to minimize the conditional information entropy. Extensive experimental results show that our method performs much better than prior arts. For example, the 4-bit Q-DETR can theoretically accelerate DETR with ResNet-50 backbone by 6.6x and achieve 39.6% mAP, with only 2.4% performance gaps than its real-valued counterpart on the COCO dataset. Our test code and models are attached on https://github.com/Anonymousresults/CVPR1697.
最近的检测变换器（DETR）推进了物体检测，但它在资源受限的设备上的应用需要大量的计算和内存资源。量化作为一种解决方案脱颖而出，它以低位参数和操作表示网络。然而，用现有的量化方法执行低位量化DETR（Q-DETR）时，性能会有明显的下降。我们通过经验分析发现，Q-DETR的瓶颈来自于查询信息失真。本文基于==分布式整流蒸馏==（DRD）来解决这个问题。我们将DRD制定为一个双层次的优化问题，它可以通过将==信息瓶颈==（IB）原则推广到QDETR的学习中。在内层，我们对查询进行分布式排列，以最大化自信息熵。在上层，我们引入了一个新的前景感知查询匹配方案，以有效地将教师信息转移到蒸馏所需的特征中，从而使条件信息熵最小。广泛的实验结果表明，我们的方法比之前的艺术表现得更好。例如，4位的Q-DETR理论上可以将带有ResNet-50主干的DETR加速6.6倍，达到39.6%的mAP，在COCO数据集上比它的实值对应方只有2.4%的性能差距。

## Introduction

原文：Inspired by the success of natural language processing (NLP), object detection with transformers (DETR) has been introduced to train an end-to-end detector via a transformer encoder-decoder . Unlike early works [22,33] that often employ convolutional neural networks (CNNs) and require post-processing procedures, e.g., non-maximum suppression (NMS), and hand-designed sample selection, DETR treats object detection as a direct set prediction problem.

Despite this attractiveness, DETR usually has a tremendous number of parameters and float-pointing operations (FLOPs). For instance, there are 39.8M parameters taking up 159MB memory usage and 86G FLOPs in the DETR model with ResNet-50 backbone [12] (DETR-R50). This leads to an unacceptable memory and computation consumption during inference, and challenges deployments on devices with limited supplies of resources.

Therefore,substantial efforts on network compression have been made towards efficient online inference [7,32, 35,44,45]. Quantization is particularly popular for deploying on AI chips by representing a network in low-bit formats. Yet prior post-training quantization (PTQ) for DETR [26] derives quantized parameters from pre-trained real-valued models, which often restricts the model performance in a sub-optimized state due to the lack of fine-tuning on the training data. In particular, the performance drastically drops when quantized to ultra-low bits (4-bits or less). Alternatively, quantization-aware training (QAT) [25] performs quantization and fine-tuning on the training dataset simultaneously, leading to trivial performance degradation even with significantly lower bits. Though QAT methods have been proven to be very effective in compressing CNNs [8,27] for computer vision tasks, an exploration of low-bit DETR remains untouched.

受自然语言处理（NLP）的成功启发，带变压器的物体检测（DETR）已被引入，通过变压器编码器-解码器来训练一个端到端的检测器[4]。与早期的工作[22,33]不同，DETR通常采用卷积神经网络（CNN），并需要后处理程序，如非最大抑制（NMS），以及手工设计的样本选择，DETR将对象检测作为一个直接的集合预测问题。

尽管有这种吸引力，DETR通常有巨大的参数和浮点运算（FLOPs）的数量。例如，在以ResNet-50为骨干的DETR模型[12]（DETR-R50）中，有39.8M的参数占用了159MB的内存和86G的FLOPs。这导致了推理过程中不可接受的内存和计算量，并对资源供应有限的设备的部署构成了挑战。

因此，为了实现高效的在线推理，人们在网络压缩方面做出了大量努力[7,32,35,44,45]。量化在人工智能芯片上特别流行，它以低位格式表示网络。然而，先前用于DETR的训练后量化（PTQ）[26]从预先训练的实值模型中得出量化参数，由于缺乏对训练数据的微调，这往往限制了模型在次优化状态下的表现。特别是，当量化到超低位（4位或更低）时，性能会急剧下降。另外，量化感知训练（QAT）[25]同时对训练数据集进行量化和微调，即使比特数很低，也会导致微弱的性能下降。尽管QAT方法已被证明在压缩CNN方面非常有效[8,27]，用于计算机视觉任务，但对低位DETR的探索仍未触及。

In this paper, we first build a low-bit DETR baseline, a straightforward solution based on common QAT techniques [2]. Through an empirical study of this baseline, we observe significant performance drops on the VOC [9] dataset. For example, a 4-bit quantized DETR-R50 using LSQ [8] only achieves 76.9% AP50, leaving a 6.4% performance gaps compared with the real-valued DETR- R50. We find that the incompatibility of existing QAT methods mainly stems from the unique attention mechanism in DETR, where the spatial dependencies are first constructed between the object queries and encoded features. Then the co-attended object queries are fed into box coordinates and class labels by a feed-forward network. A simple application of existing QAT methods on DETR leads to query information distortion, and therefore the performance severely degrades. Fig. 1 exhibits an example of information distortion in query features of 4-bit DETR-R50, where we can see significant distribution variation of the query modules in quantized DETR and real-valued version. More statistics on the VOC dataset [9] are provided in the supplementary material, where similar phnenomena can also be observed. The quiery information distortion causes the inaccurate focus of spatial attention, which can be verified by following [29] to visualize the spatial attention weight maps in 4-bit and real-valued DETR-R50 in Fig. 2. We can see that the quantized DETR-R50 bear's inaccurate object localization. Therefore, a more generic method for DETR quantization is necessary.

在本文中，我们首先建立了一个低位DETR基线，这是一个基于普通QAT技术的直接解决方案[2]。通过对这一基线的实证研究，我们观察到在VOC[9]数据集上的显著性能下降。例如，使用LSQ[8]的4位量化DETR-R50只能达到76.9%的AP50，与实值DETR-R50相比有6.4%的性能差距。我们发现，现有QAT方法的不兼容性主要源于DETR中独特的关注机制，即首先在对象查询和编码的特征之间建立空间依赖关系。然后，共同关注的对象查询通过前馈网络被反馈到箱体坐标和类别标签中。在DETR上简单应用现有的QAT方法会导致查询信息失真，因此性能会严重下降。图1展示了一个4位DETR-R50查询特征的信息失真例子，我们可以看到量化的DETR和实值版本的查询模块的明显分布变化。补充材料中提供了更多关于VOC数据集[9]的统计数据，其中也可以看到类似的现象。quiery信息失真导致了空间注意力的不准确集中，这可以通过遵循[29]来验证，在图2中可视化4位和实值DETR-R50的空间注意力权重图。我们可以看到，量化的DETR-R50承担的是不准确的物体定位。因此，一个更通用的DETR量化方法是必要的。

To tackle the issue above, we propose an efficient low-bit quantized DETR (Q-DETR) by rectifying the query information of the quantized DETR as that of the real-valued counterpart. Fig.3 provides an overview of our Q-DETR, which is mainly accomplished by a distribution rectification knowledge distillation method (DRD). We find ineffective knowledge transferring from the real-valued teacher to the quantized student primarily because of the information gap and distortion. Therefore, we formulate our DRD as a bi-level optimization framework established on the information bottleneck principle (IB). Generally, it includes an inner-level optimization to maximize the self-information entropy of student queries and an upper-level optimization to minimize the conditional information entropy between student and teacher queries. At the inner level,we conduct a distribution alignment for the query guided by its Gaussian- alike distribution, as shown in Fig.1,leading to an explicit state in compliance with its maximum information entropy in the forward propagation. At the upper level, we introduce a new foreground-aware query matching that filters out low-qualified student queries for exact one-to-one query matching between student and teacher, providling valuable knowledge gradients to push minimum conditional information entropy in the backward propagation.

为了解决上述问题，我们提出了一种有效的低比特量化DETR (Q-DETR)，方法是将量化DETR的查询信息校正为实值对应的查询信息。图3提供了我们的Q-DETR的概述，它主要是通过分布校正知识蒸馏方法(DRD)完成的。我们发现知识从实值教师向量化学生的无效传递主要是由于信息的缺口和失真。因此，我们将DRD制定为建立在信息瓶颈原理(IB)上的双层优化框架。一般包括最大化学生查询的自我信息熵的内部优化和最小化学生与教师查询之间的条件信息熵的上层优化。在内部层面，我们以查询的高斯相似分布为导向，对查询进行分布对齐，如图1所示，在正向传播中得到符合其最大信息熵的显式状态。在上层，我们引入了一种新的前景感知查询匹配，过滤掉低质量的学生查询，在学生和教师之间进行精确的一对一查询匹配，提供有价值的知识边缘梯度，以在向后传播中推动最小的条件信息熵。

This paper attempts to introduce a generic method for DETR quantization. The significant contributions in this paper are outlined as follows: (1) We develop the first QAT quantization framework for DETR, dubbed Q-DETR. (2) We use a bi-level optimization distillation framework, abbreviated as DRD. (3) We observe a significant performance increase compared to existing quantized baselines.

本文试图介绍一种一般的DETR量化方法。本文的主要贡献如下:(1)我们开发了第一个用于DETR的QAT量化框架，称为Q-DETR。(2)采用双层优化蒸馏框架，简称DRD。(3)与现有的量化基线相比，我们观察到显著的性能提升。



## Related Work

**Quantization**. Quantized neural networks often possess low-bit (1~4-bit) weights and activations to accelerate the model inference and save memory. For example, DoReFa-Net [46] exploits convolution kernels with low bit- width parameters and gradients to accelerate training and inference. TTQ [47] uses two real-valued scaling coefficients to quantize the weights to ternary values. Zhuang et al. [49] present a 2 ~ 4-bit quantization scheme using a two-stage approach to alternately quantize the weights and activations, providing an optimal tradeoff among memory, efficiency,and performance. In1 [14], the quantization in- tervals are parameterized,and optimal values are obtained by directly minimizing the task loss of the network. ZeroQ [3] supports uniform and mixed-precision quantization by optimizing for a distilled dataset which is engineered to match the statistics of the batch normalization across different network layers. Xie et al. [43] introduced transfer learning into network quantization to obtain an accurate low-precision mode1 by utilizing Kullback-Leibler (KL) divergence. Fang et al. [10] enabled accurate approximation for tensor values that have bell-shaped distributions with long tails and found the entire range by minimizing the quantization error. Li et al. [17] proposed an information rectification module and distribution-guided distillation to push the bit-width in a quantized vision transformer. At the same time, we address the quantization in DETR from the IB principle. The architectural design has also drawn increasing attention using extra shortcut [27], and parallel parameter-free shortcuts [25] for example.

量化神经网络通常采用低位(1~4位)权值和激活来加速模型推理和节省内存。例如，DoReFa-Net[46]利用具有低比特宽参数和梯度的卷积内核来加速训练和推理。ttq[47]使用两个实值标度系数将权重量化为三元值。Zhuang等人[49]提出了一种2 ~ 4位量化方案，使用两阶段方法交替量化权重和激活，提供了内存、效率和性能之间的最佳权衡。在1[14]中，对量化区间进行参数化，直接使网络的任务损失最小，从而得到最优值。ZeroQ[3]通过优化蒸馏数据集来支持统一和混合精度量化，该数据集设计用于匹配跨不同网络层的批规范化统计数据。Xie等人[43]将迁移学习引入到网络量化中，利用Kullback-Leibler (KL)发散获得精确的低精度模型1。Fang等人[10]实现了对具有长尾钟形分布的张量值的精确逼近，并通过最小化量化误差找到了整个范围。Li et al.[17]提出了一种信息校正模块和分布引导蒸馏来推动量子化视觉变压器中的位宽。

同时，我们从IB原理出发解决了DETR中的量化问题。建筑设计中使用的额外快捷键[27]和并行无参数快捷键[25]也引起了越来越多的关注。

**Detection Transformer**. Driven by the success of transformers [39], several researchers have also explored transformer frameworks for vision tasks. The first DETR [4] work introduces the Transformer structure based on the attention mechanism for object detection. But the main drawback of DETR lies in the highly inefficient training process. The approachh of another work modifies the multi-head attention mechanism (MHA). Deformable-DETR [48] constructs a sparse and point-to-point MHA mechanism using a static point-wise query sampling method around the reference points. SMCA-DETR [11] introduces a Gaussian- distributed spatial function before formulating a spatially modulated co-attention. DAB-DETR [21] re-defines the query of DETR as dynamic anchor boxes and performs soft ROI pooling layer-by-layer in a cascade manner. DN- DETR [16] introduces query denoising into query generation, reducing the bipartite graph matching difficulty and leading to faster convergence. Another set of arts improves DETR methods using additional learning constraints. For example,UP-DETR [6] proposes a novel self-supervised loss to enhance the convergence speed and the performance of DETR.

However, prior arts mainly focus on the training efficiency of DETR, few of which have discussed the quantization of DETR. To this end, we first build a quantized DETR baseline and then address the query information distortion problem based on the IB principle. Finally,a new KD method based on a **foreground-aware query** matching scheme is achieved to solve Q-DETR effectively.

在变换器[39]的成功推动下，一些研究人员也探索了视觉任务的变换器框架。第一个DETR[4]工作引入了基于注意力机制的物体检测的转化器结构。但DETR的主要缺点在于训练过程的效率很低。另一项工作的方法修改了多头注意机制（MHA）。Deformable-DETR[48]利用参考点周围的静态点对点查询采样方法构建了一个稀疏的点对点MHA机制。SMCA-DETR[11]在制定空间调制的共同注意力之前，引入了一个高斯分布的空间函数。DAB-DETR[21]将DETR的查询重新定义为动态锚箱，并以层叠的方式逐层进行软ROI池化。DN-DETR[16]在查询生成中引入了查询去噪，降低了双子图匹配的难度，导致更快的收敛。另一组艺术使用额外的学习约束来改进DETR方法。例如，UP-DETR[6]提出了一种新的自监督损失，以提高收敛速度和DETR的性能。

然而，**之前的研究主要集中在DETR的训练效率上，很少有人讨论DETR的量化问题**。为此，我们首先建立了一个量化的DETR基线，然后基于IB原则解决查询信息失真问题。最后，一种新的基于前景感知的查询匹配方案的KD方法被实现，以有效解决Q-DETR。	



### Challenge of Quantizing DETR

#### Quantized DETR baseline

We first construct a baseline to study the low-bit DETR since no relevant work has been previously proposed. To this end, we follow LSQ+ [2] to introduce a general framework of asymmetric activation quantization and symmetric weight quantization:

我们首先构建一个基线来研究低比特的DETR，因为之前没有相关的工作提出。为此，我们根据LSQ+[2]引入了不对称激活量化和对称权重量化的一般框架:
$$
\begin{aligned}
\boldsymbol{x}_{q}= & \left\lfloor\operatorname{clip}\left\{\frac{(\boldsymbol{x}-z)}{\alpha_{x}}, Q_{n}^{x}, Q_{p}^{x}\right\}\right\rceil, \mathbf{w}_{q}=\left\lfloor\operatorname{clip}\left\{\frac{\mathbf{w}}{\alpha_{\mathbf{w}}}, Q_{n}^{\mathbf{w}}, Q_{p}^{\mathbf{w}}\right\}\right\rceil, \\
& Q_{a}(x)=\alpha_{x} \circ \boldsymbol{x}_{q}+z, \quad Q_{w}(x)=\alpha_{\mathbf{w}} \circ \mathbf{w}_{q},
\end{aligned}
$$
where clip{y, r1, r2} clips the input y with value bounds r1 and r2; the [y] rounds y to its nearest integer; the o denotes the channel-wise multiplication.And Qn=-2a-1,Q= 2a-1-1,Qw=-2b-1,Qy=26-1-1 are the discrete bounds for a-bit activations and b-bit weights. x generally denotes the activation in this paper, including the input feature map of convolution and fully-connected layers and input of multi-head attention modules. Based on this,we first give the quantized fully-connected layer as:
$$
\mathrm{Q}-\mathrm{FC}(\boldsymbol{x})=Q_{a}(\boldsymbol{x}) \cdot Q_{w}(\mathbf{w})=\alpha_{x} \alpha_{\mathbf{w}} \circ\left(\boldsymbol{x}_{q} \odot \mathbf{w}_{q}+z / \alpha_{x} \circ \mathbf{w}_{q}\right),
$$
where·denotes the matrix multiplication and · denotes the matrix multiplication with efficient bitwise operations. The straight-through estimator (STE) [1] is used to retain the derivation of the gradient in backward propagation.
In DETR [4], the visual features generated by the backbone are augmented with position embedding and fed into the transformer encoder. Given an encoder output E, DETR performs co-attention between object queries O and the visual features E, which are formulated as:
$$
\begin{aligned}
\mathbf{q} & =\mathrm{Q}-\mathrm{FC}(\mathbf{O}), \quad \mathbf{k}, \mathbf{v}=\mathrm{Q}-\mathrm{FC}(\mathbf{E}) \\
\mathbf{A}_{i} & =\operatorname{softmax}\left(Q_{a}(\mathbf{q})_{i} \cdot Q_{a}(\mathbf{k})_{i}^{\top} / \sqrt{d}\right), \\
\mathbf{D}_{i} & =Q_{a}(\mathbf{A})_{i} \cdot Q_{a}(\mathbf{v})_{i},
\end{aligned}
$$
where D is the multi-head co-attention module,i.e.,the co-attended feature for the object query. The d denotes the feature dimension in each head. More FC layers transform the decoder's output features of each object query for the final output. Given box and class predictions, the Hungarian algorithm [4] is applied between predictions and ground- truth box annotations to identify the learning targets of each object query.

其中D为多头共注意模块，即，对象查询的共同参与特性。d表示每个头部的特征维度。更多的FC层将每个对象查询的解码器输出特性转换为最终输出。在给定盒子和类预测的情况下，在预测和地真盒子注释之间应用匈牙利算法[4]来识别每个对象查询的学习目标。



### challenge analysis

Intuitively, the performance of the quantized DETR baseline largely depends on the information representation capability mainly reflected by the information in the multi-head attention module. Unfortunately, such information is severely degraded by the quantized weights and inputs in the forward pass. Also, the rounded and discrete quantization significantly affect the optimization during backpropagation.

直观地看，量化的DETR基线的性能在很大程度上取决于多头注意模块中信息所体现的信息表示能力。不幸的是，这样的信息由于正向传递中的量化权重和输入而严重退化。此外，取整量化和离散量化对反向传播过程中的优化也有显著影响。

We conduct the quantitively ablative experiments by progressively replacing each module of the real-valued DETR baseline with a quantized one and compare the average precision (AP) drop on the VOC dataset [9] as shown in Fig. 4. We find that quantizing the MHA decoder module to low bits,i.e.,(1)+(2)+(3), brings the most significant accuracy drops of accuracy among all parts of the DETR methods, up to 2.1%in the 3-bit DETR-R50. At the same time, other parts of DETR show comparative robustness to the quantization function. Consequently, the critical problem of improving the quantized DETR methods is restoring the information in MHA modules after quantization. Other qualitative results in Fig. 1 and Fig. 2 also indicate that the degraded information representation is the main obstacle to a better quantized DETR.

我们通过逐步将实值DETR基线的每个模块替换为量化的模块来进行定量烧蚀实验，并比较VOC数据集[9]上的平均精度(AP)下降，如图4所示。我们发现，将MHA解码器模块量化为低比特，即(1)+(2)+(3)，在所有DETR方法中带来了最显著的精度下降，在3位DETR- r50中高达2.1%。同时，DETR的其他部分对量化函数显示出相对的鲁棒性。因此，改进量化DETR方法的关键问题是量化后MHA模块中的信息的恢复。图1和图2中的其他定性结果也表明，退化的信息表示是实现更好的量化DETR的主要障碍。

## Proposed Q-DETR

### Information Bottleneck of Q-DETR

To address the information distortion of the quantized DETR, we aim to improve the representation capacity of the quantized networks in a knowledge distillation framework. Generally, we utilize a real-valued DETR as a teacher and a quantized DETR as a student, which are distinguished with superscripts T and S, respectively.

Our Q-DETR pursues the best tradeoff between performance and compression, which is precisely the goal of the information bottleneck (IB) method through quantifying the mutual information that the intermediate layer contains about the input (less is better) and the desired output (more is better) [37,38]. In our case, the intermediate layer comes from the student, while the desired output includes the ground-truth labels as well as the queries of the teacher for distillation. Thus, the objective target of our Q-DETR is:

为了解决量化DETR的信息失真问题，我们旨在提高知识蒸馏框架下量化网络的表示能力。通常，我们使用实值的DETR作为老师，使用量子化的DETR作为学生，它们分别用上标T和S区分。

我们的Q-DETR追求性能和压缩之间的最佳权衡，这正是信息瓶颈(IB)方法的目标，通过量化中间层包含的关于输入(越少越好)和期望输出(越多越好)的相互信息[37,38]。在我们的例子中，中间层来自学生，而所需的输出包括基本事实标签以及教师用于蒸馏的查询。因此，我们Q-DETR的目标为:
$$
\min _{\theta^{\mathcal{S}}} I\left(X ; \mathbf{E}^{\mathcal{S}}\right)-\beta I\left(\mathbf{E}^{\mathcal{S}}, \mathbf{q}^{\mathcal{S}} ; \boldsymbol{y}^{G T}\right)-\gamma I\left(\mathbf{q}^{\mathcal{S}} ; \mathbf{q}^{\mathcal{T}}\right)
$$
where q' and q' represent the queries in the teacher and student DETR methods as predefined in Eq.(3); β and γ are the Lagrange multipliers [37]; θS is the parameters of the student; and I(·) returns the mutual information of two input variables. The first item I(X;ES) minimizes information between input and visual features ES to extract task-oriented hints [42]. The second item I(ES,q5;yGT) maximizes information between extracted visual features and ground-truth labels for better object detection. These two items can be easily accomplished by common network training and detection loss constraints, such as proposal classification and coordinate regression.

其中q'和q'表示在Eq.(3)中预定义的教师和学生DETR方法中的查询;β和γ为拉格朗日乘子[37];θS为学生的参数;I(·)返回两个输入变量的互信息。第一项I(X;ES)最小化输入与视觉特征ES之间的信息，提取面向任务的提示[42]。第二项I(ES,q5;yGT)最大化了提取的视觉特征和ground-truth标签之间的信息，以便更好地检测物体。这两项可以通过常见的网络训练和检测损失约束，如提案分类和坐标回归来轻松完成。

The core issue of this paper is to solve the third item I(q5;qT), which attempts to address the information distortion in student query via introducing teacher query as a priori knowledge. To accomplish our goal, we first expand the third item and reformulate it as:

本文的核心问题是解决第三项I(q5;qT)，即通过引入教师查询作为先验知识来解决学生查询中的信息失真问题。为了实现我们的目标，我们首先展开第三项并重新表述为:
$$
I\left(\mathbf{q}^{\mathcal{S}} ; \mathbf{q}^{\mathcal{T}}\right)=H\left(\mathbf{q}^{\mathcal{S}}\right)-H\left(\mathbf{q}^{\mathcal{S}} \mid \mathbf{q}^{\mathcal{T}}\right)
$$
where H(q5) returns the self information entropy expected to be maximized while H(q5|q') is the conditional entropy expected to be minimized. It is challenging to optimize the above maximum & minimum items simultaneously. Instead,we make a compromise to reformulate Eq.(5) as a bi-level issue [5,20] that alternately optimizes the two items, which is explicitly defined as:

其中H(q5)返回期望最大化的自信息熵，而H(q5|q')是期望最小化的条件熵。同时优化上述最大和最小项具有挑战性。相反，我们做出妥协，将Eq.(5)重新表述为一个双向问题[5,20]，交替优化这两项，明确定义为:
$$
\begin{array}{ll} 
& \min _{\theta} H\left(\mathbf{q}^{\mathcal{S}^{*}} \mid \mathbf{q}^{\mathcal{T}}\right), \\
\text { s.t. } & \mathbf{q}^{\mathcal{S}^{*}}=\underset{\mathbf{q}^{\mathcal{S}}}{\arg \max } H\left(\mathbf{q}^{\mathcal{S}}\right) .
\end{array}
$$
Such an objective involves two sub-problems, including an inner-level optimization to derive the current optimal query and an upper-level optimization to conduct knowledge transfer from the teacher to the student. Below, we show that the two sub-problems can be solved in the forward & backward network propagation's.

这样的目标涉及两个子问题，包括一个内部优化，以获得当前最优查询q?*和上层优化，以进行知识从教师到学生的转移。下面，我们证明了这两个子问题可以在正向和反向网络传播中得到解决。

### Distribution Rectification Distillation

Inner-level optimization内层优化. We first detail the maximization of self-information entropy. According to the definition of self information entropy, H(q5) can be implicitly expanded as:

我们首先详述自信息熵的最大化。根据自信息熵的定义，H(q5)可以隐式展开为：。
$$
H\left(\mathbf{q}^{\mathcal{S}}\right)=-\int_{\mathbf{q}_{i}^{\mathcal{S}} \in \mathbf{q}^{\mathcal{S}}} p\left(\mathbf{q}_{i}^{\mathcal{S}}\right) \log p\left(\mathbf{q}_{i}^{\mathcal{S}}\right)
$$


However, an explicit form of H(q5) can only be parameterized with a regular distribution p Luckily, the statistical results in Fig. 1 shows that the query distribution tends to follow a Gaussian distribution, which is also observed in [31]. This enables us to solve the inner- level optimization in a distribution alignment fashion. To this end, we first calculate the mean μ(q5) and variance σ(q5) of query q' whose distribution is then modeled as qs~N(μ(q5),σ(q5)). Then, the self-information en- tropy of the student query can be proceeded as:

然而，H(q5)的明确形式只能用一个有规律的分布p(q/)作为参数。幸运的是，图1中的统计结果显示，查询分布倾向于遵循高斯分布，这在[31]中也有观察。这使我们能够以分布对齐的方式解决内部层次的优化问题。为此，我们首先计算查询q'的平均值μ(q5)和方差σ(q5)，然后将其分布建模为qs~N(μ(q5),σ(q5))。然后，学生查询的自我信息内容可以进行如下处理。
$$
\begin{align}
H\left(\mathbf{q}^{\mathcal{S}}\right) & = -\mathbb{E}\left[\log \mathcal{N}\left(\mu\left(\mathbf{q}^{\mathcal{S}}\right), \sigma\left(\mathbf{q}^{\mathcal{S}}\right)\right)\right] \\ & = -\mathbb{E}\left[\log \left[\left(2 \pi \sigma\left(\mathbf{q}^{\mathcal{S}}\right)^{2}\right)^{\frac{1}{2}} \exp \left(-\frac{\left(\mathbf{q}_{i}^{\mathcal{S}}-\mu\left(\mathbf{q}^{\mathcal{S}}\right)\right)^{2}}{2 \sigma\left(\mathbf{q}^{\mathcal{S}}\right)^{2}}\right)\right]\right] \\ & = \frac{1}{2} \log 2 \pi \sigma\left(\mathbf{q}^{\mathcal{S}}\right)^{2} .
\end{align}
$$



$$
H\left(\mathbf{q}^{\mathcal{S}^{*}}\right)=   (1 /2) \log 2 \pi e\left[\sigma\left(\mathbf{q}^{\mathcal{S}}\right)^{2}+\epsilon_{\mathbf{q}^{\mathcal{S}}}\right]
$$

$$
\mathbf{q}^{\mathcal{S}^{*}}=\left[\mathbf{q}^{\mathcal{S}}-\right.   \left.\mu\left(\mathbf{q}^{\mathcal{S}}\right)\right] /\left[\sqrt{\sigma\left(\mathbf{q}^{\mathcal{S}}\right)^{2}+\epsilon_{\mathbf{q}}}\right]
$$

$$
\epsilon_{\mathbf{q}^{\mathcal{S}}}=1 e^{-5}
$$

$$
H\left(q^{s^{*}}\right)=(1 / 2) \log 2 \pi e\left[\left(\sigma_{q^{s}}^{2}+\epsilon_{q^{s}}\right) / \gamma_{q^{s}}^{2}\right]
$$

The above objective reaches its maximum of H is a small constant added to prevent a zero denominator. In practice, the mean and variance might be inaccurate due to query data bias. To solve this we use the concepts in batch normalization (BN) [13,36] where a learnable shifting parameter βgs is added to move the mean value. A learnable scaling parameter Yqs is multiplied to move the query to the adaptive position. In this situation, we rectify the information entropy of the query in the student as follow：

上述目标达到最大值H(q")=(1/2)log 2πe[σ(qs) μ(qs)]/l(q)2+s]，其中qs=1e-5是为防止分母为零而加入的一个小常数。在实践中，由于查询数据的偏差，平均值和方差可能是不准确的。为了解决这个问题，我们使用了批量归一化（BN）中的概念[13,36]，其中加入了一个可学习的移动参数βgs来移动平均值。一个可学习的缩放参数Yqs被乘以移动查询到适应的位置。在这种情况下，我们对学生中的查询的信息熵进行矫正，具体如下
$$
\mathbf{q}^{\mathcal{S}^{*}}=\frac{\mathbf{q}^{\mathcal{S}}-\mu\left(\mathbf{q}^{\mathcal{S}}\right)}{\sqrt{\sigma\left(\mathbf{q}^{\mathcal{S}}\right)^{2}+\epsilon_{\mathbf{q}^{\mathcal{S}}}}} \gamma_{\mathbf{q}^{\mathcal{S}}}+\beta_{\mathbf{q}^{\mathcal{S}}}
$$
in which case the maximum self-information entropy of student query becomes H(qs). Therefore, in the forward propagation, we can obtain the current optimal query qs* via Eq.(9), after which, the upper-level optimization is further executed as detailed in the following contents.

在这种情况下，学生查询的最大自信息熵变为H(qs)=(1/2)log 2πe[(2s+) 因此，在前向传播中，我们可以通过公式（9）得到当前的最优查询qs*，之后，进一步执行上层优化，具体内容见下文。

**Upper-level optimization高层优化**. We continue minimizing the conditional information entropy between the student and the teacher. Following DETR [4], we denote the ground-truth labels by y as a set of ground-truth objects where Ngt is the number of foregrounds,cGT and bGT respectively represent the class and coordinate (bounding box) for the i-th object. In DETR, each query is associated with an object. Therefore, we can obtain N objects for teacher and student as well, denoted as y5

The minimization of the conditional information entropy requires the student and teacher objects to be in a one-to-one matching. However, it is problematic for DETR due primarily to the sparsity of prediction results and the instability of the query's predictions [16]. We propose a foreground-aware query matching to rectify "well-matched” queries to solve this. Concretely, we match the ground-truth bounding boxes with this student to find the maximum coincidence as:
$$
G_{i}=\max _{1 \leq j \leq N} \operatorname{GIoU}\left(b_{i}^{G T}, b_{j}^{\mathcal{S}}\right)
$$
 where GIou is the generalized intersection over union function. Each Gi reflects the “closeness” of student proposals to the i-th ground-truth object. Then, we retain highly qualified student proposals around at least one ground truth to benefit object recognition as:
$$
b_{j}^{\mathcal{S}}=\left\{\begin{array}{ll}
b_{j}^{\mathcal{S}}, & \operatorname{GIoU}\left(b_{i}^{G T}, b_{j}^{\mathcal{S}}\right)>\tau G_{i}, \forall i \\
\varnothing, & \text { otherwise }
\end{array}\right.
$$
where T is a threshold controlling the proportion of distilled queries. After removing object-empty (Ø) queries in q, we form a distillation-desired query set of students denoted as q asociated with its object y. Correspondingly, we can obtain a teacher query set y. For the j-th student query, its corresponding teacher query is matched as:
$$
\tilde{c}_{j}^{\mathcal{T}}, \tilde{b}_{j}^{\mathcal{T}}=\underset{\tilde{c}_{k}^{\mathcal{T}} \tilde{b}_{k}^{\mathcal{T}}}{\arg \max } \sum_{k=1}^{N} \mu_{1} \operatorname{GIoU}\left(\tilde{b}_{j}^{\mathcal{S}}, b_{k}^{\mathcal{T}}\right)-\mu_{2}\left\|\tilde{b}_{j}^{\mathcal{S}}-b_{k}^{\mathcal{T}}\right\|_{1}
$$
Finally, the upper-level optimization after rectification in Eq. (6) becomes:
$$
\min _{\theta} H\left(\tilde{\mathbf{q}}^{\mathcal{S}^{*}} \mid {\tilde{\mathbf{q}}}^{\mathcal{T}}\right)
$$
Optimizing Eq. (13) is challenging. Alternatively, we minimize the norm distance between q and qT,optima of which, i.e., qs* = qT, is exactly the same with that in Eq.(13). Thus, the final loss for our distribution rectification distillation loss becomes:
$$
\mathcal{L}_{D R D}=\mathbb{E}\left[\left\|\tilde{\mathbf{q}}^{\mathcal{S}^{*}}-\tilde{\mathbf{q}}^{\mathcal{T}}\right\|_{2}\right]
$$
 In backward propagation, the gradient updating drives the student queries toward their teacher hints. Therefore we accomplish our distillation. The overall training losses for our Q-DETR model are:
$$
\mathcal{L}=\mathcal{L}_{G T}\left(\boldsymbol{y}^{G T}, \boldsymbol{y}^{\mathcal{S}}\right)+\lambda \mathcal{L}_{D R D}\left(\tilde{\mathbf{q}}^{\mathcal{S}^{*}}, \tilde{\mathbf{q}}^{\mathcal{T}}\right)
$$
where LGr is the common detection loss for missions such as proposal classification and coordinate regression [4],and t is a tradeoff hyper-parameter.




## concolusion

This paper introduces a novel method for training quantized DETR methods (Q-DETR) with knowledge distillation to rectify the query distribution. Q-DETR generalizes the information bottleneck (IB) principle and leads a bi-level distribution rectification distillation. We effectively employ a distribution alignment module to solve the inner- level and a foreground-aware query matching scheme to solve the upper level. As a result, our Q-DETR significantly boosts the performance of low-bit DETR. Extensive experiments show that Q-DETR surpasses state-of-the-art methods in DETR quantization.

提出了一种利用知识蒸馏训练量化DETR方法(Q-DETR)的新方法，以校正查询分布。Q-DETR推广了信息瓶颈(IB)原理，提出了一种双层分布校正蒸馏。我们有效地利用分布对齐模块解决内层问题，利用前景感知查询匹配方案解决上层问题。因此，我们的Q-DETR显著提高了低比特DETR的性能。大量实验表明，Q-DETR在DETR量化方面超越了最先进的方法。



## 实验

 Implementation Details. 

Our Q-DETR is trained with the DETR  and SMCA-DETR framework. We select the ResNet-50 and modify it with Pre-Activation structures and RPReLU function following. PyTorch is used for implementing Q-DETR. We run the experiments on 8 NVIDIA Tesla A100 GPUs with 80 GB memory. We use ImageNet ILSVRC12 to pretrain the backbone of a quantized student. The training protocol is the same as the employed frameworks [4,11]. Specifically,  we use a batch size of 16. AdamW is used to opti- mize the Q-DETR,with the initial learning rate of 1e-4 and the weight decay of 1e-4. We train for 300/500 epochs for the Q-DETR on VOC/COCO dataset, and the learning rate is multiplied by 0.1 at the 200/400-th epoch, respectively. Folowing the SMCA-DETR.we train the Q-SMCA-DETR for 50 epochs, and the learning rate is mmultiplied by 0.1 at the 40-th epoch on both the VOC and COCO datasets. We select real-valued DETR-R101 (84.5% AP50 on VOC and 43.5% mAP on COCO) and SMCA-DETR-R101 with VGG16 backbone (85.3% AP5o on VOC and 44.4% mAP on COCO) as teacher network.

 Hyper-parameter selection. 

As mentioned above,we select hyper-parameters t and r in this part using the 4-bit Q-DETR model. We show the model performance (AP50) with different setups of hyper-parameters {T,\}in Fig.5 (a),where we conduct ablative experiments on the baseline +DA (AP50=78.8%). As can be seen, the performances in- crease first and then decrease with the increase of T from left to right. Since T controls the proportion of selected distillation-desired queries, we show that the full-imitation (T =0) performs worse than the vanilla baseline with no distillation (т = 1), showing quiery selection is necessary. The figure also shows that the performances increase first and then decrease with the increase of r from left to right. The Q-DETR obtains better performances with r set as 0.5 and 0.6. With the varying value of λ,we find{λ,T}={2.5, 0.6} boost the performance of Q-DETR most,achieving 82.7% mAP on VOC test2007. Based on the ablative study above, we set hyper-parameters T and λ as 0.6 and 2.5 for the experiments in this paper.
