

## Intro



随着数字媒体技术的飞速发展，视频内容已成为互联网上最重要的数据形式之一。高效的视频编码对于带宽利用和数据存储具有重大意义，尤其在在线视频流、视频会议和数字电视等应用中尤为突出。

在过去几十年中，已经发展出多代视频编码标准，比如H.264/AVC、H.265/HEVC、H.266/VVC，这些传统编码标准都是基于预测编码范式，依靠手动设计的模块，例如基于块的运动估计和离散余弦变换（DCT）来减少视频序列中的冗余。尽管各模块都经过精心设计，但整个压缩系统并未进行端到端优化。

近年来，neural video codec已经探索了一个全新的发展方向。现有的深度视频压缩工作可分为无时延约束和时延约束两类。对于无时延约束编码，参考帧可以来自于当前帧之后的位置。由于存在多重参考帧，这种编码方式存在较大的延迟，且显著增加内存开销和计算消耗。对于低时延约束下的编码，参考帧来自于之前的重构帧。

在设计视频编码模型以应对低延迟应用场景时，我们面临一个关键挑战：这些模型常常需要在资源受限的移动设备上部署。为了适应这些设备的硬件限制，模型的压缩和加速变得至关重要，以确保能够高效地执行推理任务。然而，现有的视频编码模型往往对计算资源的需求极高，这在移动和低功耗设备上尤其成问题。With the rapid evolution of digital media technology, video has become a predominant internet data format. Efficient video encoding is vital for optimizing bandwidth and storage, particularly in online streaming, conferencing, and digital TV.

In the past decades, multiple generations of video coding standards have been developed, such as H.264/AVC, H.265/HEVC, and H.266/VVC. These traditional coding standards are based on the predictive coding paradigm, and rely on handcrafted modules, such as block-based motion estimation and discrete cosine transform (DCT), to reduce the redundancies in video sequences. But the overall compression framework is not end-to-end optimized.

In recent years, neural video codec (NVC) has explored a completely new development direction. Existing deep video compression research can be categorized into two types according to the targeted scenarios: non-delay-constrained and delay-constrained. For the first type, the reference frame can come from positions after the current frame. This method involves significant latency, increased memory overhead, and higher computational demands due to multiple reference frames. For the low-latency type, reference frame comes from the previous reconstructed frame.

When designing video coding models for low-latency scenarios, a key challenge arises: these models are often required to be deployed on resource-limited mobile devices, so model compression and acceleration become critical. However, existing video coding models have large memory and computational consumption.

因此，人们在网络压缩方面做了大量工作，来实现高效的推理。量化是降低神经网络计算复杂度的有效方法之一。我们发现，针对neura video codec的模型压缩工作很少，之前有人尝试对learned image compression进行量化。比如[8,9]中只考虑权重的量化[8,9]，并没有得到完全量化的LIC。其他一些方法同时量化权重和激活值[10,11,12]，但它们通常依赖于推理时的动态量化，这需要很高的开销。同时由于视频帧之间存在时空上的关联，直接将现有的量化方法简单应用会导致重建

To facilitate deployment，a lot of works have been proposed. Quantization is one of the promising approaches for reducing the computational complexity of neural networks. 



本文的工作针对实时传输等低延迟场景设计。



近年来，一系列研究试图在训练好的深度网络基础上构建全新的视频压缩方案。这些研究根据其目标场景可分为两类。对于第一类，Wu等人提出了一种基于循环神经网络(RNN)的基于插值的视频压缩方法[32]，其中运动信息由传统的基于块的运动估计获得，并通过图像压缩方法进行压缩。随后，Djelouah等人也提出了一种基于插值的视频压缩方法，其中插值模型结合了运动信息压缩和图像合成，对图像和残差[8]使用相同的自编码器。基于插值的压缩使用前一帧和后一帧作为参考来压缩当前帧，这在播放等随机访问场景中是有效的。然而，它不太适用于实时传输等低延迟场景。



近年来，学习视频压缩探索了一个新的方向。Lu等人将传统运动补偿预测和残差编码框架中的每个部分都替换为卷积神经网络[4]。它们联合优化了整个网络的率失真代价。Lin等人探索了多个参考帧，关联了多个运动矢量，生成了更准确的当前帧预测，降低了运动矢量的编码成本[9]。Yang等人设计了一种循环学习的视频压缩方案[16]。所提出的循环自编码器和循环概率模型利用大范围帧中的时间信息来生成潜在表示并重构压缩输出。Hu等人提取输入帧的特征并应用可变形卷积进行运动预测[10]。学习到的偏移量和残差在特征域中进行压缩。然后，将存储在解码缓冲区中的多个参考特征通过非局部注意力模块进行融合以获得重构帧;除运动补偿预测和残差编码框架外，Habibian等人将视频视为多帧的体积，提出了一种3D自编码器直接压缩多帧[23]。Liu等人使用图像编解码器对每一帧进行压缩，并提出了一个熵模型来探索潜在表示[26]的时间相关性。Li等人将残差编码转换为条件编码[27]。他们从之前解码的帧中学习时间上下文，并让编码器探索时间相关性以自动删除冗余。











## Related Work



**Network Quantization**

网络量化通过将32位浮点特征映射和权重转换为更低的位值来优化神经网络，减少计算复杂度和内存使用。

Network quantization optimizes neural networks by converting 32-bit floating-point feature maps and weights into lower bit values, reducing computational complexity and memory usage.

INQ introduce three interdependent operations, namely weight partition, group-wise quantization and re-training to optimize the training process.DoReFa-Net exploits convolution kernels with low bit-width parameters and gradients to accelerate training and inference.LQ-Net approximate the gradient propagation in quantization-aware training by straight-through estimator. Pact proposes a parameterized clipping activation function. LSQ introduces learnable quantization parameters and gradient-based optimization method, achieving commendable results in low-bit quantization. AdaQuant proposed a post-training quantization method based on layerwise calibration and integer programming, which achieved ultra-low accuracy loss on 4-bit quantization.



然而目前大多数网络量化的工作都针对于检测或分类任务，也有一些针对learned image compression的轻量化工作。比如xxxx。

但与这些不同的是，视频相比于图像具有一些其他特性，



**一段：** 强调ROI，低延迟场景，人眼关注更多在于前景

**一段：**选择静态量化，避免动态量化策略，降低开销。同时动态的精度选择不适合视频编码框架，因为需要将动态的量化策略从一端传输到另一端，增加额外的比特流。因此选择静态的量化策略





网络量化是一种通过量化权重和/或激活值来降低神经网络复杂性的方法。权重和(或)激活值的位宽可以是二进制[18]、三元[19,20]或任意的[21]。为了实现整数算术运算，最近的方法选择统一量化权重和激活[22,23]。最近的两个值得注意的研究包括超分辨率模型[24]的参数化最大尺度(PAMS)，其中量化范围是通过学习自适应确定的，以及分类模型[25]的学习步长量化(LSQ)，其中量化器步长是优化的。这两种方法的主要区别是PAMS不对权重的量化尺度进行参数化，而LSQ可以。

与JPEG等传统方法相比，LIC具有更高的复杂度。然而，提高LIC效率的研究还不多。在[6]中，对译码部分进行剪枝，以提高译码效率。目前针对LIC的网络量化研究较少。在[8,9]中，提出了权重量化的方法，其中激活仍然没有量化，因此效率的提高只是部分的。[10]的工作提出了一种两阶段的量化方法，其中量化导致了性能的显著下降。[11,12]中提出的方法同时考虑了权重和激活的量化。然而，它们采用了动态量化，其中裁剪范围不是固定的，而是根据输入的统计信息实时计算的，这需要很大的计算开销[13]。为了最小化这种额外的开销，在下一节中，我们提出了一种使用静态量化的LIC量化方法。



混合精度量化

传统的量化方法往往将所有层压缩到相同的精度，可能导致性能显著下降。解决这个问题的一种可能性是使用混合精度量化。Wang et al.[41]提出了一种基于强化学习的方法来实现混合精度量化。Dong等人[10,11]提出利用二阶(Hessian矩阵)信息来确定量化位宽，分别考虑每一层的量化效果。Guo et al.[17]提出了一种单路径单次搜索混合精度架构的方法。可微架构搜索(Differentiable architecture search, DARTS)[28]将离散搜索空间放宽为连续搜索空间，实现了梯度下降优化。Wu等人。[43]和Cai等人。[4]使用dart来找到每一层cnn的比特分配。这些方法虽然用不同的位宽量化每一层，但没有考虑到不同样本的计算量是不同的。



模型量化将浮点参数转换为低精度值，是一种流行的解决方案，以硬件友好的方式压缩模型[20,12,26,48]。大多数之前的工作都是为了量化cnn。DoReFa[49]和LQ-Net[46]通过直通式估计器(STE)[2]近似量化感知训练中的梯度传播。PACT[7]和LSQ[11,3]将激活裁剪值/步长作为可训练参数，并在低比特量化上取得了有希望的结果。此外，一些值得注意的工作采用了更先进的量化策略，包括非均匀量化[23]、信道量化[22]和混合精度量化[40,9]等。















近年来也见证了NVC的繁荣。开创性的DVC[34]遵循传统编解码器。利用光流网络生成预测帧，并对其与当前帧的残差进行编码。许多后续工作也采用这种基于残差编码的框架，并完善其中的模块。例如，[31,43,47]提出了运动预测来进一步减少冗余。设计尺度空间下的光流估计算法[1]来处理复杂运动;Yang等[61]利用循环自编码器来提高编码效率。

残差编码显式地在像素域生成预测帧作为上下文，仅使用减法去除冗余。通过比较，条件编码具有更强的可扩展性。条件的定义、学习和使用方式可以灵活设计。在[33,38]中，设计了时间条件熵模型。[27]使用条件编码对前景内容进行编码。Li等人提出DCVC[28]来学习特征域上下文以增加上下文容量。然后，DCVC-TCM[50]采用特征传播算法来提高性能。最近，DCVC-HEM[29]设计了一种同时利用空间和时间上下文的混合熵模型。





近年来也见证了NVC的繁荣。开创性的DVC[34]遵循传统编解码器。利用光流网络生成预测帧，并对其与当前帧的残差进行编码。许多后续工作也采用这种基于残差编码的框架，并完善其中的模块。例如，[31,43,47]提出了运动预测来进一步减少冗余。设计尺度空间下的光流估计算法[1]来处理复杂运动;Yang等[61]利用循环自编码器来提高编码效率。

残差编码显式地在像素域生成预测帧作为上下文，仅使用减法去除冗余。通过比较，条件编码具有更强的可扩展性。条件的定义、学习和使用方式可以灵活设计。在[33,38]中，设计了时间条件熵模型。[27]使用条件编码对前景内容进行编码。Li等人提出DCVC[28]来学习特征域上下文以增加上下文容量。然后，DCVC-TCM[50]采用特征传播算法来提高性能。最近，DCVC-HEM[29]设计了一种同时利用空间和时间上下文的混合熵模型。





近年来，学习视频压缩探索了一个新的方向。Lu等人将传统运动补偿预测和残差编码框架中的每个部分都替换为卷积神经网络[4]。它们联合优化了整个网络的率失真代价。Lin等人探索了多个参考帧，关联了多个运动矢量，生成了更准确的当前帧预测，降低了运动矢量的编码成本[9]。Yang等人设计了一种循环学习的视频压缩方案[16]。所提出的循环自编码器和循环概率模型利用大范围帧中的时间信息来生成潜在表示并重构压缩输出。Hu等人提取输入帧的特征并应用可变形卷积进行运动预测[10]。学习到的偏移量和残差在特征域中进行压缩。然后，将存储在解码缓冲区中的多个参考特征通过非局部注意力模块进行融合以获得重构帧;除运动补偿预测和残差编码框架外，Habibian等人将视频视为多帧的体积，提出了一种3D自编码器直接压缩多帧[23]。Liu等人使用图像编解码器对每一帧进行压缩，并提出了一个熵模型来探索潜在表示[26]的时间相关性。Li等人将残差编码转换为条件编码[27]。他们从之前解码的帧中学习时间上下文，并让编码器探索时间相关性以自动删除冗余。



In recent years, there have been significant advances in the research and development video coding technology and system. DVC is the first end-to-end optimized learned video compression method, Lu et al. replaced each module in traditional video codec framework with convolutional neural networks







在四个标准的视频识别数据集(ActivityNet-v1.3[3]、FCVID[25]、Mini-Sports1M[28]和Mini-Kinetics[5])上进行了广泛的实验，证明了所提方法的优越性。实验结果表明，VideoIQ可以显著节省计算和内存(例如，平均减少26.0%的GFLOPS和55.8%的内存)，同时实现更好的识别性能，超过最具竞争力的SOTA基线[34]。使用该方法学习到的决策策略可以迁移到不同数据集上未见过的类别和视频。定性结果表明，所学习的策略与视频帧中不同的视觉模式相关，即，所提出方法仅对相关视频帧使用32位全精度，并以低精度处理无信息帧，或为了计算效率跳过它们。





深度卷积神经网络(cnn)在广泛的智能应用中取得了显著的效果，包括图像处理[14,27]、视频理解[5]、自然语言处理[15]和语音识别[49]。然而，这些模型要达到令人满意的性能，对存储和计算资源的要求过高，阻碍了它们在移动和嵌入式设备上的部署。

随着大规模视频数据集的可用性[5,36]，基于2D/3D卷积神经网络(cnn)的深度学习模型[6,52,48,28,17]已经主导了视频识别领域。然而，尽管在标准基准上有令人印象深刻的性能，但由于深度CNN模型的沉重计算负担，对于许多资源受限的应用程序来说，效率仍然是一个巨大的挑战。

令人惊讶的是，这是高效视频推理的另一个自由度，在之前的工作中几乎被忽视。为了说明这一点，让我们考虑图1中的视频，由5个均匀采样的帧表示。快速浏览视频可以清楚地看到，只有第三帧可以使用32位精度处理，因为这是识别“跳远”动作的信息最多的帧，而其他帧可以在非常低的精度下处理甚至跳过(即精度设置为零)，而不牺牲精度(下)，与处理所有相同的32位精度的帧相比，导致大量的计算节省，如主流视频识别方法中通常做的(上)。

受这一观察的启发，本文提出视频实例感知量化(VideoIQ)，首次提出一种新的依赖输入的动态网络量化策略，以有效地进行视频识别。虽然动态网络量化乍一看琐碎而方便，但我们需要解决两个挑战:(1)如何有效地确定为每个目标实例使用什么样的量化精度;(2)给定特定实例的精度，如何灵活地量化权重和激活

为解决上述挑战，本文提出一种简单的端到端可微方法来学习一个决策策略，以输入为条件选择最优精度，同时在识别复杂动作时兼顾精度和效率。通过从由轻量级策略网络输出参数化的离散分布中采样策略来实现这一点，该分布实时决定每帧应该使用什么精度。由于这些决策函数是离散的和不可微的，我们通过Gumbel Softmax采样[24]使用标准的反向传播来训练策略网络，而不求助于复杂的强化学习，如[60,9,63]。此外，没有存储单独的特定精度模型，而是使用联合训练训练单个深度神经网络进行行为识别，使其能够通过简单截断最低有效位直接调整数值精度，而不会降低性能。所提出的方法不仅提供了较高的计算效率，还大大节省了内存——这是许多现实世界应用的实际需求，但在很大程度上被之前的工作[34,59,35,60]忽略了。





## Proposed Method

### Overview

In a full-precision model, all weights and activations are represented using full-precision floating-point numbers (32 bits). Quantization refers to the process of converting these full-precision weights and/or activations into fixed-point numbers with a lower bit width, such as 2, 4, or 8 bits. Mixed-precision quantization involves representing different groups of neurons with varying quantization ranges (bit depths).

We build our model based on DCVC model and follow LSQ+ to build a quantization framework. To ease the hardware implementation, we use ReLU as the nonlinearity after each layer of the network instead of a divisive normalization (GDN) block. Since GDN requires a extensive floating-point computations and has higher complexity. We use the straight-through estimator (STE) to preserve gradient information during the backward propagation phase. For all the convolutional weights and activations, we use channel-wise quantization parameters, since the range of weights and activations varies greatly between channels.

The process of for weights quantization is denoted as:
$$
\mathbf{w}_q=\left\lfloor\operatorname{clip}\left(\frac{\mathbf{w}}{s_{\mathbf{w}}},L_n^{\mathbf{w}},U_n^{\mathbf{w}}\right)\right\rceil,
$$
where $\mathbf{w}$ is full-precision weight, $\mathbf{w}_q$ is the quantized integer value, clip() clips the input with $L_{\mathbf{w}}$, $U_{\mathbf{w}}$,  $L_n^{\mathbf{w}}$, $U_n^{\mathbf{w}}$ are the integer lower and upper integer bounds of clip function respectively. n is the quantization bit-width. The quantization scale parameter, denoted as $s_w$, determines the step size for the dequantized weight.
$$
\mathbf{x}_q=\left\lfloor\operatorname{clip}\left(\frac{\mathbf{x}}{s_{\mathbf{x}}},L_n^{\mathbf{x}},U_n^{\mathbf{x}}\right)\right\rceil
$$
We initializing $s_w$ and $s_q$ for each channel as $max(|w|)/U_n^{\mathbf{w}}$ and $max(|x|)/U_n^{\mathbf{x}}$, we found this to be more effective at reducing early training errors compared to the initialization methods in LSQ.



### Approach Overview









### Approach Overview

Figure 展示了我们方法的概述。首先，我们光流生成分割掩码，来分离当前帧的前景和背景，并将光流也进行分离。之后

图2显示了我们的方法的概述。一般来说，我们学习一个特定于实例的策略a i，它实时决定使用(甚至跳过)哪个精度来处理当前帧xi，以及一个视频分类器f，它可以通过简单地截断最低有效位来灵活地量化到当前帧的所需精度，而不需要任何额外的计算或内存成本。为此，VideoIQ由一个轻量级的策略网络g和一个视频识别网络f组成。策略网络g包含一个特征提取器和一个LSTM模块，用于学习每个输入帧使用哪种精度的离散决策(见第3.3节)。此外，由于静态场景或帧质量很低导致的大量冗余，对视频中的每一帧进行处理往往是不必要和低效的。因此，除了在统一框架中动态选择精度外，还跳过帧(即精度设置为零)，以提高视频识别的效率。为了进一步实现灵活和可扩展的量化，将视频分类器学习为任意精度的网络，并设计了一个简单而有效的优化方案，以确保单组网络权重以多种精度执行，而没有额外的存储和计算成本(见第3.4节)。







