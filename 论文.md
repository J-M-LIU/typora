# End-to-end learned video compression，channel-wise

## 瞎想

**什么是硬件感知的量化呢 Hardware-Aware quantization**

1. **idea：由于video codec模型网络比较大，所以不同的layer或channel对于量化的敏感度也不同，计算量化误差对于task loss的影响，来获得不同layer的bit-width进行量化**
2. **idea3：引入量化误差作为辅助损失帮助训练，这个不确定，再看看**
3. **找一个最优化方法，来对layer按照distribution的差异进行分组，每个组对应一个scale和zero_point**



基于海森矩阵的量化敏感度分析



**参考GN等，对activation进行分组的量化？每个组有单独的scale和zeor_point；group-wise的分组方法采用自适应的学习方法？可以参考混合精度的搜索方式；同时为了避免group数过大，导致和channel数一致，进而产生高的计算开销，可以添加一个约束项，具体参考GroupQ这里的目标函数。（约束模型大小/约束计算复杂度）**



正则项：计算flops和量化误差保持一个平衡关系 lmbda

**辅助损失？**

**辅助监督的混合精度量化？**



平衡计算负载和量化性能

prune和clip



## TASK

### 关于模型中不同模块对于量化的敏感度差异

- [x] 只量化mv encoder/decoder
- [x] 只量化 mv estimator
- [x] 只量化 contextual encoder/decoder

### 每个阶段收敛后进行quant

- [x] quant时其他模块保持全精度

  > 其他模块保持全精度的同时，并未处于训练状态，参数不会更新

- [x] quant时前面已训练的模块保持量化精度

### 量化敏感度

- [ ] 确定模型关于量化敏感度的计算方式（根据每个channel的提取细节、结构和纹理不同，展示量化敏感度和每个channel提取的context细节的关联。）
- [ ] 统计不同layer/channel对于量化的敏感度
- [ ] 根据量化敏感度确定per-layer/channel/patch的bit——混合精度

### 如何确定flops等性能？导出ONXX模型？



## IDEA

### 辅助模块-类蒸馏思想——直接改成时间/空间context信息蒸馏好了

- 由于替换relu后丢失了非线性性，在adapter后跟一个GDN，弥补非线性；
- 防止误差累计：在每一个模块后，引入全精度的特征进行矫正？
- 构建一个提取复杂纹理/结构/等信息的模块，对低精度模块进行辅助指导？
- 误差累计要如何解决？多阶段训练过程

### 混合bit

- 构建一个提取复杂纹理/结构/等信息的模块，然后混合精度？

### channel split + context 空间+时间+层次蒸馏

channel split 多用于消除异常值+提取高频纹理信息

### 结合Memory augmented的多帧信息融合

### 编码+prompt 超低bit rate下的视频编码









mv/context通过全精度的编码器+解码器后，（其中需包含GDN补足非线性和通道交互），尤其是再额外添加一个复杂结构/纹理等信息的提取模块，与mv/context concat等操作后，解码得到的全精度mv/context与低精度的这两个信息比较分布相似度，loss按照分布相似度或者其他计算方式。相当于以低精度网络的

**引入自蒸馏的思想来解决低精度和全精度之间分布差异的问题**

**纹理**

**结构**

**动作信息**

而且全精度辅助模块，应该在编码mv/context信息后就做一次分布指导，因为需要分别指导低精度encoder和decoder学习信息的能力。

**CVPR2020：Structure-Preserving Super Resolution with Gradient Guidance**基于梯度信息指导的图像超分











行文思路：

1. 分阶段量化后，探究各阶段量化后对网络性能的影响程度。单量化ME/量化ME+MV/量化ME、MV、Context编解码器，发现单独量化ME对于性能的影响很小， 发现MV encoder/decoder 和 context encoder/decoder对于性能的影响起到关键作用。因此针对这两个编解码器的调优进行了优化。——然后给图：context和encoder分布差异大；GDN的影响分析。



2. 纹理/结构提取块
3. 时间信息提取块
4. 运动信息辅助提取块









对于视频编码，设计辅助任务时需要考虑到视频的特性，以及与视频编码紧密相关的子任务。以下是一些建议的辅助任务：

1. **帧间运动估计**： 视频是连续的图像帧，其中相邻帧之间的差异往往与运动有关。一个可能的辅助任务是在编码器的中间层估计相邻帧之间的运动。这可以帮助网络更好地理解视频中的动态信息，并可能有助于更有效的编码。
2. **局部特征重建**： 可以设计一个辅助任务，使网络在中间层尝试重建视频的某个局部特征，如某个区域的纹理或颜色。这可以帮助网络关注视频的某些重要特征，并可能改进编码质量。
3. **场景切换检测**： 视频中可能存在场景切换，这是编码的一个挑战。作为辅助任务，网络可以尝试检测这些场景切换，从而在主任务中更好地处理它们。
4. **视频内容分类**： 尽管这是一个高级任务，但为网络提供关于视频内容的高级语义信息可能是有益的。例如，如果网络知道视频是关于足球比赛的，它可能会更关注球员和球的动态，而忽略观众。
5. **残差预测**： 在视频编码中，经常使用预测和残差编码的方法。辅助任务可以是预测某个帧与其参考帧之间的残差。这有助于网络更好地理解视频的变化和模式。
6. **视频质量评估**： 为网络提供一个机制，使其可以评估其编码质量（例如，通过计算编码帧与原始帧之间的差异）。这可以作为一个辅助任务，帮助网络在训练过程中优化其编码策略。

设计辅助任务时，重要的是要确保这些任务与主任务（视频编码）紧密相关，并且能为主任务提供有价值的信息。此外，还需要通过实验验证这些辅助任务是否确实能提高编码性能。



**基于视频理解的量化**

**基于视频理解的混合精度量化**

混合精度是对patch还是针对layer/channel呢

视频前后景分离的量化

特征金字塔



动态量化策略：每一帧以前一帧或参考帧的量化策略作为base；

- 每个patch识别不同精度：这个对应到feature map是怎么对应特征图的精度呢？:
  - CADYQ 中是每个conv之后的feature map都对其使用一次bit selector，计算feature map的梯度信息。
  - Dynamic Network Quantization for Efficient Video Inference 中是对视频的每一帧进行量化policy计算，确定每一frame需要的bit，甚至为了计算效率跳过某一帧

生成重要性图/attention map——>具有更高分数/系数的channel分配更高的bit，否则就是更低的bit

如何分配？参考CADyQ或者其他

然后再加一个 权重和激活参数量与模型性能的均衡lmbda参数

1. 运动信息+纹理结构的复杂度判断模块，bit
2. 时间+空间的知识蒸馏



- 为什么运动如此重要？相比运动物体，人眼对于静止物体能感知到更多细节。因此，编码器可以对运动物体采用更大的压缩（去除更多细节），对静止物体采用更小的压缩（保留更多细节）。人眼视觉系统会被运动分散注意力，且运动物体在屏幕上停留时间少，所以能觉察到的失真少。而静止物体在屏幕停留时间长有足够时间观察，且无法分散注意力，所以能觉察到的失真多。
- 通常有大量运动的视频需要更多比特，空间细节丰富和纹理复杂的视频也较难编码。
- 下一帧延用上一帧的量化策略？
- 添加一个约束项：对模型复杂度和训练精度的平衡（同样参看CADyQ和DAQ）
- weight混合精度——确定量化敏感度：（对me、mv、context分段量化，发现
- 时空知识蒸馏？纹理知识蒸馏？



思路2：对每一层之后的structure map和texture map进行

由于需要保证编解码平台的一致性：编码端和解码端的混合bit-width需要固定







前景背景分开？/或者直接提取ROI感兴趣区域！+ channel重组+group-wise 一个layer分为不同的组对应不同的bit

前景标注为1，背景标注为0

我们的人眼大多集中于前景的感兴趣区域

前景、背景分成两张图来传输，最后融合图像

ROI编码：低延迟、直播、视频会议等场景

前景+背景分离可能试用于特定视频监控等背景非移动场景

可以再加一个基于视频理解的：判断前景的状态：运动大、运动小、运动中、静态画面、纹理复杂程度等来综合判断是否要提取前景：提取的话：前景和背景分别是什么精度/不提取的话：查看纹理结构，根据纹理复杂度来判断当前帧的精度



1. 生成前景/背景掩码：动态判断是否分离前景和背景，以一个模型参数运算量为限制
2. 根据纹理/结构复杂度分数判断混合精度的bit
3. 判断是否分离前景和背景+根据纹理/结构复杂度分数判断混合精度bit的policy network与main network联合优化。
4. 



思路n：提取出每一帧的结构/纹理图，单独进行压缩传输，在解码端与其他的特征进行融合重构。

动态地给运动信息、纹理结构特征、和其他特征分配bit，policy network动态调整.(当然我觉得如果分级提取出特征后，可以不动态调整，在学习过程中搜索出最优精度)



提取出原图边缘结构和重建图边缘结构：分析下失真差异，以及量化引起的误差



多尺度特征？：不同分辨率特征分配不同的通道数——>不同分辨率特征分配不同bits

**什么需要重点的多bits**

1. 由运动信息可得到的前景信息
2. 由纹理结构图可得到的需要高精度的重建细节信息



## 前景掩码

### 光流生成前景物体掩码：[Object discovery in videos as **foreground** motion clustering](http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Object_Discovery_in_Videos_as_Foreground_Motion_Clustering_CVPR_2019_paper.html)**CCF A**

### 仅仅是光流可能不太准确，还需要多一个其他操作来确定前景区域或敏感区域







## patch的划分优化

1. **两个思路**：（1）直接在图像输入的一开始就划分好patch，这样一开始就决定了精度，且后期不会发生变化；（2）还是在每个feature map生成过程中，训练出合适的patch大小呢，然后再将前景和背景分别占据的patch与掩码进行对应？
2. policy network在划分patch的过程就确定了每个patch的精度吗？在后期经过conv后的feature map的patch精度是和之前一样，还是需要动态变化呢？

## 提取texture信息，encode入先验中；作为纹理恢复的一个重要信息

融合texture信息，将texture作为先验信息编入hyper中，然后最后对生成的特征图融合增强；

**掩码信息应该也需要传给解码端，根据掩码信息来恢复patch的精度设置**

## 在生成掩码的区域添加空间、时间信息增强块？





## patch的精度如何设置？数学逻辑？





## mv的精度如何确定（因为没法softmax）：candidates中的最高bit-width吧



## 在生成掩码的区域添加空间、时间信息增强块





## 时间、空间蒸馏



## 生成掩码

**仅仅是光流信息对前景感知不准确，添加一些其他的细节！**

尤其是在low latency的应用场景中，人眼会更多关注于ROI区域，对其他区域具有更大的忽略





**分组卷积+channel shuffle？或者参考DCVC-DC的cross-group fusion**

提取前景feature+背景feature，卷积变换再在通道维度concat后，变成原有的输入channels数

由于分组卷积+reLu代替GDN，缺乏通道间的信息交互，因此时间、空间蒸馏

**可学习的分组卷积**



~~将自监督的分割网络融合到编码网络中，联合训练 感觉这样复杂度太高了，没什么必要其实~~

视频理解——>语义信息——>掩码——>前后景分离





**所以MASK到底要怎么生成！！！！**

当没有明确前景时，考虑将纹理结构丰富的地方提取，作为需要高精度的channels

每个layer的bit candidate怎么选择：forward 和 backward







如何制定混合精度的策略呢：

如果是dynamic：就是bit controller输出概率

如果是static：就是可学习的参数，利用量化敏感度来求







视频理解+编码：生成语义内容、字幕、标签、理解内容等等





**weights：静态策略**

**activation：前景/背景 动态策略**

**光流前景分割**：mask generation 二值的掩码 而非real-valued



**图像质量评价**





本周主要是继续完善之前根据ROI来自适应分配帧前景和背景的bit-width。目前ROI掩码的生成采用预训练的光流分割，分别提取前景/背景帧后再进行concat，以group-wise的方式分配前景group和背景group的bit-width。相比于之前的实验，对每一帧动态评估运动大小、纹理等来获取每一帧的bit-width，考虑到编码端和解码端的一致性，动态的策略需要将额外的量化信息传入到比特流，所以更改方案为静态的。目前补充DoReFa和LQ-Nets的实验；混合精度目前简单按照cadyq中bit candidate的概率进行选择。



1. 继续补充在HAWQ和HAQ等上的实验结果；
2. 完善在mv en/decoder和context en/decoder的编解码信息的时空蒸馏。
3. 补充消融实验部分。





本周主要是继续完善之前根据ROI来自适应分配帧前景和背景的bit-width。目前ROI掩码的生成采用预训练的光流分割，分别提取前景/背景帧后再进行concat，以group-wise的方式分配前景group和背景group的bit-width。相比于之前的实验，对每一帧动态评估运动大小、纹理等来获取每一帧的bit-width，考虑到编码端和解码端的一致性，动态的策略需要将额外的量化信息传入到比特流，所以更改方案为静态的。目前补充DoReFa和LQ-Nets的实验；混合精度目前简单按照cadyq中bit candidate的概率进行选择。

本周主要是继续完善之前根据ROI来自适应分配帧前景和背景的bit-width。本周主要是优化熵编码结构以完成并行熵编码，之前的auto-regressive先验是一种序列化的解决方案，遵循严格的扫描顺序。对并行不友好，推理速度很慢，因此优化为并行熵编码可加速在移动设备上的推理速度；对比动态和静态的bit-width 量化选择策略，关于bit-width的量化策略网络还需要再优化一下。

1. 继续补充在HAWQ和HAQ等上的实验结果；
2. 完善在mv en/decoder和context en/decoder的编解码信息的时空蒸馏。
3. 补充消融实验部分。





参考 Self-supervised Video Object Segmentation by Motion Grouping 自监督分割光流

并行熵编码

不将前景和背景concat，换为两个处理分支呢？然后重建前添加融合模块

将自监督分割模型整合进入DCVC中，端到端的训练？/还是固定这一部分的参数（貌似固定参数比较可行）



**LOSS**

$\lambda$ ROI loss + $\beta$ non-ROI loss ROI区域和NON-ROI区域的重建质量分离



mv和context均划分为roi和非roi？

需要重构的时候恢复由于划分前景和背景造成的损失（相比于完整图像）decoded_frame也经过mask







本周主要是继续完善之前根据ROI来自适应分配帧前景和背景的bit-width。本周主要是优化熵编码结构以完成并行熵编码，之前的auto-regressive先验是一种序列化的解决方案，遵循严格的扫描顺序。对并行不友好，推理速度很慢，因此优化为并行熵编码可加速在移动设备上的推理速度；对比动态和静态的bit-width 量化选择策略，关于bit-width的量化策略网络还需要再优化一下。

1. 继续完善在mv en/decoder和context en/decoder的编解码信息的时空蒸馏。
2. 补充消融实验部分，继续补充在HAWQ和HAQ等上的实验结果。



本周完善对ROI区域mask生成的优化。之前采用预训练的分割网络，但考虑到压缩数据源的多样性，对于不同种类数据源无法同样精确确定，且引入额外的开销成本。参考motion group引入光流自监督分割，引入slot attention绑定运动实例，ROI和non-ROI在解码后的融合图像与raw image计算重建误差，该部分的loss与R-Dloss合并来联合训练，可实现端到端的优化。光流分割阶段的时间开销相对于编解码阶段的时间成本可较小，可忽略不计。除此之外，对CutLER中的maskcut的粗糙掩码生成方法进行了实验，但生成时间开销较大。

1. 尽快完成mask生成阶段的优化。
2. 继续完善在mv en/decoder和context en/decoder的编解码信息的时空蒸馏。
3. 补充消融实验部分，继续补充在HAWQ和HAQ等上的实验结果

**当下一帧与上一帧之间的运动差异较大时，放弃ROI和NON-ROI区域的单独处理**







应该是每一对frame会有两个mask gap1和gap-1 参考motion group



找个引用：由于人眼只能在视频帧中最多关注几个目标，选择前3面积最大的mask
