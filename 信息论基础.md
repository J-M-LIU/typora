# 信息论

## 熵

### 自信息、条件自信息和联合自信息

自信息、条件 自信息和联合自信息用来衡量单一事件发生时所包含的信息量。

#### 自信息

随机变量 X = a~i~ 的自信息定义为：
$$
I(x) = log\frac{1}{p(x)} = -logp(x)
$$
**含义**

(I) 在事件发生前，自信息表示事件发生的不确定性。因为概率小的事件不易发生，所以包含较大的不确定性;而概率大的事件容易发生， 因此不确定性较小。

(2)在事件发生后，自信息表示事件所包含的信息量，是提供给信宿的信息量， 也是解除这种不确定性所需要的信息量。概率大的事件不仅容易预测，发生后所提 供的信息量也小;而概率小的事件不仅难于预测，发生后所提供的信息量也大。

#### 条件自信息

事件 x=a~i~ 在事件 y=b~i~ 给定条件下的自信息：
$$
I(x|y) = -log \ p(x|y) 
$$

#### 联合自信息

联合事件集合 XY中的事件 x=a~i~;, y= b~j~ 的自信息定义为
$$
I(xy) = -log\ p(xy) = -log\ p(x)p(y|x) = I(x) + I(y|x) \\
I(xy) = -log\ p(xy) = -log\ p(y)p(x|y) = I(y) + I(x|y)
$$
当 x y相互独立时
$$
I(xy) = I(x) + I(y)
$$

### 互信息和信息散度

x与y的互信息等于 x 的自信息减去在 y 条件下 x 的自信息
$$
I(x;y) = I(x) - I(x|y) = log\ \frac{p(x|y)}{p(x)} \\
= log\ \frac{p(xy)}{p(x)p(y)} = I(x)+I(y)-I(xy)
$$


#### KL散度

若P和Q为定义在同 一概率空间的两个概率测度，定义P相对于Q的散度为
$$
D(P||Q) = \sum_i[p(x_i)log\ p(x_i) - p(x_i)log\ q(x_i)] = \sum_ip(x_i)log\ \frac{p(x_i)}{q(x_i)}
$$

#### 交叉熵

即 P 为真实已知的标签/分布，Q 为预测概率分布，最小化交叉熵损失来使 P 逼近 Q 分布。
$$
H(P,Q) = E_{p(x)～q(x)}[I_p[x]] = E_{p(x)～q(x)}[-log\ p(x)] =  -\sum_i q(x_i)log\ p(x_i)
$$


### 离散集合的平均信息熵

#### 信息熵

离散信源 X 的煽定义为自信息的平均值，记为 H(X)
$$
H(X) = E_{p(x)}[I(x)] = -\sum_i p(x_i)log\ p(x_i)
$$

#### 条件熵

联合集XY上条件自信息 I(ylx) 的平均值定义为条件墒

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221106002634840.png" alt="image-20221106002634840" style="zoom:40%;" />

#### 联合熵

联合集XY上联合自信息 I(xy) 的平均值称为联合墒，即
$$
H(XY) = E_{p(xy)}[I(xy)] = -\sum_x \sum_y p(xy)log\ p(xy)
$$
联合熵表示为
$$
H(XY) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$
