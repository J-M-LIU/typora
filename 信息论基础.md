# 信息论

## 熵

熵是服从某一特定概率分布事件的理论最小平均编码长度.

一文搞懂熵(Entropy),交叉熵(Cross-Entropy) - 将为帅的文章 - 知乎 https://zhuanlan.zhihu.com/p/149186719

### 自信息、条件自信息和联合自信息

自信息、条件自信息和联合自信息用来衡量单一事件发生时所包含的信息量。

#### 自信息

随机变量 X = a~i~ 的自信息定义为：
$$
I(x) = log\frac{1}{p(x)} = -logp(x)
$$
**含义**

(1) 在事件发生前，自信息表示事件发生的不确定性。因为概率小的事件不易发生，所以包含较大的不确定性;而概率大的事件容易发生，因此不确定性较小。

(2)在事件发生后，自信息表示事件所包含的信息量，是提供给信宿的信息量， 也是解除这种不确定性所需要的信息量。概率大的事件不仅容易预测，发生后所提供的信息量也小；而概率小的事件不仅难于预测，发生后所提供的信息量也大。

#### 条件自信息

事件 x=a~i~ 在事件 y=b~i~ 给定条件下的自信息：
$$
I(x|y) = -log \ p(x|y) 
$$

#### 联合自信息

联合事件集合 XY中的事件 x=a~i~ ; y= b~j~ 的自信息定义为
$$
I(xy) = -log\ p(xy) = -log\ p(x)p(y|x) = I(x) + I(y|x) \\
I(xy) = -log\ p(xy) = -log\ p(y)p(x|y) = I(y) + I(x|y)
$$
当 x y相互独立时
$$
I(xy) = I(x) + I(y)
$$

### 互信息和信息散度

x与y的互信息等于 x 的自信息减去在 y 条件下 x 的自信息
$$
I(x;y) = I(x) - I(x|y) = log\ \frac{p(x|y)}{p(x)} \\
= log\ \frac{p(xy)}{p(x)p(y)} = I(x)+I(y)-I(xy)
$$


#### KL散度/相对熵

若P和Q为定义在同 一概率空间的两个概率测度，定义P相对于Q的散度为
$$
D(P||Q) = \sum_i[p(x_i)log\ p(x_i) - p(x_i)log\ q(x_i)] = \sum_ip(x_i)log\ \frac{p(x_i)}{q(x_i)}
$$

等式的前一部分是P分布的信息熵，后一部分是Q分布相对P的交叉熵。

#### 交叉熵

即 P 为真实已知的标签/分布，Q 为预测概率分布，最小化交叉熵损失来使 Q 逼近 P 分布。
$$
H(Q,P) = E_{x～P}[I_Q[x]] = E_{x～P}[-log\ q(x)] =  -\sum_i p(x_i)log\ q(x_i)
$$

$E_{x~P}[f(x)]=\sum_xP(x)f(x)$ 可理解为 $x$ 由分布P产生，$f$ 作用于 $x$ 时 $f(x)$ 的平均值。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵，$H(P,Q) \geq H(P)$

### 离散集合的平均信息熵

#### 信息熵

离散信源 X 的熵定义为自信息的平均值，记为 H(X)。信息熵是信源平均不确定性大小的度量：在信源输出前，表示信源的平均不确定性；信源输出后，表示一个信源符号提供的平均信息量。H(X)越大，不确定性越大。
$$
H(X) = E_{p(x)}[I(x)] = -\sum_i p(x_i)log\ p(x_i)
$$

#### 条件熵

联合集XY上条件自信息 I(ylx) 的平均值定义为条件墒

<img src="https://cdn.jsdelivr.net/gh/J-M-LIU/pic-bed@master//img/image-20221106002634840.png" alt="image-20221106002634840" style="zoom:40%;" />

#### 联合熵

联合集XY上联合自信息 I(xy) 的平均值称为联合墒，即
$$
H(XY) = E_{p(xy)}[I(xy)] = -\sum_x \sum_y p(xy)log\ p(xy)
$$
联合熵表示为
$$
H(XY) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$


## 马尔可夫链

​	马尔可夫链（Markov chain），又称离散时间马尔可夫链，是马尔可夫过程中的一个特例，为具备马尔可夫性质与离散时间状态的随机过程。该过程中，在给定当前知识或信息的情况下，只有当前的状态用来预测将来，过去（即当前以前的历史状态）对于预测将来（即当前以后的未来状态）是无关的。
​	在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变称为过渡，与不同的状态改变相关的概率称为过渡概率。随机漫步就是马尔可夫链的例子。随机漫步中每一步的状态都是在图形中的点，每一步可以移动到任何一个相邻的点，在这里移动到每一个点的概率都是相同的。

### 有限状态马氏链

​	考虑一个随机序列 $\{x_n，n≥0\}$，其中每个随机变量 $x_n(n≥1)$仅通过最接近的变量 $x_{n-1}$依赖于过去的随机变量 $x_{n-1}$， $x_{n-2}$，··，即对所有 $i,j,k,...$，有
$$
p(x_n=j|x_{n-1}=i,x_{n-2}=k,···,x_0=m) = p(x_n=j|x_{n-1}=i)
$$

​	则称$\{x_n，n≥0\}$为马尔可夫链，简称马氏链。随机变量$x$，称为马氏链在$n$时刻的状态。在时刻 $n$ 状态的可能值，即$1,···, J$ 通常也称为状态；$1,···, J$ 构成的集合S称为状态集合。上式定义的是一阶马氏链，即当前所处状态只和前一个状态有直接关系，即系统在时刻 $n-1$ 的状态给定条件下在时刻 $n$ 所处状态的概率与过去的其他时刻的状态无关。类似地，也可以定义 $n$ 阶马氏链，即当前所处状态的概率仅与前 $n$ 个状态有直接关系。

### 状态转移概率

​	状态是指客观事物可能出现或存在的状态；状态转移概率是指客观事物由一种状态转移到另一种状态的概率。对于离散时刻 m 和 n，相应的状态转移概率表示为：

